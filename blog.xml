<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>John Richmond</title>
<link>https://fromLittleAcorns.github.io/john.richmond.github.io/blog.html</link>
<atom:link href="https://fromLittleAcorns.github.io/john.richmond.github.io/blog.xml" rel="self" type="application/rss+xml"/>
<description>7QN8N70N41&#39;s personal website</description>
<generator>quarto-1.2.269</generator>
<lastBuildDate>Sun, 27 Nov 2022 14:37:18 GMT</lastBuildDate>
<item>
  <title>Notebook to establish mini-batch training from first principles</title>
  <dc:creator>John Richmond</dc:creator>
  <link>https://fromLittleAcorns.github.io/john.richmond.github.io/posts/2022-11-30-fastai-course22p/04_dataloaders_optimisers_training.html</link>
  <description><![CDATA[ 



<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>Notebook based upon the fastai course 22p, “Part 2 of Practical Deep Learning for Coders”. This notebook builds the capability for training a model in batches. As such it covers: * understanding managing and accessing model parameters * Cross entropy loss for classification * dataloaders, including samplers and multiprocessing * optimisers - implementation of SGD and how to implement using model parameters * setting up training loops with validation</p>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;">import</span> pickle,gzip,math,os,time,shutil,torch,matplotlib <span class="im" style="color: #00769E;">as</span> mpl,numpy <span class="im" style="color: #00769E;">as</span> np,matplotlib.pyplot <span class="im" style="color: #00769E;">as</span> plt</span>
<span id="cb1-2"><span class="im" style="color: #00769E;">from</span> pathlib <span class="im" style="color: #00769E;">import</span> Path</span>
<span id="cb1-3"><span class="im" style="color: #00769E;">from</span> torch <span class="im" style="color: #00769E;">import</span> tensor,nn</span>
<span id="cb1-4"><span class="im" style="color: #00769E;">import</span> torch.nn.functional <span class="im" style="color: #00769E;">as</span> F</span>
<span id="cb1-5"><span class="im" style="color: #00769E;">from</span> fastcore.test <span class="im" style="color: #00769E;">import</span> test_close</span>
<span id="cb1-6"></span>
<span id="cb1-7">torch.set_printoptions(precision<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">2</span>, linewidth<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">140</span>, sci_mode<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span>)</span>
<span id="cb1-8">torch.manual_seed(<span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb1-9">mpl.rcParams[<span class="st" style="color: #20794D;">'image.cmap'</span>] <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">'gray'</span></span>
<span id="cb1-10"></span>
<span id="cb1-11">path_to_data <span class="op" style="color: #5E5E5E;">=</span> Path(<span class="st" style="color: #20794D;">'/Users/johnrichmond/local_datasets'</span>) <span class="op" style="color: #5E5E5E;">/</span> Path(<span class="st" style="color: #20794D;">'data'</span>)</span>
<span id="cb1-12">path_to_data.mkdir(exist_ok<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>)</span>
<span id="cb1-13">path_gz <span class="op" style="color: #5E5E5E;">=</span> path_to_data <span class="op" style="color: #5E5E5E;">/</span> <span class="st" style="color: #20794D;">'mnist.pkg.gz'</span></span>
<span id="cb1-14"></span>
<span id="cb1-15"><span class="cf" style="color: #003B4F;">with</span> gzip.<span class="bu" style="color: null;">open</span>(path_gz, <span class="st" style="color: #20794D;">'rb'</span>) <span class="im" style="color: #00769E;">as</span> f: ((x_train, y_train), (x_valid, y_valid), _) <span class="op" style="color: #5E5E5E;">=</span> pickle.load(f, encoding<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'latin-1'</span>)</span>
<span id="cb1-16">x_train, y_train, x_valid, y_valid <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">map</span>(tensor, [x_train, y_train, x_valid, y_valid])</span></code></pre></div>
</div>
</section>
<section id="initial-setup" class="level1">
<h1>Initial setup</h1>
<section id="data" class="level2">
<h2 class="anchored" data-anchor-id="data">Data</h2>
<p>As before use Mnist data with a single hidden layer of 50 neurons</p>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1">n,m <span class="op" style="color: #5E5E5E;">=</span> x_train.shape</span>
<span id="cb2-2">c <span class="op" style="color: #5E5E5E;">=</span> y_train.<span class="bu" style="color: null;">max</span>()<span class="op" style="color: #5E5E5E;">+</span><span class="dv" style="color: #AD0000;">1</span></span>
<span id="cb2-3">nh <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">50</span></span>
<span id="cb2-4">n, m, c</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="2">
<pre><code>(50000, 784, tensor(10))</code></pre>
</div>
</div>
<p>Create a simple model that inherits from the pytorch nn.Module class</p>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="kw" style="color: #003B4F;">class</span> Model(nn.Module):</span>
<span id="cb4-2">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, n_in, nh, n_out):</span>
<span id="cb4-3">        <span class="bu" style="color: null;">super</span>().<span class="fu" style="color: #4758AB;">__init__</span>()</span>
<span id="cb4-4">        <span class="va" style="color: #111111;">self</span>.layers <span class="op" style="color: #5E5E5E;">=</span> [nn.Linear(n_in,nh), nn.ReLU(), nn.Linear(nh,n_out)]</span>
<span id="cb4-5">        </span>
<span id="cb4-6">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__call__</span>(<span class="va" style="color: #111111;">self</span>, x):</span>
<span id="cb4-7">        <span class="cf" style="color: #003B4F;">for</span> l <span class="kw" style="color: #003B4F;">in</span> <span class="va" style="color: #111111;">self</span>.layers: x <span class="op" style="color: #5E5E5E;">=</span> l(x)</span>
<span id="cb4-8">        <span class="cf" style="color: #003B4F;">return</span> x</span></code></pre></div>
</div>
<p>Pass the training data though the model and store the predictions. Check the shape of the output and also what the range of the preds is</p>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1">model <span class="op" style="color: #5E5E5E;">=</span> Model(n_in<span class="op" style="color: #5E5E5E;">=</span>m, nh<span class="op" style="color: #5E5E5E;">=</span>nh, n_out<span class="op" style="color: #5E5E5E;">=</span>c)</span>
<span id="cb5-2">preds <span class="op" style="color: #5E5E5E;">=</span> model(x_train.clone())</span>
<span id="cb5-3">preds.shape, preds.<span class="bu" style="color: null;">min</span>(), preds.<span class="bu" style="color: null;">max</span>()</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="4">
<pre><code>(torch.Size([50000, 10]),
 tensor(-0.35, grad_fn=&lt;MinBackward1&gt;),
 tensor(0.42, grad_fn=&lt;MaxBackward1&gt;))</code></pre>
</div>
</div>
</section>
</section>
<section id="loss-function" class="level1">
<h1>Loss Function</h1>
<p>In the previous notebook a very simple loss function was used. This will now be replaced with a cross entropy loss. There are several “tricks” that are used to take what is basically a relatively simple concept and implement it in a robust and efficient fashion. These will be explained.</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Chbox%7Bsoftmax(x)%7D_%7Bi%7D%20=%20%5Cfrac%7Be%5E%7Bx%7B_i%7D%7D%7D%7Be%5E%7Bx%7B_1%7D%7D+e%5E%7Bx%7B_2%7D%7D%20+%20%5Ccdots%20+%20e%5E%7Bx%7B_n%7D%7D%7D"></p>
<p>Or:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Chbox%7Bsoftmax(x)%7D_%7Bi%7D%20=%20%5Cfrac%7Be%5E%7Bx_%7Bi%7D%7D%7D%7B%5Csum%5Climits_%7B0%20%5Cleq%20j%20%3C%20n%7D%7Be%5E%7Bx_%7Bj%7D%7D%7D%7D"></p>
<div class="cell" data-execution_count="113">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><span class="co" style="color: #5E5E5E;"># Note that this relies upon broadcasting the sum along axis 1 to make the divide work</span></span>
<span id="cb7-2"><span class="kw" style="color: #003B4F;">def</span> log_softmax(x): <span class="cf" style="color: #003B4F;">return</span> (x.exp() <span class="op" style="color: #5E5E5E;">/</span> (x.exp().<span class="bu" style="color: null;">sum</span>(axis<span class="op" style="color: #5E5E5E;">=-</span><span class="dv" style="color: #AD0000;">1</span>, keepdim<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>))).log()</span></code></pre></div>
</div>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1">log_softmax(preds)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="6">
<pre><code>tensor([[-2.37, -2.49, -2.36,  ..., -2.31, -2.28, -2.22],
        [-2.37, -2.44, -2.44,  ..., -2.27, -2.26, -2.16],
        [-2.48, -2.33, -2.28,  ..., -2.30, -2.30, -2.27],
        ...,
        [-2.33, -2.52, -2.34,  ..., -2.31, -2.21, -2.16],
        [-2.38, -2.38, -2.33,  ..., -2.29, -2.26, -2.17],
        [-2.33, -2.55, -2.36,  ..., -2.29, -2.27, -2.16]], grad_fn=&lt;LogBackward0&gt;)</code></pre>
</div>
</div>
<p>Since the log of a division is the same as subtracting the logs of each value this can be further simplified</p>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><span class="kw" style="color: #003B4F;">def</span> log_softmax(x): <span class="cf" style="color: #003B4F;">return</span> (x <span class="op" style="color: #5E5E5E;">-</span> (x.exp().<span class="bu" style="color: null;">sum</span>(axis<span class="op" style="color: #5E5E5E;">=-</span><span class="dv" style="color: #AD0000;">1</span>, keepdim<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>)).log())</span></code></pre></div>
</div>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1">log_softmax(preds)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="8">
<pre><code>tensor([[-2.37, -2.49, -2.36,  ..., -2.31, -2.28, -2.22],
        [-2.37, -2.44, -2.44,  ..., -2.27, -2.26, -2.16],
        [-2.48, -2.33, -2.28,  ..., -2.30, -2.30, -2.27],
        ...,
        [-2.33, -2.52, -2.34,  ..., -2.31, -2.21, -2.16],
        [-2.38, -2.38, -2.33,  ..., -2.29, -2.26, -2.17],
        [-2.33, -2.55, -2.36,  ..., -2.29, -2.27, -2.16]], grad_fn=&lt;SubBackward0&gt;)</code></pre>
</div>
</div>
<p>Then, there is a way to compute the log of the sum of exponentials in a more stable way, called the <a href="https://en.wikipedia.org/wiki/LogSumExp">LogSumExp trick</a>. The idea is to use the following formula:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Clog%20%5Cleft%20(%20%5Csum_%7Bj=1%7D%5E%7Bn%7D%20e%5E%7Bx_%7Bj%7D%7D%20%5Cright%20)%20=%20%5Clog%20%5Cleft%20(%20e%5E%7Ba%7D%20%5Csum_%7Bj=1%7D%5E%7Bn%7D%20e%5E%7Bx_%7Bj%7D-a%7D%20%5Cright%20)%20=%20a%20+%20%5Clog%20%5Cleft%20(%20%5Csum_%7Bj=1%7D%5E%7Bn%7D%20e%5E%7Bx_%7Bj%7D-a%7D%20%5Cright%20)"></p>
<p>Where a is the maximum of the <img src="https://latex.codecogs.com/png.latex?x_j"> values, and is used to scale other values, preventing numerical overflow</p>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1">x <span class="op" style="color: #5E5E5E;">=</span> preds.clone()</span>
<span id="cb13-2">x.shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="9">
<pre><code>torch.Size([50000, 10])</code></pre>
</div>
</div>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><span class="kw" style="color: #003B4F;">def</span> logsumexp(x):</span>
<span id="cb15-2">    <span class="co" style="color: #5E5E5E;"># obtain the maximum x in each input row</span></span>
<span id="cb15-3">    max_row_vals <span class="op" style="color: #5E5E5E;">=</span> x.<span class="bu" style="color: null;">max</span>(axis<span class="op" style="color: #5E5E5E;">=-</span><span class="dv" style="color: #AD0000;">1</span>)[<span class="dv" style="color: #AD0000;">0</span>]</span>
<span id="cb15-4">    <span class="cf" style="color: #003B4F;">return</span> (max_row_vals <span class="op" style="color: #5E5E5E;">+</span> (x <span class="op" style="color: #5E5E5E;">-</span> max_row_vals[:, <span class="va" style="color: #111111;">None</span>]).exp().<span class="bu" style="color: null;">sum</span>(<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>).log())</span></code></pre></div>
</div>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1">logsumexp(x)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="11">
<pre><code>tensor([2.28, 2.30, 2.29,  ..., 2.30, 2.28, 2.30], grad_fn=&lt;AddBackward0&gt;)</code></pre>
</div>
</div>
<p>Pytorch already has a pre build logsumexp and this can be used instead of the above. To check the same values are returned:</p>
<div class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb18" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1">test_close(logsumexp(x), x.logsumexp(dim<span class="op" style="color: #5E5E5E;">=-</span><span class="dv" style="color: #AD0000;">1</span>, keepdim<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span>))</span></code></pre></div>
</div>
<p>Hence it is possible to simplify the above log softmax to</p>
<div class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb19" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><span class="kw" style="color: #003B4F;">def</span> log_softmax(x):</span>
<span id="cb19-2">    <span class="cf" style="color: #003B4F;">return</span> x<span class="op" style="color: #5E5E5E;">-</span>x.logsumexp(<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>, keepdim<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb20" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1">lsm_preds <span class="op" style="color: #5E5E5E;">=</span> log_softmax(x)</span>
<span id="cb20-2">lsm_preds</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="14">
<pre><code>tensor([[-2.37, -2.49, -2.36,  ..., -2.31, -2.28, -2.22],
        [-2.37, -2.44, -2.44,  ..., -2.27, -2.26, -2.16],
        [-2.48, -2.33, -2.28,  ..., -2.30, -2.30, -2.27],
        ...,
        [-2.33, -2.52, -2.34,  ..., -2.31, -2.21, -2.16],
        [-2.38, -2.38, -2.33,  ..., -2.29, -2.26, -2.17],
        [-2.33, -2.55, -2.36,  ..., -2.29, -2.27, -2.16]], grad_fn=&lt;SubBackward0&gt;)</code></pre>
</div>
</div>
<p>Having calculated the log softmax we now need a proper loss function. For classification we can use the cross entropy loss. This is defined as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%20CE%20Loss%20=%20%5Csum_i%7Bx_i%20%5Clog%7B%20p(x_i)%7D%7D"></p>
<p>where x is a vector of targets. Note that in Pytorch this is not one hot encoded and so the value can be used as an index to select the column of the preds that it refers to.</p>
<p>In this case the softmax can be interpreted as a probability (since it adds up to one for each case) and the output from the log_softmax is <img src="https://latex.codecogs.com/png.latex?log(p(x_i))"> is a vector for each class.</p>
<p>The CE loss can therefore be obtained by taking the mean of the CE loss over the dataset. Since x is zero over every element apart from the target, where it is one, this in practice means that all that has to be done is to calculate <img src="https://latex.codecogs.com/png.latex?-%5Clog%7Bp(x_i)%7D"> where i is the index of the actual target</p>
<p>Obtaining the index of the target can be done using the target as an index (as discussed above)</p>
<div class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb22" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1">y_train.shape, y_train[<span class="dv" style="color: #AD0000;">0</span>:<span class="dv" style="color: #AD0000;">3</span>]</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="15">
<pre><code>(torch.Size([50000]), tensor([5, 0, 4]))</code></pre>
</div>
</div>
<p>Since y_train is a vector we can use it to select the column as below. This could be summed and divided by the number of datapoint but the alternative is used, which is to take the mean</p>
<div class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb24" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><span class="kw" style="color: #003B4F;">def</span> nll(preds, targets):</span>
<span id="cb24-2">    <span class="cf" style="color: #003B4F;">return</span> <span class="op" style="color: #5E5E5E;">-</span>preds[<span class="bu" style="color: null;">range</span>(preds.shape[<span class="dv" style="color: #AD0000;">0</span>]), targets].mean()</span></code></pre></div>
</div>
<div class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb25" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1">loss <span class="op" style="color: #5E5E5E;">=</span> nll(lsm_preds, y_train)</span>
<span id="cb25-2">loss</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="17">
<pre><code>tensor(2.30, grad_fn=&lt;NegBackward0&gt;)</code></pre>
</div>
</div>
<p>Check that the loss value calculate above matches that from Pytorch</p>
<div class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb27" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1">test_close(F.nll_loss(F.log_softmax(x, dim<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>), y_train), loss, <span class="fl" style="color: #AD0000;">1.e-3</span>)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb28" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1">test_close(F.cross_entropy(x, y_train), loss, <span class="fl" style="color: #AD0000;">1.e-3</span>)</span></code></pre></div>
</div>
<p>We have now generate a loss function from first principles and can use Pytorch’s version moving forward</p>
</section>
<section id="development-of-a-training-loop" class="level1">
<h1>Development of a training loop</h1>
<p>The training loop repeats the following iteratively: * Load a batch of input data and the corresponding target outputs * Calculate the predictions from the input data * Calculate the loss based upon the targets and predictions * Calculate the gradients with respect to the loss of all of the model parameters * Update the parameters based upon the gradients so as to reduce the loss</p>
<section id="a-simple-training-loop" class="level2">
<h2 class="anchored" data-anchor-id="a-simple-training-loop">A simple training loop</h2>
<p>Before we train it is convenient to have a way to ascertain the accuracy of the model. To develop this a single batch of data will be processed</p>
<div class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb29" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1">bs <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">64</span></span>
<span id="cb29-2"><span class="co" style="color: #5E5E5E;"># load a minibatch</span></span>
<span id="cb29-3">xb <span class="op" style="color: #5E5E5E;">=</span> x_train[<span class="dv" style="color: #AD0000;">0</span>:bs]</span>
<span id="cb29-4">yb <span class="op" style="color: #5E5E5E;">=</span> y_train[<span class="dv" style="color: #AD0000;">0</span>:bs]</span>
<span id="cb29-5">preds <span class="op" style="color: #5E5E5E;">=</span> model(xb)</span>
<span id="cb29-6">preds.shape, preds[<span class="dv" style="color: #AD0000;">0</span>]</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="20">
<pre><code>(torch.Size([64, 10]),
 tensor([-0.09, -0.21, -0.08,  0.10, -0.04,  0.08, -0.04, -0.03,  0.01,  0.06], grad_fn=&lt;SelectBackward0&gt;))</code></pre>
</div>
</div>
<div class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb31" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><span class="co" style="color: #5E5E5E;"># Define a loss function and calculate the loss for the batch</span></span>
<span id="cb31-2">loss_func <span class="op" style="color: #5E5E5E;">=</span> F.cross_entropy</span>
<span id="cb31-3">loss <span class="op" style="color: #5E5E5E;">=</span> loss_func(preds, yb)</span>
<span id="cb31-4">loss</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="21">
<pre><code>tensor(2.30, grad_fn=&lt;NllLossBackward0&gt;)</code></pre>
</div>
</div>
<section id="calculate-accuracy" class="level3">
<h3 class="anchored" data-anchor-id="calculate-accuracy">Calculate Accuracy</h3>
<p>The predicted class can be determined from the index of the class having the highest predicted value</p>
<div class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb33" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1">pred_class <span class="op" style="color: #5E5E5E;">=</span> torch.argmax(preds, axis<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb33-2">pred_class</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="22">
<pre><code>tensor([3, 9, 3, 8, 5, 9, 3, 9, 3, 9, 5, 3, 9, 9, 3, 9, 9, 5, 8, 7, 9, 5, 3, 8, 9, 5, 9, 5, 5, 9, 3, 5, 9, 7, 5, 7, 9, 9, 3, 9, 3, 5, 3, 8,
        3, 5, 9, 5, 9, 5, 3, 9, 3, 8, 9, 5, 9, 5, 9, 5, 8, 8, 9, 8])</code></pre>
</div>
</div>
<p>From this the accuracy can be calculated</p>
<div class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb35" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><span class="kw" style="color: #003B4F;">def</span> accuracy(preds, targets):</span>
<span id="cb35-2">    <span class="cf" style="color: #003B4F;">return</span> (torch.argmax(preds, axis<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>) <span class="op" style="color: #5E5E5E;">==</span> targets).<span class="bu" style="color: null;">float</span>().mean()</span></code></pre></div>
</div>
<div class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb36" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1">accuracy(preds, yb)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="24">
<pre><code>tensor(0.09)</code></pre>
</div>
</div>
<p>Not suprisingly, at this point the accuracy is what would be expected for a random prediction.</p>
<p>To improve this we need to train the model. To do that it is necessary to say what learning rate we would like and how many epochs to train for</p>
<div class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb38" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1">lr <span class="op" style="color: #5E5E5E;">=</span> <span class="fl" style="color: #AD0000;">0.5</span></span>
<span id="cb38-2">epochs <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">3</span></span></code></pre></div>
</div>
<p>Now create a simple training loop following the above steps. Note that the loss, backward and parameter update is happening for every batch</p>
<div class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb39" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><span class="cf" style="color: #003B4F;">for</span> epoch <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(epochs):</span>
<span id="cb39-2">    <span class="co" style="color: #5E5E5E;"># iterate through batches</span></span>
<span id="cb39-3">    <span class="cf" style="color: #003B4F;">for</span> batch <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="dv" style="color: #AD0000;">0</span>, n, bs):</span>
<span id="cb39-4">        <span class="co" style="color: #5E5E5E;"># get data</span></span>
<span id="cb39-5">        s <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">slice</span>(batch, <span class="bu" style="color: null;">min</span>(n, batch<span class="op" style="color: #5E5E5E;">+</span>bs))</span>
<span id="cb39-6">        xs <span class="op" style="color: #5E5E5E;">=</span> x_train[s]</span>
<span id="cb39-7">        ys <span class="op" style="color: #5E5E5E;">=</span> y_train[s]</span>
<span id="cb39-8">        <span class="co" style="color: #5E5E5E;"># Pass data through the model</span></span>
<span id="cb39-9">        preds <span class="op" style="color: #5E5E5E;">=</span> model(xs)</span>
<span id="cb39-10">        <span class="co" style="color: #5E5E5E;"># Calculate loss</span></span>
<span id="cb39-11">        loss <span class="op" style="color: #5E5E5E;">=</span> loss_func(preds, ys)</span>
<span id="cb39-12">        <span class="co" style="color: #5E5E5E;"># Calculate the gradients</span></span>
<span id="cb39-13">        loss.backward()</span>
<span id="cb39-14">        <span class="co" style="color: #5E5E5E;"># Print the loss periodically</span></span>
<span id="cb39-15">        <span class="cf" style="color: #003B4F;">if</span> batch <span class="op" style="color: #5E5E5E;">//</span> bs <span class="op" style="color: #5E5E5E;">//</span> <span class="dv" style="color: #AD0000;">10</span> <span class="op" style="color: #5E5E5E;">==</span> <span class="dv" style="color: #AD0000;">0</span>: <span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f"Loss: </span><span class="sc" style="color: #5E5E5E;">{</span>loss<span class="sc" style="color: #5E5E5E;">.</span>item()<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">, accuracy = </span><span class="sc" style="color: #5E5E5E;">{</span>accuracy(preds, ys)<span class="sc" style="color: #5E5E5E;">.</span>item()<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">"</span>)</span>
<span id="cb39-16">        <span class="cf" style="color: #003B4F;">with</span> torch.no_grad():</span>
<span id="cb39-17">            <span class="cf" style="color: #003B4F;">for</span> l <span class="kw" style="color: #003B4F;">in</span> model.layers:</span>
<span id="cb39-18">                <span class="cf" style="color: #003B4F;">if</span> <span class="bu" style="color: null;">hasattr</span>(l, <span class="st" style="color: #20794D;">"weight"</span>):</span>
<span id="cb39-19">                    l.weight <span class="op" style="color: #5E5E5E;">-=</span> l.weight.grad <span class="op" style="color: #5E5E5E;">*</span> lr</span>
<span id="cb39-20">                    l.bias <span class="op" style="color: #5E5E5E;">-=</span> l.bias.grad <span class="op" style="color: #5E5E5E;">*</span> lr</span>
<span id="cb39-21">                    l.weight.grad.zero_()</span>
<span id="cb39-22">                    l.bias.grad.zero_()</span>
<span id="cb39-23">        </span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss: 2.3036487102508545, accuracy = 0.09375
Loss: 2.2052316665649414, accuracy = 0.296875
Loss: 2.1905035972595215, accuracy = 0.21875
Loss: 2.010064125061035, accuracy = 0.53125
Loss: 1.9125093221664429, accuracy = 0.5
Loss: 1.7081104516983032, accuracy = 0.703125
Loss: 1.6426200866699219, accuracy = 0.5
Loss: 1.6326062679290771, accuracy = 0.625
Loss: 1.5491729974746704, accuracy = 0.4375
Loss: 1.5245637893676758, accuracy = 0.53125
Loss: 0.12374816089868546, accuracy = 0.96875
Loss: 0.14763614535331726, accuracy = 0.984375
Loss: 0.29614493250846863, accuracy = 0.890625
Loss: 0.16717804968357086, accuracy = 0.9375
Loss: 0.1868995726108551, accuracy = 0.90625
Loss: 0.05588085949420929, accuracy = 0.984375
Loss: 0.10521863400936127, accuracy = 0.96875
Loss: 0.2990257740020752, accuracy = 0.9375
Loss: 0.04911200702190399, accuracy = 1.0
Loss: 0.25950485467910767, accuracy = 0.9375
Loss: 0.09232573211193085, accuracy = 0.96875
Loss: 0.10159078985452652, accuracy = 0.984375
Loss: 0.28459155559539795, accuracy = 0.9375
Loss: 0.05941237136721611, accuracy = 0.984375
Loss: 0.11075811833143234, accuracy = 0.9375
Loss: 0.031431593000888824, accuracy = 0.984375
Loss: 0.07170089334249496, accuracy = 0.984375
Loss: 0.24644441902637482, accuracy = 0.953125
Loss: 0.05402196943759918, accuracy = 0.984375
Loss: 0.18466435372829437, accuracy = 0.984375</code></pre>
</div>
</div>
</section>
</section>
<section id="adding-parameters-and-optim" class="level2">
<h2 class="anchored" data-anchor-id="adding-parameters-and-optim">Adding parameters and optim</h2>
<p>The above training loop works but accessing the model parameters us cumbersome since it requires advanced knowledge of the layers. Pytorch has some methods to allow accessing and modifying layer information in a more straightforward manner</p>
<section id="parameters" class="level3">
<h3 class="anchored" data-anchor-id="parameters">Parameters</h3>
<p>Recreate the model with individual layers</p>
<div class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb41" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><span class="kw" style="color: #003B4F;">class</span> Model(nn.Module):</span>
<span id="cb41-2">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, n_in, nh, n_out):</span>
<span id="cb41-3">        <span class="bu" style="color: null;">super</span>().<span class="fu" style="color: #4758AB;">__init__</span>()</span>
<span id="cb41-4">        <span class="va" style="color: #111111;">self</span>.l1 <span class="op" style="color: #5E5E5E;">=</span> nn.Linear(n_in,nh)</span>
<span id="cb41-5">        <span class="va" style="color: #111111;">self</span>.relu <span class="op" style="color: #5E5E5E;">=</span> nn.ReLU()</span>
<span id="cb41-6">        <span class="va" style="color: #111111;">self</span>.l2 <span class="op" style="color: #5E5E5E;">=</span> nn.Linear(nh,n_out)</span>
<span id="cb41-7">        </span>
<span id="cb41-8">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__call__</span>(<span class="va" style="color: #111111;">self</span>, x):</span>
<span id="cb41-9">        x <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.l2(<span class="va" style="color: #111111;">self</span>.relu(<span class="va" style="color: #111111;">self</span>.l1(x)))</span>
<span id="cb41-10">        <span class="cf" style="color: #003B4F;">return</span> x</span></code></pre></div>
</div>
<div class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb42" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1">model <span class="op" style="color: #5E5E5E;">=</span> Model(m, nh, c)</span></code></pre></div>
</div>
<p>Examine the model parameters</p>
<div class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb43" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><span class="cf" style="color: #003B4F;">for</span> name, layer <span class="kw" style="color: #003B4F;">in</span> model.named_children():</span>
<span id="cb43-2">    <span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f"Layer: </span><span class="sc" style="color: #5E5E5E;">{</span>name<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">, Parameters: </span><span class="sc" style="color: #5E5E5E;">{</span>layer<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">"</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Layer: l1, Parameters: Linear(in_features=784, out_features=50, bias=True)
Layer: relu, Parameters: ReLU()
Layer: l2, Parameters: Linear(in_features=50, out_features=10, bias=True)</code></pre>
</div>
</div>
<p>The layers can be accessed using the name alone, for example</p>
<div class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb45" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1">model.l1</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="30">
<pre><code>Linear(in_features=784, out_features=50, bias=True)</code></pre>
</div>
</div>
<p>The layer parameters can also be accessed by the model.parameters property, however, this is an iterator and hence needs to be listed through a loop or creation of a list…</p>
<div class="cell" data-execution_count="31">
<div class="sourceCode cell-code" id="cb47" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><span class="cf" style="color: #003B4F;">for</span> param <span class="kw" style="color: #003B4F;">in</span> model.parameters():</span>
<span id="cb47-2">    <span class="bu" style="color: null;">print</span>(param.shape)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>torch.Size([50, 784])
torch.Size([50])
torch.Size([10, 50])
torch.Size([10])</code></pre>
</div>
</div>
<div class="cell" data-execution_count="32">
<div class="sourceCode cell-code" id="cb49" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><span class="cf" style="color: #003B4F;">for</span> param <span class="kw" style="color: #003B4F;">in</span> model.parameters():</span>
<span id="cb49-2">    <span class="bu" style="color: null;">print</span>(param[<span class="dv" style="color: #AD0000;">0</span>:<span class="dv" style="color: #AD0000;">2</span>])</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([[ 0.04,  0.03, -0.02,  ...,  0.03,  0.03,  0.00],
        [ 0.04,  0.01,  0.03,  ..., -0.01,  0.02,  0.03]], grad_fn=&lt;SliceBackward0&gt;)
tensor([-0.01, -0.01], grad_fn=&lt;SliceBackward0&gt;)
tensor([[-0.05,  0.02,  0.07,  0.09,  0.10,  0.12, -0.13,  0.10, -0.09, -0.08, -0.12, -0.01,  0.07, -0.00,  0.12, -0.03, -0.07, -0.14,
          0.10,  0.13, -0.12,  0.14,  0.12,  0.08, -0.11,  0.03, -0.09,  0.12,  0.01, -0.03,  0.06, -0.00,  0.01, -0.05,  0.11,  0.14,
          0.07,  0.05, -0.09, -0.03, -0.01, -0.01,  0.08,  0.02, -0.09, -0.05,  0.03,  0.13, -0.08,  0.13],
        [ 0.09, -0.04,  0.00,  0.14, -0.13, -0.06,  0.03, -0.09, -0.11,  0.05,  0.04, -0.02, -0.04, -0.03,  0.01, -0.10, -0.03, -0.02,
          0.00,  0.07,  0.10, -0.08, -0.14, -0.02, -0.13, -0.08,  0.07,  0.04, -0.13, -0.13, -0.05,  0.12, -0.08,  0.13,  0.13, -0.10,
          0.06,  0.08, -0.13, -0.08, -0.06, -0.11,  0.07,  0.06,  0.09,  0.04, -0.13,  0.04, -0.10, -0.01]], grad_fn=&lt;SliceBackward0&gt;)
tensor([-0.01, -0.06], grad_fn=&lt;SliceBackward0&gt;)</code></pre>
</div>
</div>
<p>Using parameters it is possible to simplify the code to optimise the weights and biases by looping through teh model parameters. It is also possible to zero all of the model’s gradients with a since call:</p>
<div class="cell" data-execution_count="33">
<div class="sourceCode cell-code" id="cb51" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><span class="kw" style="color: #003B4F;">def</span> fit():</span>
<span id="cb51-2">    <span class="cf" style="color: #003B4F;">for</span> epoch <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(epochs):</span>
<span id="cb51-3">        <span class="co" style="color: #5E5E5E;"># iterate through batches</span></span>
<span id="cb51-4">        <span class="cf" style="color: #003B4F;">for</span> batch <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="dv" style="color: #AD0000;">0</span>, n, bs):</span>
<span id="cb51-5">            <span class="co" style="color: #5E5E5E;"># get data</span></span>
<span id="cb51-6">            s <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">slice</span>(batch, <span class="bu" style="color: null;">min</span>(n, batch<span class="op" style="color: #5E5E5E;">+</span>bs))</span>
<span id="cb51-7">            xs <span class="op" style="color: #5E5E5E;">=</span> x_train[s]</span>
<span id="cb51-8">            ys <span class="op" style="color: #5E5E5E;">=</span> y_train[s]</span>
<span id="cb51-9">            <span class="co" style="color: #5E5E5E;"># Pass data through the model</span></span>
<span id="cb51-10">            preds <span class="op" style="color: #5E5E5E;">=</span> model(xs)</span>
<span id="cb51-11">            <span class="co" style="color: #5E5E5E;"># Calculate loss</span></span>
<span id="cb51-12">            loss <span class="op" style="color: #5E5E5E;">=</span> loss_func(preds, ys)</span>
<span id="cb51-13">            <span class="co" style="color: #5E5E5E;"># Calculate the gradients</span></span>
<span id="cb51-14">            loss.backward()</span>
<span id="cb51-15">            <span class="co" style="color: #5E5E5E;"># Print the loss periodically</span></span>
<span id="cb51-16">            <span class="cf" style="color: #003B4F;">if</span> batch <span class="op" style="color: #5E5E5E;">//</span> bs <span class="op" style="color: #5E5E5E;">//</span> <span class="dv" style="color: #AD0000;">10</span> <span class="op" style="color: #5E5E5E;">==</span> <span class="dv" style="color: #AD0000;">0</span>: <span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f"Loss: </span><span class="sc" style="color: #5E5E5E;">{</span>loss<span class="sc" style="color: #5E5E5E;">.</span>item()<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">, accuracy = </span><span class="sc" style="color: #5E5E5E;">{</span>accuracy(preds, ys)<span class="sc" style="color: #5E5E5E;">.</span>item()<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">"</span>)</span>
<span id="cb51-17">            <span class="cf" style="color: #003B4F;">with</span> torch.no_grad():</span>
<span id="cb51-18">                <span class="cf" style="color: #003B4F;">for</span> params <span class="kw" style="color: #003B4F;">in</span> model.parameters():</span>
<span id="cb51-19">                    params <span class="op" style="color: #5E5E5E;">-=</span> params.grad <span class="op" style="color: #5E5E5E;">*</span> lr</span>
<span id="cb51-20">                model.zero_grad()                      </span></code></pre></div>
</div>
<div class="cell" data-execution_count="34">
<div class="sourceCode cell-code" id="cb52" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1">fit()</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss: 2.309434175491333, accuracy = 0.0625
Loss: 2.177271842956543, accuracy = 0.234375
Loss: 2.259394645690918, accuracy = 0.078125
Loss: 2.0509822368621826, accuracy = 0.578125
Loss: 1.9572300910949707, accuracy = 0.34375
Loss: 1.7929021120071411, accuracy = 0.71875
Loss: 1.6334275007247925, accuracy = 0.65625
Loss: 1.5452338457107544, accuracy = 0.625
Loss: 1.507757306098938, accuracy = 0.4375
Loss: 1.6010795831680298, accuracy = 0.484375
Loss: 0.2006874829530716, accuracy = 0.953125
Loss: 0.11892185360193253, accuracy = 0.96875
Loss: 0.2844753861427307, accuracy = 0.90625
Loss: 0.17238563299179077, accuracy = 0.96875
Loss: 0.15464989840984344, accuracy = 0.90625
Loss: 0.04285932332277298, accuracy = 1.0
Loss: 0.12839168310165405, accuracy = 0.96875
Loss: 0.3218374252319336, accuracy = 0.953125
Loss: 0.09430787712335587, accuracy = 0.96875
Loss: 0.3029904365539551, accuracy = 0.953125
Loss: 0.18196934461593628, accuracy = 0.9375
Loss: 0.0731976106762886, accuracy = 0.984375
Loss: 0.29567304253578186, accuracy = 0.9375
Loss: 0.0792182981967926, accuracy = 0.984375
Loss: 0.09979861974716187, accuracy = 0.953125
Loss: 0.028896179050207138, accuracy = 1.0
Loss: 0.10094335675239563, accuracy = 0.96875
Loss: 0.28324049711227417, accuracy = 0.9375
Loss: 0.055503442883491516, accuracy = 0.984375
Loss: 0.2787169814109802, accuracy = 0.96875</code></pre>
</div>
</div>
<p>The way that the model parameters are setup is by Pytorch overriding the <strong>setattrib</strong> method in nn.Module. The way that this works is as below</p>
<div class="cell" data-execution_count="35">
<div class="sourceCode cell-code" id="cb54" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><span class="kw" style="color: #003B4F;">class</span> DummyModule():</span>
<span id="cb54-2">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, n_in, nh, n_out):</span>
<span id="cb54-3">        <span class="va" style="color: #111111;">self</span>._modules <span class="op" style="color: #5E5E5E;">=</span> {}</span>
<span id="cb54-4">        <span class="va" style="color: #111111;">self</span>.l1 <span class="op" style="color: #5E5E5E;">=</span> nn.Linear(n_in,nh)</span>
<span id="cb54-5">        <span class="va" style="color: #111111;">self</span>.relu <span class="op" style="color: #5E5E5E;">=</span> nn.ReLU()</span>
<span id="cb54-6">        <span class="va" style="color: #111111;">self</span>.l2 <span class="op" style="color: #5E5E5E;">=</span> nn.Linear(nh,n_out)</span>
<span id="cb54-7">        </span>
<span id="cb54-8">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__setattr__</span>(<span class="va" style="color: #111111;">self</span>, k,v):</span>
<span id="cb54-9">        <span class="co" style="color: #5E5E5E;"># Assign the layers to the modules dict before calling the set attribute class.  The parameters are </span></span>
<span id="cb54-10">        <span class="co" style="color: #5E5E5E;"># simply name (k), value (v)</span></span>
<span id="cb54-11">        <span class="cf" style="color: #003B4F;">if</span> <span class="kw" style="color: #003B4F;">not</span> k.startswith(<span class="st" style="color: #20794D;">"_"</span>):</span>
<span id="cb54-12">            <span class="va" style="color: #111111;">self</span>._modules[k] <span class="op" style="color: #5E5E5E;">=</span> v</span>
<span id="cb54-13">        <span class="bu" style="color: null;">super</span>().<span class="fu" style="color: #4758AB;">__setattr__</span>(k,v)</span>
<span id="cb54-14">            </span>
<span id="cb54-15">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__repr__</span>(<span class="va" style="color: #111111;">self</span>): </span>
<span id="cb54-16">        <span class="co" style="color: #5E5E5E;"># Setup so that the official string representation of the class instance is a listing of the module</span></span>
<span id="cb54-17">        <span class="cf" style="color: #003B4F;">return</span> <span class="ss" style="color: #20794D;">f"</span><span class="sc" style="color: #5E5E5E;">{</span><span class="va" style="color: #111111;">self</span><span class="sc" style="color: #5E5E5E;">.</span>_modules<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">"</span></span>
<span id="cb54-18">    </span>
<span id="cb54-19">    <span class="kw" style="color: #003B4F;">def</span> parameters(<span class="va" style="color: #111111;">self</span>):</span>
<span id="cb54-20">        <span class="co" style="color: #5E5E5E;"># Create an iterator to yield all of the model parameters</span></span>
<span id="cb54-21">        <span class="cf" style="color: #003B4F;">for</span> layer <span class="kw" style="color: #003B4F;">in</span> <span class="va" style="color: #111111;">self</span>._modules.values():</span>
<span id="cb54-22">            <span class="cf" style="color: #003B4F;">for</span> param <span class="kw" style="color: #003B4F;">in</span> layer.parameters(): <span class="cf" style="color: #003B4F;">yield</span>(param)</span>
<span id="cb54-23">            </span></code></pre></div>
</div>
<div class="cell" data-execution_count="36">
<div class="sourceCode cell-code" id="cb55" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1">dm <span class="op" style="color: #5E5E5E;">=</span> DummyModule(m, nh, c)</span>
<span id="cb55-2">dm</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="36">
<pre><code>{'l1': Linear(in_features=784, out_features=50, bias=True), 'relu': ReLU(), 'l2': Linear(in_features=50, out_features=10, bias=True)}</code></pre>
</div>
</div>
<div class="cell" data-execution_count="37">
<div class="sourceCode cell-code" id="cb57" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><span class="cf" style="color: #003B4F;">for</span> param <span class="kw" style="color: #003B4F;">in</span> dm.parameters():</span>
<span id="cb57-2">    <span class="bu" style="color: null;">print</span>(param.shape)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>torch.Size([50, 784])
torch.Size([50])
torch.Size([10, 50])
torch.Size([10])</code></pre>
</div>
</div>
<p>This approach won’t work as it with lists of layers as things were originally setup. To allow this. to work the layers have to be individually registered</p>
<div class="cell" data-execution_count="38">
<div class="sourceCode cell-code" id="cb59" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1">layers <span class="op" style="color: #5E5E5E;">=</span> [nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,<span class="dv" style="color: #AD0000;">10</span>)]</span></code></pre></div>
</div>
<div class="cell" data-execution_count="39">
<div class="sourceCode cell-code" id="cb60" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><span class="kw" style="color: #003B4F;">class</span> Model(nn.Module):</span>
<span id="cb60-2">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, layers):</span>
<span id="cb60-3">        <span class="bu" style="color: null;">super</span>().<span class="fu" style="color: #4758AB;">__init__</span>()</span>
<span id="cb60-4">        <span class="va" style="color: #111111;">self</span>.layers <span class="op" style="color: #5E5E5E;">=</span> layers</span>
<span id="cb60-5">        <span class="cf" style="color: #003B4F;">for</span> i, layer <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">enumerate</span>(<span class="va" style="color: #111111;">self</span>.layers):</span>
<span id="cb60-6">            <span class="va" style="color: #111111;">self</span>.add_module(<span class="ss" style="color: #20794D;">f"layer_</span><span class="sc" style="color: #5E5E5E;">{</span>i<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">"</span>, layer)</span>
<span id="cb60-7">    </span>
<span id="cb60-8">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__call__</span>(<span class="va" style="color: #111111;">self</span>, x):</span>
<span id="cb60-9">        <span class="cf" style="color: #003B4F;">for</span> layer <span class="kw" style="color: #003B4F;">in</span> <span class="va" style="color: #111111;">self</span>.layers:</span>
<span id="cb60-10">            x <span class="op" style="color: #5E5E5E;">=</span> layer(x)</span>
<span id="cb60-11">        <span class="cf" style="color: #003B4F;">return</span> x</span>
<span id="cb60-12">    </span></code></pre></div>
</div>
<div class="cell" data-execution_count="40">
<div class="sourceCode cell-code" id="cb61" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1">model <span class="op" style="color: #5E5E5E;">=</span> Model(layers)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="41">
<div class="sourceCode cell-code" id="cb62" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb62-1">model</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="41">
<pre><code>Model(
  (layer_0): Linear(in_features=784, out_features=50, bias=True)
  (layer_1): ReLU()
  (layer_2): Linear(in_features=50, out_features=10, bias=True)
)</code></pre>
</div>
</div>
<div class="cell" data-execution_count="42">
<div class="sourceCode cell-code" id="cb64" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb64-1">fit()</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss: 2.3222596645355225, accuracy = 0.03125
Loss: 2.2330336570739746, accuracy = 0.21875
Loss: 2.222136974334717, accuracy = 0.140625
Loss: 2.079106330871582, accuracy = 0.421875
Loss: 1.958361029624939, accuracy = 0.5625
Loss: 1.7925820350646973, accuracy = 0.625
Loss: 1.675372838973999, accuracy = 0.515625
Loss: 1.6172857284545898, accuracy = 0.609375
Loss: 1.4986895322799683, accuracy = 0.546875
Loss: 1.5764800310134888, accuracy = 0.515625
Loss: 0.16430063545703888, accuracy = 0.953125
Loss: 0.12203794717788696, accuracy = 0.984375
Loss: 0.3307473063468933, accuracy = 0.890625
Loss: 0.21634256839752197, accuracy = 0.921875
Loss: 0.17808055877685547, accuracy = 0.921875
Loss: 0.04548545181751251, accuracy = 0.984375
Loss: 0.13622808456420898, accuracy = 0.96875
Loss: 0.32709184288978577, accuracy = 0.9375
Loss: 0.09109017997980118, accuracy = 0.984375
Loss: 0.27433082461357117, accuracy = 0.953125
Loss: 0.09979915618896484, accuracy = 0.96875
Loss: 0.08315068483352661, accuracy = 0.984375
Loss: 0.27988025546073914, accuracy = 0.90625
Loss: 0.1201467365026474, accuracy = 0.953125
Loss: 0.14109008014202118, accuracy = 0.9375
Loss: 0.01877472549676895, accuracy = 1.0
Loss: 0.10770482569932938, accuracy = 0.96875
Loss: 0.3286954462528229, accuracy = 0.953125
Loss: 0.06740975379943848, accuracy = 0.984375
Loss: 0.18299852311611176, accuracy = 0.984375</code></pre>
</div>
</div>
<p>Registering the layers can be done using the nn.ModuleList class: <a href="https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html?highlight=nn+modulelist#torch.nn.ModuleList">Link to nn.ModuleList docs</a></p>
<p>Thus this allows further simplification</p>
<div class="cell" data-execution_count="43">
<div class="sourceCode cell-code" id="cb66" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb66-1"><span class="kw" style="color: #003B4F;">class</span> SequentialModel(nn.Module):</span>
<span id="cb66-2">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, layers):</span>
<span id="cb66-3">        <span class="bu" style="color: null;">super</span>().<span class="fu" style="color: #4758AB;">__init__</span>()</span>
<span id="cb66-4">        <span class="va" style="color: #111111;">self</span>.layers <span class="op" style="color: #5E5E5E;">=</span> nn.ModuleList(layers)</span>
<span id="cb66-5">        </span>
<span id="cb66-6">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__call__</span>(<span class="va" style="color: #111111;">self</span>, x):</span>
<span id="cb66-7">        <span class="cf" style="color: #003B4F;">for</span> l <span class="kw" style="color: #003B4F;">in</span> <span class="va" style="color: #111111;">self</span>.layers: x <span class="op" style="color: #5E5E5E;">=</span> l(x)</span>
<span id="cb66-8">        <span class="cf" style="color: #003B4F;">return</span> x</span></code></pre></div>
</div>
<div class="cell" data-execution_count="44">
<div class="sourceCode cell-code" id="cb67" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb67-1">model <span class="op" style="color: #5E5E5E;">=</span> SequentialModel(layers)</span>
<span id="cb67-2">model</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="44">
<pre><code>SequentialModel(
  (layers): ModuleList(
    (0): Linear(in_features=784, out_features=50, bias=True)
    (1): ReLU()
    (2): Linear(in_features=50, out_features=10, bias=True)
  )
)</code></pre>
</div>
</div>
<div class="cell" data-execution_count="45">
<div class="sourceCode cell-code" id="cb69" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb69-1">fit()</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss: 0.06515892595052719, accuracy = 0.96875
Loss: 0.06317746639251709, accuracy = 0.984375
Loss: 0.21157535910606384, accuracy = 0.921875
Loss: 0.10860879719257355, accuracy = 0.96875
Loss: 0.08964528143405914, accuracy = 0.9375
Loss: 0.011501152999699116, accuracy = 1.0
Loss: 0.08443643152713776, accuracy = 0.96875
Loss: 0.3277687132358551, accuracy = 0.9375
Loss: 0.06517940014600754, accuracy = 0.984375
Loss: 0.17099246382713318, accuracy = 0.96875
Loss: 0.04173600673675537, accuracy = 0.984375
Loss: 0.04945621266961098, accuracy = 0.984375
Loss: 0.1965624988079071, accuracy = 0.90625
Loss: 0.0814109668135643, accuracy = 0.96875
Loss: 0.06253797560930252, accuracy = 0.96875
Loss: 0.009009594097733498, accuracy = 1.0
Loss: 0.08011716604232788, accuracy = 0.96875
Loss: 0.32447099685668945, accuracy = 0.9375
Loss: 0.05112294852733612, accuracy = 0.984375
Loss: 0.15682904422283173, accuracy = 0.984375
Loss: 0.02698095701634884, accuracy = 1.0
Loss: 0.035535961389541626, accuracy = 0.984375
Loss: 0.15110039710998535, accuracy = 0.921875
Loss: 0.0608980655670166, accuracy = 0.96875
Loss: 0.03945880010724068, accuracy = 1.0
Loss: 0.005177198443561792, accuracy = 1.0
Loss: 0.07748386263847351, accuracy = 0.96875
Loss: 0.3055875599384308, accuracy = 0.9375
Loss: 0.04133985936641693, accuracy = 0.984375
Loss: 0.16388559341430664, accuracy = 0.984375</code></pre>
</div>
</div>
<div class="cell" data-execution_count="46">
<div class="sourceCode cell-code" id="cb71" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb71-1">loss_func(model(xb), yb), accuracy(model(xb), yb)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="46">
<pre><code>(tensor(0.02, grad_fn=&lt;NllLossBackward0&gt;), tensor(1.))</code></pre>
</div>
</div>
</section>
<section id="the-nn.sequential-class" class="level3">
<h3 class="anchored" data-anchor-id="the-nn.sequential-class">The nn.Sequential Class</h3>
<p>This class does what is done above, in other words it takes a list of layers and created a model by registering and then saving the layers in sequence.</p>
<p>The sequential class will not accept a list as an input and so we have to pass in the individual layers</p>
<div class="cell" data-execution_count="47">
<div class="sourceCode cell-code" id="cb73" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb73-1">model <span class="op" style="color: #5E5E5E;">=</span> nn.Sequential(nn.Linear(m, nh), nn.ReLU(), nn.Linear(nh, c))</span>
<span id="cb73-2">model</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="47">
<pre><code>Sequential(
  (0): Linear(in_features=784, out_features=50, bias=True)
  (1): ReLU()
  (2): Linear(in_features=50, out_features=10, bias=True)
)</code></pre>
</div>
</div>
<div class="cell" data-execution_count="48">
<div class="sourceCode cell-code" id="cb75" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb75-1">fit()</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss: 2.3087124824523926, accuracy = 0.09375
Loss: 2.220724582672119, accuracy = 0.4375
Loss: 2.2094054222106934, accuracy = 0.15625
Loss: 2.0799782276153564, accuracy = 0.484375
Loss: 1.9859000444412231, accuracy = 0.421875
Loss: 1.8239319324493408, accuracy = 0.5625
Loss: 1.692657232284546, accuracy = 0.625
Loss: 1.5962902307510376, accuracy = 0.671875
Loss: 1.4929006099700928, accuracy = 0.46875
Loss: 1.5572547912597656, accuracy = 0.53125
Loss: 0.2031664103269577, accuracy = 0.90625
Loss: 0.12023147940635681, accuracy = 0.984375
Loss: 0.36551305651664734, accuracy = 0.890625
Loss: 0.1421089768409729, accuracy = 0.96875
Loss: 0.13040338456630707, accuracy = 0.921875
Loss: 0.03445771709084511, accuracy = 0.984375
Loss: 0.14158782362937927, accuracy = 0.96875
Loss: 0.29142311215400696, accuracy = 0.953125
Loss: 0.1052810475230217, accuracy = 0.953125
Loss: 0.22577130794525146, accuracy = 0.984375
Loss: 0.20330604910850525, accuracy = 0.921875
Loss: 0.1524616777896881, accuracy = 0.96875
Loss: 0.3139619827270508, accuracy = 0.921875
Loss: 0.10483880341053009, accuracy = 0.953125
Loss: 0.09913913160562515, accuracy = 0.9375
Loss: 0.02614644728600979, accuracy = 1.0
Loss: 0.1354278326034546, accuracy = 0.96875
Loss: 0.2590445876121521, accuracy = 0.953125
Loss: 0.06993013620376587, accuracy = 0.96875
Loss: 0.1877637505531311, accuracy = 0.984375</code></pre>
</div>
</div>
</section>
<section id="introducing-the-optimiser-class-optim" class="level3">
<h3 class="anchored" data-anchor-id="introducing-the-optimiser-class-optim">Introducing the optimiser class Optim</h3>
<p>So far we have developed the loss function and the model but the appliction of the weights has been done using a relatively simple implmentation of SGD. The process of optimising the model parameters can be built into a class. In Pytorch this is the Optim class. This will now be developed.</p>
<p>Note that in the class below there are two things to ensure: 1. The params are converted into a list in the init function (to facilitate iteration) 2. when zeroing the gradients it is important to use the in place version of the function</p>
<div class="cell" data-execution_count="49">
<div class="sourceCode cell-code" id="cb77" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb77-1"><span class="kw" style="color: #003B4F;">class</span> Optimizer():</span>
<span id="cb77-2">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, params, lr<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.5</span>):</span>
<span id="cb77-3">        <span class="va" style="color: #111111;">self</span>.params <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">list</span>(params)<span class="op" style="color: #5E5E5E;">;</span> <span class="va" style="color: #111111;">self</span>.lr <span class="op" style="color: #5E5E5E;">=</span> lr</span>
<span id="cb77-4">        </span>
<span id="cb77-5">    <span class="kw" style="color: #003B4F;">def</span> step(<span class="va" style="color: #111111;">self</span>):</span>
<span id="cb77-6">        <span class="cf" style="color: #003B4F;">with</span> torch.no_grad():</span>
<span id="cb77-7">            <span class="cf" style="color: #003B4F;">for</span> param <span class="kw" style="color: #003B4F;">in</span> <span class="va" style="color: #111111;">self</span>.params: param <span class="op" style="color: #5E5E5E;">-=</span> param.grad <span class="op" style="color: #5E5E5E;">*</span> <span class="va" style="color: #111111;">self</span>.lr</span>
<span id="cb77-8">    </span>
<span id="cb77-9">    <span class="kw" style="color: #003B4F;">def</span> zero_grad(<span class="va" style="color: #111111;">self</span>):</span>
<span id="cb77-10">        <span class="cf" style="color: #003B4F;">for</span> param <span class="kw" style="color: #003B4F;">in</span> <span class="va" style="color: #111111;">self</span>.params: param.grad.data.zero_() </span></code></pre></div>
</div>
<div class="cell" data-execution_count="50">
<div class="sourceCode cell-code" id="cb78" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb78-1">model <span class="op" style="color: #5E5E5E;">=</span> nn.Sequential(nn.Linear(m, nh), nn.ReLU(), nn.Linear(nh, c))</span></code></pre></div>
</div>
<div class="cell" data-execution_count="51">
<div class="sourceCode cell-code" id="cb79" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb79-1">opt <span class="op" style="color: #5E5E5E;">=</span> Optimizer(model.parameters())</span></code></pre></div>
</div>
<p>Update the training loop to work with the optimizer</p>
<div class="cell" data-execution_count="52">
<div class="sourceCode cell-code" id="cb80" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb80-1"><span class="cf" style="color: #003B4F;">for</span> epoch <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(epochs):</span>
<span id="cb80-2">    <span class="cf" style="color: #003B4F;">for</span> batch <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="dv" style="color: #AD0000;">0</span>, n, bs):</span>
<span id="cb80-3">        <span class="co" style="color: #5E5E5E;"># get data</span></span>
<span id="cb80-4">        s <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">slice</span>(batch, <span class="bu" style="color: null;">min</span>(n, batch<span class="op" style="color: #5E5E5E;">+</span>bs))</span>
<span id="cb80-5">        xs <span class="op" style="color: #5E5E5E;">=</span> x_train[s]</span>
<span id="cb80-6">        ys <span class="op" style="color: #5E5E5E;">=</span> y_train[s]</span>
<span id="cb80-7">        <span class="co" style="color: #5E5E5E;"># Pass data through the model</span></span>
<span id="cb80-8">        preds <span class="op" style="color: #5E5E5E;">=</span> model(xs)</span>
<span id="cb80-9">        loss <span class="op" style="color: #5E5E5E;">=</span> loss_func(preds, ys)</span>
<span id="cb80-10">        <span class="co" style="color: #5E5E5E;"># Print results</span></span>
<span id="cb80-11">        <span class="cf" style="color: #003B4F;">if</span> batch <span class="op" style="color: #5E5E5E;">//</span> bs <span class="op" style="color: #5E5E5E;">//</span> <span class="dv" style="color: #AD0000;">10</span> <span class="op" style="color: #5E5E5E;">==</span> <span class="dv" style="color: #AD0000;">0</span>: <span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f"Loss: </span><span class="sc" style="color: #5E5E5E;">{</span>loss<span class="sc" style="color: #5E5E5E;">.</span>item()<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">, accuracy = </span><span class="sc" style="color: #5E5E5E;">{</span>accuracy(preds, ys)<span class="sc" style="color: #5E5E5E;">.</span>item()<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">"</span>)</span>
<span id="cb80-12">        <span class="co" style="color: #5E5E5E;"># Calculate gradients</span></span>
<span id="cb80-13">        loss.backward()</span>
<span id="cb80-14">        <span class="co" style="color: #5E5E5E;"># Apply the optimiser</span></span>
<span id="cb80-15">        opt.step()</span>
<span id="cb80-16">        opt.zero_grad()</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss: 2.30017352104187, accuracy = 0.0625
Loss: 2.1758975982666016, accuracy = 0.234375
Loss: 2.199198007583618, accuracy = 0.1875
Loss: 2.005563497543335, accuracy = 0.453125
Loss: 1.9248197078704834, accuracy = 0.359375
Loss: 1.772184133529663, accuracy = 0.578125
Loss: 1.62507963180542, accuracy = 0.546875
Loss: 1.5354200601577759, accuracy = 0.640625
Loss: 1.5023629665374756, accuracy = 0.46875
Loss: 1.6080732345581055, accuracy = 0.421875
Loss: 0.13067957758903503, accuracy = 0.96875
Loss: 0.1198587417602539, accuracy = 0.984375
Loss: 0.2864210307598114, accuracy = 0.921875
Loss: 0.21201932430267334, accuracy = 0.921875
Loss: 0.1917288601398468, accuracy = 0.921875
Loss: 0.04466291889548302, accuracy = 0.96875
Loss: 0.132561594247818, accuracy = 0.96875
Loss: 0.3979755938053131, accuracy = 0.9375
Loss: 0.10060809552669525, accuracy = 0.953125
Loss: 0.3057703375816345, accuracy = 0.9375
Loss: 0.12934750318527222, accuracy = 0.96875
Loss: 0.08657151460647583, accuracy = 0.96875
Loss: 0.21188294887542725, accuracy = 0.90625
Loss: 0.14039435982704163, accuracy = 0.96875
Loss: 0.09377827495336533, accuracy = 0.984375
Loss: 0.0185707900673151, accuracy = 1.0
Loss: 0.081519216299057, accuracy = 0.96875
Loss: 0.32961714267730713, accuracy = 0.953125
Loss: 0.051018428057432175, accuracy = 1.0
Loss: 0.21368607878684998, accuracy = 0.984375</code></pre>
</div>
</div>
<p>The same can be achieved by the use of the pytorch optim library, specifically in this case optim.SGD</p>
<div class="cell" data-execution_count="53">
<div class="sourceCode cell-code" id="cb82" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb82-1"><span class="im" style="color: #00769E;">from</span> torch <span class="im" style="color: #00769E;">import</span> optim</span></code></pre></div>
</div>
<div class="cell" data-execution_count="54">
<div class="sourceCode cell-code" id="cb83" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb83-1"><span class="kw" style="color: #003B4F;">def</span> get_model_and_optimizer():</span>
<span id="cb83-2">    model <span class="op" style="color: #5E5E5E;">=</span> nn.Sequential(nn.Linear(m, nh), nn.ReLU(), nn.Linear(nh, c))</span>
<span id="cb83-3">    opt <span class="op" style="color: #5E5E5E;">=</span> optim.SGD(model.parameters(), lr<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.5</span>)</span>
<span id="cb83-4">    <span class="cf" style="color: #003B4F;">return</span> model, opt</span></code></pre></div>
</div>
<div class="cell" data-execution_count="55">
<div class="sourceCode cell-code" id="cb84" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb84-1">model, opt <span class="op" style="color: #5E5E5E;">=</span> get_model_and_optimizer()</span>
<span id="cb84-2">loss_func(model(xb), yb)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="55">
<pre><code>tensor(2.31, grad_fn=&lt;NllLossBackward0&gt;)</code></pre>
</div>
</div>
<div class="cell" data-execution_count="56">
<div class="sourceCode cell-code" id="cb86" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb86-1"><span class="cf" style="color: #003B4F;">for</span> epoch <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(epochs):</span>
<span id="cb86-2">    <span class="cf" style="color: #003B4F;">for</span> batch <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="dv" style="color: #AD0000;">0</span>, n, bs):</span>
<span id="cb86-3">        <span class="co" style="color: #5E5E5E;"># get data</span></span>
<span id="cb86-4">        s <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">slice</span>(batch, <span class="bu" style="color: null;">min</span>(n, batch<span class="op" style="color: #5E5E5E;">+</span>bs))</span>
<span id="cb86-5">        xs <span class="op" style="color: #5E5E5E;">=</span> x_train[s]</span>
<span id="cb86-6">        ys <span class="op" style="color: #5E5E5E;">=</span> y_train[s]</span>
<span id="cb86-7">        <span class="co" style="color: #5E5E5E;"># Pass data through the model</span></span>
<span id="cb86-8">        preds <span class="op" style="color: #5E5E5E;">=</span> model(xs)</span>
<span id="cb86-9">        loss <span class="op" style="color: #5E5E5E;">=</span> loss_func(preds, ys)</span>
<span id="cb86-10">        <span class="co" style="color: #5E5E5E;"># Print results</span></span>
<span id="cb86-11">        <span class="cf" style="color: #003B4F;">if</span> batch <span class="op" style="color: #5E5E5E;">//</span> bs <span class="op" style="color: #5E5E5E;">//</span> <span class="dv" style="color: #AD0000;">10</span> <span class="op" style="color: #5E5E5E;">==</span> <span class="dv" style="color: #AD0000;">0</span>: <span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f"Loss: </span><span class="sc" style="color: #5E5E5E;">{</span>loss<span class="sc" style="color: #5E5E5E;">.</span>item()<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">, accuracy = </span><span class="sc" style="color: #5E5E5E;">{</span>accuracy(preds, ys)<span class="sc" style="color: #5E5E5E;">.</span>item()<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">"</span>)</span>
<span id="cb86-12">        <span class="co" style="color: #5E5E5E;"># Calculate gradients</span></span>
<span id="cb86-13">        loss.backward()</span>
<span id="cb86-14">        <span class="co" style="color: #5E5E5E;"># Apply the optimiser</span></span>
<span id="cb86-15">        opt.step()</span>
<span id="cb86-16">        opt.zero_grad()</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss: 2.312685012817383, accuracy = 0.078125
Loss: 2.1906702518463135, accuracy = 0.265625
Loss: 2.1999802589416504, accuracy = 0.1875
Loss: 2.0161678791046143, accuracy = 0.546875
Loss: 1.8945502042770386, accuracy = 0.421875
Loss: 1.7321070432662964, accuracy = 0.640625
Loss: 1.5854045152664185, accuracy = 0.640625
Loss: 1.4941891431808472, accuracy = 0.671875
Loss: 1.4101829528808594, accuracy = 0.546875
Loss: 1.4672781229019165, accuracy = 0.5
Loss: 0.2142210453748703, accuracy = 0.90625
Loss: 0.14814303815364838, accuracy = 0.96875
Loss: 0.32211652398109436, accuracy = 0.90625
Loss: 0.1859450340270996, accuracy = 0.953125
Loss: 0.12843002378940582, accuracy = 0.9375
Loss: 0.05411830171942711, accuracy = 0.984375
Loss: 0.13670694828033447, accuracy = 0.96875
Loss: 0.3170986473560333, accuracy = 0.953125
Loss: 0.10294653475284576, accuracy = 0.953125
Loss: 0.1652507185935974, accuracy = 0.953125
Loss: 0.178289994597435, accuracy = 0.921875
Loss: 0.10243190824985504, accuracy = 0.96875
Loss: 0.2527705132961273, accuracy = 0.921875
Loss: 0.13069084286689758, accuracy = 0.953125
Loss: 0.09603864699602127, accuracy = 0.953125
Loss: 0.02239409275352955, accuracy = 1.0
Loss: 0.1111244335770607, accuracy = 0.96875
Loss: 0.2913336157798767, accuracy = 0.953125
Loss: 0.04398681968450546, accuracy = 1.0
Loss: 0.1203981563448906, accuracy = 0.96875</code></pre>
</div>
</div>
</section>
<section id="dataset-and-dataloader" class="level3">
<h3 class="anchored" data-anchor-id="dataset-and-dataloader">Dataset and DataLoader</h3>
<p>The next part of the development is to make the dataloading more generic, faster and more robust. This is done through the creation of datasets and dataloaders</p>
<section id="dataset" class="level4">
<h4 class="anchored" data-anchor-id="dataset">Dataset</h4>
<p>It’s clunky to iterate through minibatches of x and y values separately:</p>
<div class="sourceCode" id="cb88" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb88-1">    xb <span class="op" style="color: #5E5E5E;">=</span> x_train[s]</span>
<span id="cb88-2">    yb <span class="op" style="color: #5E5E5E;">=</span> y_train[s]</span></code></pre></div>
<p>Instead, let’s do these two steps together, by introducing a <code>Dataset</code> class:</p>
<div class="sourceCode" id="cb89" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb89-1">    xb,yb <span class="op" style="color: #5E5E5E;">=</span> train_ds[s]</span></code></pre></div>
<p>In essence the dataset class is a way to robustly link inputs and targets through index values</p>
<div class="cell" data-execution_count="57">
<div class="sourceCode cell-code" id="cb90" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb90-1"><span class="kw" style="color: #003B4F;">class</span> Dataset():</span>
<span id="cb90-2">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, x, y): <span class="va" style="color: #111111;">self</span>.x <span class="op" style="color: #5E5E5E;">=</span> x<span class="op" style="color: #5E5E5E;">;</span> <span class="va" style="color: #111111;">self</span>.y <span class="op" style="color: #5E5E5E;">=</span> y</span>
<span id="cb90-3">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__len__</span>(<span class="va" style="color: #111111;">self</span>): <span class="cf" style="color: #003B4F;">return</span> <span class="bu" style="color: null;">len</span>(<span class="va" style="color: #111111;">self</span>.x)</span>
<span id="cb90-4">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__getitem__</span>(<span class="va" style="color: #111111;">self</span>, index): <span class="cf" style="color: #003B4F;">return</span> <span class="va" style="color: #111111;">self</span>.x[index], <span class="va" style="color: #111111;">self</span>.y[index]</span></code></pre></div>
</div>
<div class="cell" data-execution_count="58">
<div class="sourceCode cell-code" id="cb91" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb91-1">train_ds, valid_ds <span class="op" style="color: #5E5E5E;">=</span> Dataset(x_train, y_train), Dataset(x_valid, y_valid)</span>
<span id="cb91-2"><span class="cf" style="color: #003B4F;">assert</span> <span class="bu" style="color: null;">len</span>(x_train) <span class="op" style="color: #5E5E5E;">==</span> <span class="bu" style="color: null;">len</span>(train_ds)</span>
<span id="cb91-3"><span class="cf" style="color: #003B4F;">assert</span> <span class="bu" style="color: null;">len</span>(x_valid) <span class="op" style="color: #5E5E5E;">==</span> <span class="bu" style="color: null;">len</span>(valid_ds)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="59">
<div class="sourceCode cell-code" id="cb92" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb92-1">datum_xb, datum_yb <span class="op" style="color: #5E5E5E;">=</span> x_train[<span class="dv" style="color: #AD0000;">0</span>:bs], y_train[<span class="dv" style="color: #AD0000;">0</span>:bs]</span>
<span id="cb92-2">xb, yb <span class="op" style="color: #5E5E5E;">=</span> train_ds[<span class="dv" style="color: #AD0000;">0</span>:bs]</span>
<span id="cb92-3"><span class="cf" style="color: #003B4F;">assert</span> datum_xb.shape <span class="op" style="color: #5E5E5E;">==</span> xb.shape</span>
<span id="cb92-4"><span class="cf" style="color: #003B4F;">assert</span> datum_yb.shape <span class="op" style="color: #5E5E5E;">==</span> yb.shape</span></code></pre></div>
</div>
<div class="cell" data-execution_count="60">
<div class="sourceCode cell-code" id="cb93" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb93-1">xb[<span class="dv" style="color: #AD0000;">0</span>:<span class="dv" style="color: #AD0000;">3</span>], yb[<span class="dv" style="color: #AD0000;">0</span>:<span class="dv" style="color: #AD0000;">3</span>]</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="60">
<pre><code>(tensor([[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]),
 tensor([5, 0, 4]))</code></pre>
</div>
</div>
<p>Now adapt the training loop to use the dataset</p>
<div class="cell" data-execution_count="61">
<div class="sourceCode cell-code" id="cb95" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb95-1">model, opt <span class="op" style="color: #5E5E5E;">=</span> get_model_and_optimizer()</span></code></pre></div>
</div>
<div class="cell" data-execution_count="62">
<div class="sourceCode cell-code" id="cb96" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb96-1"><span class="cf" style="color: #003B4F;">for</span> epoch <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(epochs):</span>
<span id="cb96-2">    <span class="cf" style="color: #003B4F;">for</span> batch <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="dv" style="color: #AD0000;">0</span>, n, bs):</span>
<span id="cb96-3">        <span class="co" style="color: #5E5E5E;"># get data</span></span>
<span id="cb96-4">        xs, ys <span class="op" style="color: #5E5E5E;">=</span> train_ds[batch: <span class="bu" style="color: null;">min</span>(n, batch<span class="op" style="color: #5E5E5E;">+</span>bs)]</span>
<span id="cb96-5">        <span class="co" style="color: #5E5E5E;"># Pass data through the model</span></span>
<span id="cb96-6">        preds <span class="op" style="color: #5E5E5E;">=</span> model(xs)</span>
<span id="cb96-7">        loss <span class="op" style="color: #5E5E5E;">=</span> loss_func(preds, ys)</span>
<span id="cb96-8">        <span class="co" style="color: #5E5E5E;"># Print results</span></span>
<span id="cb96-9">        <span class="cf" style="color: #003B4F;">if</span> batch <span class="op" style="color: #5E5E5E;">//</span> bs <span class="op" style="color: #5E5E5E;">//</span> <span class="dv" style="color: #AD0000;">10</span> <span class="op" style="color: #5E5E5E;">==</span> <span class="dv" style="color: #AD0000;">0</span>: <span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f"Loss: </span><span class="sc" style="color: #5E5E5E;">{</span>loss<span class="sc" style="color: #5E5E5E;">.</span>item()<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">, accuracy = </span><span class="sc" style="color: #5E5E5E;">{</span>accuracy(preds, ys)<span class="sc" style="color: #5E5E5E;">.</span>item()<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">"</span>)</span>
<span id="cb96-10">        <span class="co" style="color: #5E5E5E;"># Calculate gradients</span></span>
<span id="cb96-11">        loss.backward()</span>
<span id="cb96-12">        <span class="co" style="color: #5E5E5E;"># Apply the optimiser</span></span>
<span id="cb96-13">        opt.step()</span>
<span id="cb96-14">        opt.zero_grad()</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss: 2.2979648113250732, accuracy = 0.09375
Loss: 2.1677284240722656, accuracy = 0.40625
Loss: 2.192817211151123, accuracy = 0.171875
Loss: 1.992324709892273, accuracy = 0.5
Loss: 1.8881453275680542, accuracy = 0.5
Loss: 1.6967313289642334, accuracy = 0.703125
Loss: 1.6032072305679321, accuracy = 0.578125
Loss: 1.5213433504104614, accuracy = 0.65625
Loss: 1.4437696933746338, accuracy = 0.46875
Loss: 1.5202784538269043, accuracy = 0.53125
Loss: 0.2117222249507904, accuracy = 0.953125
Loss: 0.15176746249198914, accuracy = 0.96875
Loss: 0.2823413908481598, accuracy = 0.921875
Loss: 0.1383940577507019, accuracy = 0.953125
Loss: 0.13499413430690765, accuracy = 0.921875
Loss: 0.04650742560625076, accuracy = 0.984375
Loss: 0.12225606292486191, accuracy = 0.953125
Loss: 0.31971240043640137, accuracy = 0.953125
Loss: 0.07508628815412521, accuracy = 0.96875
Loss: 0.22737926244735718, accuracy = 0.96875
Loss: 0.1587153971195221, accuracy = 0.921875
Loss: 0.12036815285682678, accuracy = 0.984375
Loss: 0.27983012795448303, accuracy = 0.90625
Loss: 0.07115809619426727, accuracy = 0.984375
Loss: 0.10483404994010925, accuracy = 0.9375
Loss: 0.025242304429411888, accuracy = 1.0
Loss: 0.09612352401018143, accuracy = 0.953125
Loss: 0.2911374866962433, accuracy = 0.953125
Loss: 0.05527857318520546, accuracy = 0.984375
Loss: 0.16760416328907013, accuracy = 0.984375</code></pre>
</div>
</div>
</section>
<section id="dataloader" class="level4">
<h4 class="anchored" data-anchor-id="dataloader">DataLoader</h4>
<p>The dataloader takes responsibility for which data to load, which allows choices to me made such as whether to sample randomly or sequentially etc. Effectively the dataloader is an iterator that will feed data one batch at a time until the dataset is exhausted.</p>
<p>In the example below a simple sequential sampler it implemented, others will be added later</p>
<div class="cell" data-execution_count="63">
<div class="sourceCode cell-code" id="cb98" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb98-1"><span class="kw" style="color: #003B4F;">class</span> DataLoader():</span>
<span id="cb98-2">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, dataset, bs):</span>
<span id="cb98-3">        <span class="va" style="color: #111111;">self</span>.dataset, <span class="va" style="color: #111111;">self</span>.bs <span class="op" style="color: #5E5E5E;">=</span> dataset, bs</span>
<span id="cb98-4">    </span>
<span id="cb98-5">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__iter__</span>(<span class="va" style="color: #111111;">self</span>):</span>
<span id="cb98-6">        <span class="cf" style="color: #003B4F;">for</span> i <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="dv" style="color: #AD0000;">0</span>, <span class="bu" style="color: null;">len</span>(<span class="va" style="color: #111111;">self</span>.dataset), <span class="va" style="color: #111111;">self</span>.bs):</span>
<span id="cb98-7">            <span class="cf" style="color: #003B4F;">yield</span> <span class="va" style="color: #111111;">self</span>.dataset[i:<span class="bu" style="color: null;">min</span>(i<span class="op" style="color: #5E5E5E;">+</span><span class="va" style="color: #111111;">self</span>.bs, <span class="bu" style="color: null;">len</span>(<span class="va" style="color: #111111;">self</span>.dataset))]</span>
<span id="cb98-8">    </span></code></pre></div>
</div>
<div class="cell" data-execution_count="64">
<div class="sourceCode cell-code" id="cb99" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb99-1"><span class="bu" style="color: null;">len</span>(train_ds)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="64">
<pre><code>50000</code></pre>
</div>
</div>
<div class="cell" data-execution_count="65">
<div class="sourceCode cell-code" id="cb101" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb101-1">train_dl <span class="op" style="color: #5E5E5E;">=</span> DataLoader(train_ds, bs)</span>
<span id="cb101-2">valid_dl <span class="op" style="color: #5E5E5E;">=</span> DataLoader(valid_ds, bs)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="66">
<div class="sourceCode cell-code" id="cb102" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb102-1">xb, yb <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">next</span>(<span class="bu" style="color: null;">iter</span>(train_dl))</span></code></pre></div>
</div>
<div class="cell" data-execution_count="67">
<div class="sourceCode cell-code" id="cb103" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb103-1">xb.shape, yb.shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="67">
<pre><code>(torch.Size([64, 784]), torch.Size([64]))</code></pre>
</div>
</div>
<div class="cell" data-execution_count="68">
<div class="sourceCode cell-code" id="cb105" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb105-1">plt.imshow(xb[<span class="dv" style="color: #AD0000;">0</span>].view(<span class="dv" style="color: #AD0000;">28</span>,<span class="dv" style="color: #AD0000;">28</span>))</span>
<span id="cb105-2">yb[<span class="dv" style="color: #AD0000;">0</span>]</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="68">
<pre><code>tensor(5)</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="https://fromLittleAcorns.github.io/john.richmond.github.io/posts/2022-11-30-fastai-course22p/04_dataloaders_optimisers_training_files/figure-html/cell-69-output-2.png" class="img-fluid"></p>
</div>
</div>
<p>Now implement a training loop with the dataloader</p>
<div class="cell" data-execution_count="69">
<div class="sourceCode cell-code" id="cb107" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb107-1">model, opt <span class="op" style="color: #5E5E5E;">=</span> get_model_and_optimizer()</span></code></pre></div>
</div>
<p>Now create a function for the fit process since this will be used several times</p>
<div class="cell" data-execution_count="70">
<div class="sourceCode cell-code" id="cb108" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb108-1"><span class="kw" style="color: #003B4F;">def</span> fit():</span>
<span id="cb108-2">    <span class="cf" style="color: #003B4F;">for</span> epoch <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(epochs):</span>
<span id="cb108-3">        <span class="cf" style="color: #003B4F;">for</span> xb, yb <span class="kw" style="color: #003B4F;">in</span> train_dl:</span>
<span id="cb108-4">            <span class="co" style="color: #5E5E5E;"># Pass data through the model</span></span>
<span id="cb108-5">            preds <span class="op" style="color: #5E5E5E;">=</span> model(xb)</span>
<span id="cb108-6">            loss <span class="op" style="color: #5E5E5E;">=</span> loss_func(preds, yb)</span>
<span id="cb108-7">            <span class="co" style="color: #5E5E5E;"># Calculate gradients</span></span>
<span id="cb108-8">            loss.backward()</span>
<span id="cb108-9">            <span class="co" style="color: #5E5E5E;"># Apply the optimiser</span></span>
<span id="cb108-10">            opt.step()</span>
<span id="cb108-11">            opt.zero_grad()</span>
<span id="cb108-12">        <span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f'After epoch </span><span class="sc" style="color: #5E5E5E;">{</span>epoch<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">, batch loss is: </span><span class="sc" style="color: #5E5E5E;">{</span>loss<span class="sc" style="color: #5E5E5E;">.</span>item()<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">'</span>)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="71">
<div class="sourceCode cell-code" id="cb109" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb109-1">fit()</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>After epoch 0, batch loss is: 0.3089807331562042
After epoch 1, batch loss is: 0.19600419700145721
After epoch 2, batch loss is: 0.10103154182434082</code></pre>
</div>
</div>
<div class="cell" data-execution_count="72">
<div class="sourceCode cell-code" id="cb111" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb111-1">loss_func(model(xb), yb), accuracy(model(xb), yb)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="72">
<pre><code>(tensor(0.17, grad_fn=&lt;NllLossBackward0&gt;), tensor(0.94))</code></pre>
</div>
</div>
<div class="cell" data-execution_count="73">
<div class="sourceCode cell-code" id="cb113" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb113-1">xb.shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="73">
<pre><code>torch.Size([64, 784])</code></pre>
</div>
</div>
</section>
</section>
</section>
<section id="adding-in-different-sampling-methods" class="level2">
<h2 class="anchored" data-anchor-id="adding-in-different-sampling-methods">Adding in different sampling methods</h2>
<p>To enable random or linear sampling a class is added to manage the sampling. This will now be developed. The inputs to the sampler need to be the dataset and a flag to indicate the type of sampling. The return should be the same dataset either shuffled or processed as necessary. This could be done upon just the item index values or the whole dataset could be processed and returned. In this case the item indexies are returned as a list as requested by the iter call</p>
<p>I’m not sure why yield is not used here - something to check up on</p>
<div class="cell" data-execution_count="74">
<div class="sourceCode cell-code" id="cb115" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb115-1"><span class="im" style="color: #00769E;">import</span> random</span></code></pre></div>
</div>
<div class="cell" data-execution_count="75">
<div class="sourceCode cell-code" id="cb116" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb116-1"><span class="kw" style="color: #003B4F;">class</span> Sampler():</span>
<span id="cb116-2">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, ds, shuffle<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span>):</span>
<span id="cb116-3">        <span class="va" style="color: #111111;">self</span>.n <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">len</span>(ds)</span>
<span id="cb116-4">        <span class="va" style="color: #111111;">self</span>.ds <span class="op" style="color: #5E5E5E;">=</span> ds</span>
<span id="cb116-5">        <span class="va" style="color: #111111;">self</span>.shuffle <span class="op" style="color: #5E5E5E;">=</span> shuffle</span>
<span id="cb116-6">    </span>
<span id="cb116-7">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__iter__</span>(<span class="va" style="color: #111111;">self</span>):</span>
<span id="cb116-8">        indecies <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">list</span>(<span class="bu" style="color: null;">range</span>(<span class="va" style="color: #111111;">self</span>.n))</span>
<span id="cb116-9">        <span class="cf" style="color: #003B4F;">if</span> <span class="va" style="color: #111111;">self</span>.shuffle:</span>
<span id="cb116-10">            random.shuffle(indecies)</span>
<span id="cb116-11">        <span class="cf" style="color: #003B4F;">return</span> <span class="bu" style="color: null;">iter</span>(indecies)</span>
<span id="cb116-12">        </span></code></pre></div>
</div>
<div class="cell" data-execution_count="76">
<div class="sourceCode cell-code" id="cb117" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb117-1"><span class="im" style="color: #00769E;">from</span> itertools <span class="im" style="color: #00769E;">import</span> islice</span></code></pre></div>
</div>
<div class="cell" data-execution_count="77">
<div class="sourceCode cell-code" id="cb118" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb118-1">sampler <span class="op" style="color: #5E5E5E;">=</span> Sampler(x_train, shuffle<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span>)</span>
<span id="cb118-2">iterator <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">iter</span>(sampler)</span>
<span id="cb118-3">ids <span class="op" style="color: #5E5E5E;">=</span> []</span>
<span id="cb118-4"><span class="cf" style="color: #003B4F;">for</span> i <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="dv" style="color: #AD0000;">5</span>): ids.append(<span class="bu" style="color: null;">next</span>(iterator))</span>
<span id="cb118-5">ids</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="77">
<pre><code>[0, 1, 2, 3, 4]</code></pre>
</div>
</div>
<p>This is returning things one item at a time from the iterator. It is possible to use islice to generate a range of items. Note that since this iterator has already returned five entries it will supply the next 5, it doesn’t start from to front of the list unless a new iterator is created</p>
<div class="cell" data-execution_count="78">
<div class="sourceCode cell-code" id="cb120" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb120-1">ids <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">list</span>(islice(iterator, <span class="dv" style="color: #AD0000;">0</span>, <span class="dv" style="color: #AD0000;">5</span>))</span>
<span id="cb120-2">ids</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="78">
<pre><code>[5, 6, 7, 8, 9]</code></pre>
</div>
</div>
<p>When the random flag is set to true then</p>
<div class="cell" data-execution_count="79">
<div class="sourceCode cell-code" id="cb122" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb122-1">sampler <span class="op" style="color: #5E5E5E;">=</span> Sampler(x_train, shuffle<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>)</span>
<span id="cb122-2"><span class="bu" style="color: null;">list</span>(islice(sampler, <span class="dv" style="color: #AD0000;">0</span>, <span class="dv" style="color: #AD0000;">10</span>))</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="79">
<pre><code>[19149, 3623, 33271, 45722, 34626, 44572, 33273, 31591, 1328, 44705]</code></pre>
</div>
</div>
<div class="cell" data-execution_count="80">
<div class="sourceCode cell-code" id="cb124" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb124-1">sampler_v <span class="op" style="color: #5E5E5E;">=</span> Sampler(x_valid, shuffle<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span>)</span>
<span id="cb124-2"><span class="bu" style="color: null;">list</span>(islice(sampler_v, <span class="dv" style="color: #AD0000;">0</span>, <span class="dv" style="color: #AD0000;">10</span>))</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="80">
<pre><code>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]</code></pre>
</div>
</div>
<p>Having proven the basics of the sampler the fastcore library will be used to create a batch sampler. Two fastcore methods are used: 1. store_attr. This simply saves any parameters supplied as class properties with the same name as the supplied parameter 2. chunked. This returns batches of indecies from an iterator of user defined size with the option to specity whether to drop the last chunk if not the batch size, and also to provide only a defined number of chunks (useful to use a part of a dataset when get things working)</p>
<div class="cell" data-execution_count="81">
<div class="sourceCode cell-code" id="cb126" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb126-1"><span class="im" style="color: #00769E;">import</span> fastcore.<span class="bu" style="color: null;">all</span> <span class="im" style="color: #00769E;">as</span> fc</span></code></pre></div>
</div>
<p>The class BatchSampler is require to take as input the dataset, batch size, the sampler, and whether to drop the last batch if not the correct size. In this case the dataset is already embedded into the sampler and hence does not need to be added separately. The output should be an iterator returning batches of indecies</p>
<div class="cell" data-execution_count="82">
<div class="sourceCode cell-code" id="cb127" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb127-1"><span class="kw" style="color: #003B4F;">class</span> BatchSampler:</span>
<span id="cb127-2">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, sampler, bs<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">16</span>, drop_last<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span>):</span>
<span id="cb127-3">        fc.store_attr()</span>
<span id="cb127-4">    </span>
<span id="cb127-5">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__iter__</span>(<span class="va" style="color: #111111;">self</span>, ):</span>
<span id="cb127-6">        <span class="cf" style="color: #003B4F;">yield</span> <span class="cf" style="color: #003B4F;">from</span> fc.chunked(<span class="bu" style="color: null;">iter</span>(<span class="va" style="color: #111111;">self</span>.sampler), chunk_sz<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">self</span>.bs, drop_last<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">self</span>.drop_last)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="83">
<div class="sourceCode cell-code" id="cb128" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb128-1">batches <span class="op" style="color: #5E5E5E;">=</span> BatchSampler(sampler, bs<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">4</span>, drop_last<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span>)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="84">
<div class="sourceCode cell-code" id="cb129" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb129-1"><span class="bu" style="color: null;">next</span>(<span class="bu" style="color: null;">iter</span>(batches))</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="84">
<pre><code>[25976, 3805, 2452, 7221]</code></pre>
</div>
</div>
<div class="cell" data-execution_count="85">
<div class="sourceCode cell-code" id="cb131" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb131-1"><span class="bu" style="color: null;">list</span>(islice(batches, <span class="dv" style="color: #AD0000;">5</span>))</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="85">
<pre><code>[[43889, 38760, 1660, 43812],
 [19474, 27881, 1648, 5994],
 [43292, 20760, 47927, 46678],
 [16207, 19001, 31035, 2216],
 [21505, 234, 47366, 36409]]</code></pre>
</div>
</div>
<p>BatchSampler will provide a list of ids with which to make up the batch. The Sampler will then return the individual input data and targets for each id. The then have to collate the inputs and targets into torch arrays for processing.</p>
<p>This collation and stacking of data is done by a collate function</p>
<p>We now need a collate method to take the samples and convert them into a stacked tensor or inputs and outputs</p>
<p>Collator inputs: list of tuples of input value pairs outputs: tuple of input and target values as torch tensors</p>
<div class="cell" data-execution_count="86">
<div class="sourceCode cell-code" id="cb133" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb133-1"><span class="kw" style="color: #003B4F;">def</span> collate(b):</span>
<span id="cb133-2">    <span class="co" style="color: #5E5E5E;"># the * means that multiple items will be received, which effectively means the list is taken an item</span></span>
<span id="cb133-3">    <span class="co" style="color: #5E5E5E;"># at a time</span></span>
<span id="cb133-4">    xb, yb <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">zip</span>(<span class="op" style="color: #5E5E5E;">*</span>b)</span>
<span id="cb133-5">    <span class="cf" style="color: #003B4F;">return</span> (torch.stack(xb), torch.stack(yb))</span>
<span id="cb133-6">    </span></code></pre></div>
</div>
<div class="cell" data-execution_count="87">
<div class="sourceCode cell-code" id="cb134" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb134-1">bi <span class="op" style="color: #5E5E5E;">=</span> [train_ds[<span class="dv" style="color: #AD0000;">0</span>], train_ds[<span class="dv" style="color: #AD0000;">1</span>], train_ds[<span class="dv" style="color: #AD0000;">2</span>]]</span>
<span id="cb134-2">collate(bi)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="87">
<pre><code>(tensor([[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]),
 tensor([5, 0, 4]))</code></pre>
</div>
</div>
<section id="finally-create-a-full-dataloader-using-the-above-components" class="level3">
<h3 class="anchored" data-anchor-id="finally-create-a-full-dataloader-using-the-above-components">Finally create a full dataloader using the above components</h3>
<p>inputs:dataset, batch size, sampling method to use, whether to use last batch</p>
<p>outputs: tuple of stacked array of inputs and targets</p>
<p>methods: initiation - setup the dataset, sampling method, batch size, options iter - return a batch of data as a tuple</p>
<div class="cell" data-execution_count="88">
<div class="sourceCode cell-code" id="cb136" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb136-1"><span class="kw" style="color: #003B4F;">class</span> DataLoader():</span>
<span id="cb136-2">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, dataset, collate_fn<span class="op" style="color: #5E5E5E;">=</span>collate, bs<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">64</span>, shuffle<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span>, drop_last<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span>):</span>
<span id="cb136-3">        fc.store_attr()</span>
<span id="cb136-4">        sampler <span class="op" style="color: #5E5E5E;">=</span> Sampler(ds<span class="op" style="color: #5E5E5E;">=</span>dataset, shuffle<span class="op" style="color: #5E5E5E;">=</span>shuffle)</span>
<span id="cb136-5">        <span class="va" style="color: #111111;">self</span>.batch_sampler <span class="op" style="color: #5E5E5E;">=</span> BatchSampler(sampler, bs, drop_last)</span>
<span id="cb136-6">        </span>
<span id="cb136-7">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__iter__</span>(<span class="va" style="color: #111111;">self</span>):</span>
<span id="cb136-8">        <span class="cf" style="color: #003B4F;">yield</span> <span class="cf" style="color: #003B4F;">from</span> (<span class="va" style="color: #111111;">self</span>.collate_fn(<span class="va" style="color: #111111;">self</span>.dataset[i] <span class="cf" style="color: #003B4F;">for</span> i <span class="kw" style="color: #003B4F;">in</span> b) <span class="cf" style="color: #003B4F;">for</span> b <span class="kw" style="color: #003B4F;">in</span> <span class="va" style="color: #111111;">self</span>.batch_sampler)</span>
<span id="cb136-9">        </span></code></pre></div>
</div>
<div class="cell" data-execution_count="89">
<div class="sourceCode cell-code" id="cb137" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb137-1">train_dl <span class="op" style="color: #5E5E5E;">=</span> DataLoader(train_ds, collate, bs<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">64</span>, shuffle<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>)</span>
<span id="cb137-2">valid_dl <span class="op" style="color: #5E5E5E;">=</span> DataLoader(valid_ds, collate, bs<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">64</span>, shuffle<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span>)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="90">
<div class="sourceCode cell-code" id="cb138" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb138-1">train_batch <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">next</span>(<span class="bu" style="color: null;">iter</span>(train_dl))</span>
<span id="cb138-2">train_batch[<span class="dv" style="color: #AD0000;">0</span>].shape, train_batch[<span class="dv" style="color: #AD0000;">1</span>].shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="90">
<pre><code>(torch.Size([64, 784]), torch.Size([64]))</code></pre>
</div>
</div>
<div class="cell" data-execution_count="91">
<div class="sourceCode cell-code" id="cb140" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb140-1">xb,yb <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">next</span>(<span class="bu" style="color: null;">iter</span>(valid_dl))</span>
<span id="cb140-2">xb.shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="91">
<pre><code>torch.Size([64, 784])</code></pre>
</div>
</div>
<div class="cell" data-execution_count="92">
<div class="sourceCode cell-code" id="cb142" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb142-1">xb,yb <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">next</span>(<span class="bu" style="color: null;">iter</span>(valid_dl))</span>
<span id="cb142-2">plt.imshow(xb[<span class="dv" style="color: #AD0000;">0</span>].view(<span class="dv" style="color: #AD0000;">28</span>,<span class="dv" style="color: #AD0000;">28</span>))</span>
<span id="cb142-3">yb[<span class="dv" style="color: #AD0000;">0</span>]</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="92">
<pre><code>tensor(3)</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="https://fromLittleAcorns.github.io/john.richmond.github.io/posts/2022-11-30-fastai-course22p/04_dataloaders_optimisers_training_files/figure-html/cell-93-output-2.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="93">
<div class="sourceCode cell-code" id="cb144" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb144-1"><span class="co" style="color: #5E5E5E;"># Check the accuracy of the model when trained with the Dataloader</span></span>
<span id="cb144-2">model,opt <span class="op" style="color: #5E5E5E;">=</span> get_model_and_optimizer()</span>
<span id="cb144-3">fit()</span>
<span id="cb144-4"></span>
<span id="cb144-5">loss_func(model(xb), yb), accuracy(model(xb), yb)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>After epoch 0, batch loss is: 0.05507710948586464
After epoch 1, batch loss is: 0.055910542607307434
After epoch 2, batch loss is: 0.07595346868038177</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="93">
<pre><code>(tensor(0.10, grad_fn=&lt;NllLossBackward0&gt;), tensor(0.97))</code></pre>
</div>
</div>
</section>
</section>
<section id="multiprocesssing-dataloader" class="level2">
<h2 class="anchored" data-anchor-id="multiprocesssing-dataloader">Multiprocesssing DataLoader</h2>
<p>Data loading is often a constraint in terms or time to process a job. Fortunately this is a task that lends itself to multi-processing and hence it makes sense to apply this.</p>
<p>Pytorch provide a multi-processing library and that is what will be used here.</p>
<p>The way it will be done is that the above library will provide a pool or workers, the number of which can be defined. Each worker can then be asked to load a batch of data. The batches returned will be returned as requested</p>
<div class="cell" data-execution_count="94">
<div class="sourceCode cell-code" id="cb147" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb147-1"><span class="im" style="color: #00769E;">import</span> torch.multiprocessing <span class="im" style="color: #00769E;">as</span> mp</span>
<span id="cb147-2"><span class="im" style="color: #00769E;">from</span> fastcore.basics <span class="im" style="color: #00769E;">import</span> store_attr</span></code></pre></div>
</div>
<p>The way a dataset returns values based upon the index uses the getitem dunder as can be seen here</p>
<div class="cell" data-execution_count="95">
<div class="sourceCode cell-code" id="cb148" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb148-1">train_ds.<span class="fu" style="color: #4758AB;">__getitem__</span>([[<span class="dv" style="color: #AD0000;">3</span>,<span class="dv" style="color: #AD0000;">5</span>,<span class="dv" style="color: #AD0000;">8</span>,<span class="dv" style="color: #AD0000;">10</span>]])</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="95">
<pre><code>(tensor([[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]),
 tensor([1, 2, 1, 3]))</code></pre>
</div>
</div>
<div class="cell" data-execution_count="96">
<div class="sourceCode cell-code" id="cb150" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb150-1">train_ds.<span class="fu" style="color: #4758AB;">__getitem__</span>([<span class="dv" style="color: #AD0000;">3</span>,<span class="dv" style="color: #AD0000;">5</span>,<span class="dv" style="color: #AD0000;">8</span>,<span class="dv" style="color: #AD0000;">10</span>])</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="96">
<pre><code>(tensor([[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]),
 tensor([1, 2, 1, 3]))</code></pre>
</div>
</div>
<p>It is possible to use the map function to split individual groups of items as below</p>
<div class="cell" data-execution_count="97">
<div class="sourceCode cell-code" id="cb152" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb152-1"><span class="cf" style="color: #003B4F;">for</span> o <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">map</span>(train_ds.<span class="fu" style="color: #4758AB;">__getitem__</span>, [[<span class="dv" style="color: #AD0000;">3</span>,<span class="dv" style="color: #AD0000;">5</span>], [<span class="dv" style="color: #AD0000;">8</span>,<span class="dv" style="color: #AD0000;">10</span>]]):</span>
<span id="cb152-2">    <span class="bu" style="color: null;">print</span>(o)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([1, 2]))
(tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([1, 3]))</code></pre>
</div>
</div>
<p>The dataloader can then be modified so that each worker in a pool loads a batch and return it when requested. Note that it appears that the collate function is not needed here since the way in which the batch sampler passes a list of indecies results in an array of values being returned.</p>
<div class="cell" data-execution_count="98">
<div class="sourceCode cell-code" id="cb154" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb154-1"><span class="kw" style="color: #003B4F;">class</span> DataLoader():</span>
<span id="cb154-2">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, dataset, bs<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">64</span>, shuffle<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span>, drop_last<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span>, num_workers<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>):</span>
<span id="cb154-3">        fc.store_attr()</span>
<span id="cb154-4">        sampler <span class="op" style="color: #5E5E5E;">=</span> Sampler(ds<span class="op" style="color: #5E5E5E;">=</span>dataset, shuffle<span class="op" style="color: #5E5E5E;">=</span>shuffle)</span>
<span id="cb154-5">        <span class="va" style="color: #111111;">self</span>.batch_sampler <span class="op" style="color: #5E5E5E;">=</span> BatchSampler(sampler, bs, drop_last)</span>
<span id="cb154-6">        </span>
<span id="cb154-7">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__iter__</span>(<span class="va" style="color: #111111;">self</span>):</span>
<span id="cb154-8">        <span class="cf" style="color: #003B4F;">with</span> mp.Pool(<span class="va" style="color: #111111;">self</span>.num_workers) <span class="im" style="color: #00769E;">as</span> ex:</span>
<span id="cb154-9">            <span class="cf" style="color: #003B4F;">yield</span> <span class="cf" style="color: #003B4F;">from</span> ex.<span class="bu" style="color: null;">map</span>(<span class="va" style="color: #111111;">self</span>.dataset.<span class="fu" style="color: #4758AB;">__getitem__</span>, <span class="bu" style="color: null;">iter</span>(<span class="va" style="color: #111111;">self</span>.batch_sampler))</span>
<span id="cb154-10"></span>
<span id="cb154-11">        </span></code></pre></div>
</div>
<div class="cell" data-execution_count="99">
<div class="sourceCode cell-code" id="cb155" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb155-1">train_dl <span class="op" style="color: #5E5E5E;">=</span> DataLoader(train_ds, bs<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">64</span>, shuffle<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>, num_workers<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">2</span>)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="100">
<div class="sourceCode cell-code" id="cb156" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb156-1">it <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">iter</span>(train_dl)</span>
<span id="cb156-2">res <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">next</span>(it)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="101">
<div class="sourceCode cell-code" id="cb157" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb157-1">xb, yb <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">next</span>(it)</span>
<span id="cb157-2">xb.shape, yb.shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="101">
<pre><code>(torch.Size([64, 784]), torch.Size([64]))</code></pre>
</div>
</div>
<div class="cell" data-execution_count="102">
<div class="sourceCode cell-code" id="cb159" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb159-1"><span class="bu" style="color: null;">len</span>(res),res[<span class="dv" style="color: #AD0000;">0</span>].shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="102">
<pre><code>(2, torch.Size([64, 784]))</code></pre>
</div>
</div>
</section>
<section id="pytorch-dataloader" class="level2">
<h2 class="anchored" data-anchor-id="pytorch-dataloader">Pytorch DataLoader</h2>
<div class="cell" data-execution_count="103">
<div class="sourceCode cell-code" id="cb161" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb161-1"><span class="im" style="color: #00769E;">from</span> torch.utils.data <span class="im" style="color: #00769E;">import</span> DataLoader, SequentialSampler, RandomSampler, BatchSampler</span></code></pre></div>
</div>
<p>Steps to follow: 1. Create batch sampler 2. create dataloader (with multi worker) 3. Train model entirely using PyTorch 4.</p>
<div class="cell" data-execution_count="104">
<div class="sourceCode cell-code" id="cb162" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb162-1">train_bs <span class="op" style="color: #5E5E5E;">=</span> BatchSampler(RandomSampler(train_ds), batch_size<span class="op" style="color: #5E5E5E;">=</span>bs, drop_last<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span>)</span>
<span id="cb162-2">valid_bs <span class="op" style="color: #5E5E5E;">=</span> BatchSampler(SequentialSampler(valid_ds), batch_size<span class="op" style="color: #5E5E5E;">=</span>bs, drop_last<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span>)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="105">
<div class="sourceCode cell-code" id="cb163" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb163-1"><span class="co" style="color: #5E5E5E;"># In this case the collate function is not required</span></span>
<span id="cb163-2">train_dl <span class="op" style="color: #5E5E5E;">=</span> DataLoader(train_ds, batch_sampler<span class="op" style="color: #5E5E5E;">=</span>train_bs, num_workers<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">4</span>, collate_fn<span class="op" style="color: #5E5E5E;">=</span>collate)</span>
<span id="cb163-3">valid_dl <span class="op" style="color: #5E5E5E;">=</span> DataLoader(valid_ds, batch_sampler<span class="op" style="color: #5E5E5E;">=</span>valid_bs, num_workers<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">4</span>)</span></code></pre></div>
</div>
<p>In this case the collate function is not necessary as the dataset will already do this, as was shown above by the way a list of items returns stacked arrays. Allowing Pytorch to autogenerate teh samplers as well then this can all be simplified to</p>
<div class="cell" data-execution_count="106">
<div class="sourceCode cell-code" id="cb164" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb164-1">train_dl <span class="op" style="color: #5E5E5E;">=</span> DataLoader(train_ds, batch_size<span class="op" style="color: #5E5E5E;">=</span>bs, shuffle<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>, drop_last<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>, num_workers<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">4</span>)</span>
<span id="cb164-2">valid_dl <span class="op" style="color: #5E5E5E;">=</span> DataLoader(valid_ds, batch_size<span class="op" style="color: #5E5E5E;">=</span>bs, shuffle<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span>, drop_last<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span>, num_workers<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">4</span>)</span></code></pre></div>
</div>
<p>Check accuracy as usual</p>
<div class="cell" data-execution_count="107">
<div class="sourceCode cell-code" id="cb165" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb165-1">model,opt <span class="op" style="color: #5E5E5E;">=</span> get_model_and_optimizer()</span>
<span id="cb165-2">fit()</span>
<span id="cb165-3"></span>
<span id="cb165-4">loss_func(model(xb), yb), accuracy(model(xb), yb)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>After epoch 0, batch loss is: 0.3000752925872803
After epoch 1, batch loss is: 0.034155331552028656
After epoch 2, batch loss is: 0.07146771252155304</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="107">
<pre><code>(tensor(0.12, grad_fn=&lt;NllLossBackward0&gt;), tensor(0.97))</code></pre>
</div>
</div>
</section>
<section id="validation" class="level2">
<h2 class="anchored" data-anchor-id="validation">Validation</h2>
<p>It is good (essential) practice to have a validation set and to check the accuracu of the model periodically, such as at the end of each epoch.</p>
<p>Before calling the validation dataset it is necessary to put the model into eval mode, which avoids issues with batchnorm and dropout layers, where different setting are used for training</p>
<p>Create an update fit routine which calculates and prints out loss and accuracy at the end of each epoch. As input define the number of epochs, the model to use, the loss function, the optimiser and the train and test dataloasers</p>
<div class="cell" data-execution_count="108">
<div class="sourceCode cell-code" id="cb168" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb168-1"><span class="kw" style="color: #003B4F;">def</span> fit(epochs, model, loss_func, opt, train_dl, valid_dl):</span>
<span id="cb168-2">    <span class="co" style="color: #5E5E5E;"># Loop through the epochs</span></span>
<span id="cb168-3">    <span class="cf" style="color: #003B4F;">for</span> epoch <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(epochs):</span>
<span id="cb168-4">        <span class="co" style="color: #5E5E5E;"># Set model into training mode</span></span>
<span id="cb168-5">        model.train()</span>
<span id="cb168-6">        <span class="co" style="color: #5E5E5E;">#iterate through each batch</span></span>
<span id="cb168-7">        <span class="cf" style="color: #003B4F;">for</span> xb, yb <span class="kw" style="color: #003B4F;">in</span> train_dl:</span>
<span id="cb168-8">            preds <span class="op" style="color: #5E5E5E;">=</span> model(xb)</span>
<span id="cb168-9">            loss <span class="op" style="color: #5E5E5E;">=</span> loss_func(preds, yb)</span>
<span id="cb168-10">            loss.backward()</span>
<span id="cb168-11">            opt.step()</span>
<span id="cb168-12">            opt.zero_grad()</span>
<span id="cb168-13">        </span>
<span id="cb168-14">        <span class="co" style="color: #5E5E5E;"># Set model to eval mode for validation</span></span>
<span id="cb168-15">        model.<span class="bu" style="color: null;">eval</span>()</span>
<span id="cb168-16">        <span class="co" style="color: #5E5E5E;"># Run without grad calculations for validation (speed up model and use less memory)</span></span>
<span id="cb168-17">        <span class="cf" style="color: #003B4F;">with</span> torch.no_grad():</span>
<span id="cb168-18">            <span class="co" style="color: #5E5E5E;"># Reset the loss, accuracy and input count totals so that they can be used to sum values over the whole ds</span></span>
<span id="cb168-19">            total_loss <span class="op" style="color: #5E5E5E;">=</span> <span class="fl" style="color: #AD0000;">0.</span><span class="op" style="color: #5E5E5E;">;</span> total_acc<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.</span><span class="op" style="color: #5E5E5E;">;</span> total_count<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">0</span></span>
<span id="cb168-20">            <span class="cf" style="color: #003B4F;">for</span> xb, yb <span class="kw" style="color: #003B4F;">in</span> valid_dl:</span>
<span id="cb168-21">                n_items <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">len</span>(yb)</span>
<span id="cb168-22">                total_count <span class="op" style="color: #5E5E5E;">+=</span> n_items</span>
<span id="cb168-23">                preds <span class="op" style="color: #5E5E5E;">=</span> model(xb)</span>
<span id="cb168-24">                total_loss <span class="op" style="color: #5E5E5E;">+=</span> loss_func(preds, yb)<span class="op" style="color: #5E5E5E;">*</span>n_items</span>
<span id="cb168-25">                total_acc <span class="op" style="color: #5E5E5E;">+=</span> (torch.topk(preds, <span class="dv" style="color: #AD0000;">1</span>)[<span class="dv" style="color: #AD0000;">1</span>][:,<span class="dv" style="color: #AD0000;">0</span>]<span class="op" style="color: #5E5E5E;">==</span>yb).<span class="bu" style="color: null;">sum</span>()</span>
<span id="cb168-26">            <span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f"Epoch: </span><span class="sc" style="color: #5E5E5E;">{</span>epoch<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">, loss: </span><span class="sc" style="color: #5E5E5E;">{</span>total_loss<span class="op" style="color: #5E5E5E;">/</span>total_count<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">, acc: </span><span class="sc" style="color: #5E5E5E;">{</span>total_acc<span class="op" style="color: #5E5E5E;">/</span>total_count<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">"</span>)</span>
<span id="cb168-27">    </span></code></pre></div>
</div>
<div class="cell" data-execution_count="109">
<div class="sourceCode cell-code" id="cb169" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb169-1">model,opt <span class="op" style="color: #5E5E5E;">=</span> get_model_and_optimizer()</span></code></pre></div>
</div>
<div class="cell" data-execution_count="110">
<div class="sourceCode cell-code" id="cb170" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb170-1">fit(epochs<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">3</span>, model<span class="op" style="color: #5E5E5E;">=</span>model, loss_func<span class="op" style="color: #5E5E5E;">=</span>loss_func, opt<span class="op" style="color: #5E5E5E;">=</span>opt, train_dl<span class="op" style="color: #5E5E5E;">=</span>train_dl, valid_dl<span class="op" style="color: #5E5E5E;">=</span>valid_dl)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch: 0, loss: 0.16384194791316986, acc: 0.9517999887466431
Epoch: 1, loss: 0.11577688157558441, acc: 0.963699996471405
Epoch: 2, loss: 0.11353033781051636, acc: 0.9666000008583069</code></pre>
</div>
</div>
<p>Finally create the dataloader using a function and then simplify the whole training process to three lines</p>
<div class="cell" data-execution_count="111">
<div class="sourceCode cell-code" id="cb172" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb172-1"><span class="kw" style="color: #003B4F;">def</span> get_data_loaderers(train_ds, valid_ds, bs):</span>
<span id="cb172-2">    train_dl <span class="op" style="color: #5E5E5E;">=</span> DataLoader(train_ds, batch_size<span class="op" style="color: #5E5E5E;">=</span>bs, shuffle<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>)</span>
<span id="cb172-3">    valid_ds <span class="op" style="color: #5E5E5E;">=</span> DataLoader(valid_ds, batch_size<span class="op" style="color: #5E5E5E;">=</span>bs, shuffle<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span>)</span>
<span id="cb172-4">    <span class="cf" style="color: #003B4F;">return</span> train_dl, valid_dl</span></code></pre></div>
</div>
<div class="cell" data-execution_count="112">
<div class="sourceCode cell-code" id="cb173" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb173-1">model, opt <span class="op" style="color: #5E5E5E;">=</span> get_model_and_optimizer()</span>
<span id="cb173-2">train_dl, valid_dl <span class="op" style="color: #5E5E5E;">=</span> get_data_loaderers(train_ds, valid_ds, bs<span class="op" style="color: #5E5E5E;">=</span>bs)</span>
<span id="cb173-3">fit(epochs<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">3</span>, model<span class="op" style="color: #5E5E5E;">=</span>model, loss_func<span class="op" style="color: #5E5E5E;">=</span>loss_func, opt<span class="op" style="color: #5E5E5E;">=</span>opt, train_dl<span class="op" style="color: #5E5E5E;">=</span>train_dl, valid_dl<span class="op" style="color: #5E5E5E;">=</span>valid_dl)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch: 0, loss: 0.20603491365909576, acc: 0.9343000054359436
Epoch: 1, loss: 0.20518435537815094, acc: 0.9363999962806702
Epoch: 2, loss: 0.1369418054819107, acc: 0.9598000049591064</code></pre>
</div>
</div>


</section>
</section>

 ]]></description>
  <guid>https://fromLittleAcorns.github.io/john.richmond.github.io/posts/2022-11-30-fastai-course22p/04_dataloaders_optimisers_training.html</guid>
  <pubDate>Sun, 27 Nov 2022 14:37:18 GMT</pubDate>
</item>
<item>
  <title>Notebook to explore DiffEdit</title>
  <dc:creator>John Richmond</dc:creator>
  <link>https://fromLittleAcorns.github.io/john.richmond.github.io/posts/2022-11-10-StableDiffusion/2022-11-11-diffedit_explore.html</link>
  <description><![CDATA[ 



<section id="notebook-to-try-out-the-diffedit-methodology" class="level2">
<h2 class="anchored" data-anchor-id="notebook-to-try-out-the-diffedit-methodology">Notebook to try out the DiffEdit methodology</h2>
<p>This follows the paper: http://arxiv.org/abs/2210.11427</p>
<p>The first part of the notebook is setup to generate image masks based upon the differences in images generated by starting with the same noised image and denoising it with two different prompts, the first one the prompt that goes with the image, the second a prompt related to a “target” image, which is what it is desired to change some aspect of the image into.</p>
<p>The methodology in the paper is not very well described and so a few alternative approaches are considered</p>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;">from</span> pathlib <span class="im" style="color: #00769E;">import</span> Path</span>
<span id="cb1-2"><span class="im" style="color: #00769E;">from</span> PIL <span class="im" style="color: #00769E;">import</span> Image</span>
<span id="cb1-3"><span class="im" style="color: #00769E;">from</span> tqdm.auto <span class="im" style="color: #00769E;">import</span> tqdm</span>
<span id="cb1-4"><span class="im" style="color: #00769E;">import</span> numpy <span class="im" style="color: #00769E;">as</span> np</span>
<span id="cb1-5"><span class="im" style="color: #00769E;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;">as</span> plt</span>
<span id="cb1-6"></span>
<span id="cb1-7"><span class="im" style="color: #00769E;">import</span> torch</span>
<span id="cb1-8"><span class="im" style="color: #00769E;">from</span> torchvision <span class="im" style="color: #00769E;">import</span> transforms <span class="im" style="color: #00769E;">as</span> tfms</span>
<span id="cb1-9"><span class="im" style="color: #00769E;">from</span> torch <span class="im" style="color: #00769E;">import</span> autocast</span>
<span id="cb1-10"></span>
<span id="cb1-11"><span class="im" style="color: #00769E;">from</span> diffusers <span class="im" style="color: #00769E;">import</span> DDIMScheduler, LMSDiscreteScheduler</span>
<span id="cb1-12"><span class="im" style="color: #00769E;">from</span> transformers <span class="im" style="color: #00769E;">import</span> CLIPTextModel, CLIPTokenizer</span>
<span id="cb1-13"><span class="im" style="color: #00769E;">from</span> diffusers <span class="im" style="color: #00769E;">import</span> AutoencoderKL, UNet2DConditionModel</span>
<span id="cb1-14"><span class="im" style="color: #00769E;">from</span> transformers <span class="im" style="color: #00769E;">import</span> logging</span></code></pre></div>
</div>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="co" style="color: #5E5E5E;">#!pip install accelerate</span></span></code></pre></div>
</div>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="co" style="color: #5E5E5E;"># Note that this step is helpful to avoid verbose warnings when loading the text encoder</span></span>
<span id="cb3-2">logging.set_verbosity_error()</span></code></pre></div>
</div>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="co" style="color: #5E5E5E;"># Set device</span></span>
<span id="cb4-2">torch_device <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"cuda"</span> <span class="cf" style="color: #003B4F;">if</span> torch.cuda.is_available() <span class="cf" style="color: #003B4F;">else</span> <span class="st" style="color: #20794D;">"cpu"</span></span></code></pre></div>
</div>
<div class="cell" data-tags="[]" data-execution_count="5">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="co" style="color: #5E5E5E;"># Load the tokenizer and text encoder</span></span>
<span id="cb5-2">tokenizer <span class="op" style="color: #5E5E5E;">=</span> CLIPTokenizer.from_pretrained(<span class="st" style="color: #20794D;">"openai/clip-vit-large-patch14"</span>, torch_dtype<span class="op" style="color: #5E5E5E;">=</span>torch.float16)</span>
<span id="cb5-3">text_encoder <span class="op" style="color: #5E5E5E;">=</span> CLIPTextModel.from_pretrained(<span class="st" style="color: #20794D;">"openai/clip-vit-large-patch14"</span>, torch_dtype<span class="op" style="color: #5E5E5E;">=</span>torch.float16).to(torch_device)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="co" style="color: #5E5E5E;"># Load the VAE and Unet</span></span>
<span id="cb6-2">vae <span class="op" style="color: #5E5E5E;">=</span> AutoencoderKL.from_pretrained(<span class="st" style="color: #20794D;">"stabilityai/sd-vae-ft-ema"</span>, torch_dtype<span class="op" style="color: #5E5E5E;">=</span>torch.float16).to(torch_device)</span>
<span id="cb6-3">unet <span class="op" style="color: #5E5E5E;">=</span> UNet2DConditionModel.from_pretrained(<span class="st" style="color: #20794D;">"CompVis/stable-diffusion-v1-4"</span>, subfolder<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"unet"</span>, torch_dtype<span class="op" style="color: #5E5E5E;">=</span>torch.float16).to(torch_device)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><span class="co" style="color: #5E5E5E;"># Create a DDIM scheduler</span></span>
<span id="cb7-2">ddim_sched <span class="op" style="color: #5E5E5E;">=</span> DDIMScheduler(beta_start<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.00085</span>, </span>
<span id="cb7-3">                                    beta_end<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.012</span>, </span>
<span id="cb7-4">                                    beta_schedule<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'scaled_linear'</span>, </span>
<span id="cb7-5">                                    clip_sample<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span>, </span>
<span id="cb7-6">                                    set_alpha_to_one<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span>)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><span class="co" style="color: #5E5E5E;"># Create a LMS scheduler</span></span>
<span id="cb8-2">scheduler <span class="op" style="color: #5E5E5E;">=</span> LMSDiscreteScheduler(beta_start<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.00085</span>, beta_end<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.012</span>, beta_schedule<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"scaled_linear"</span>, num_train_timesteps<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1000</span>)</span></code></pre></div>
</div>
</section>
<section id="add-functions" class="level2">
<h2 class="anchored" data-anchor-id="add-functions">Add Functions</h2>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><span class="kw" style="color: #003B4F;">def</span> prompt_to_embedding(prompt: <span class="bu" style="color: null;">str</span>, torch_device):</span>
<span id="cb9-2">    text_input <span class="op" style="color: #5E5E5E;">=</span> tokenizer(prompt, padding<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"max_length"</span>, max_length<span class="op" style="color: #5E5E5E;">=</span>tokenizer.model_max_length, truncation<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>, return_tensors<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"pt"</span>)</span>
<span id="cb9-3">    <span class="cf" style="color: #003B4F;">with</span> torch.no_grad():</span>
<span id="cb9-4">        embeddings <span class="op" style="color: #5E5E5E;">=</span> text_encoder(text_input.input_ids.to(torch_device))[<span class="dv" style="color: #AD0000;">0</span>]</span>
<span id="cb9-5">    <span class="cf" style="color: #003B4F;">return</span> embeddings</span></code></pre></div>
</div>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><span class="kw" style="color: #003B4F;">def</span> pil_to_latent(input_im):</span>
<span id="cb10-2">    <span class="co" style="color: #5E5E5E;"># Single image -&gt; single latent in a batch (so size 1, 4, 64, 64)</span></span>
<span id="cb10-3">    <span class="cf" style="color: #003B4F;">with</span> torch.no_grad():</span>
<span id="cb10-4">        latent <span class="op" style="color: #5E5E5E;">=</span> vae.encode(tfms.ToTensor()(input_im).unsqueeze(<span class="dv" style="color: #AD0000;">0</span>).half().to(torch_device)<span class="op" style="color: #5E5E5E;">*</span><span class="dv" style="color: #AD0000;">2</span><span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>) <span class="co" style="color: #5E5E5E;"># Note scaling</span></span>
<span id="cb10-5">    <span class="cf" style="color: #003B4F;">return</span> <span class="fl" style="color: #AD0000;">0.18215</span> <span class="op" style="color: #5E5E5E;">*</span> latent.latent_dist.sample()</span></code></pre></div>
</div>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><span class="kw" style="color: #003B4F;">def</span> latents_to_array(latents):</span>
<span id="cb11-2">    latents <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">1</span> <span class="op" style="color: #5E5E5E;">/</span> <span class="fl" style="color: #AD0000;">0.18215</span> <span class="op" style="color: #5E5E5E;">*</span> latents</span>
<span id="cb11-3">    <span class="cf" style="color: #003B4F;">with</span> torch.no_grad():</span>
<span id="cb11-4">        image <span class="op" style="color: #5E5E5E;">=</span> vae.decode(latents).sample</span>
<span id="cb11-5"></span>
<span id="cb11-6">    <span class="co" style="color: #5E5E5E;"># Create image array</span></span>
<span id="cb11-7">    image <span class="op" style="color: #5E5E5E;">=</span> (image <span class="op" style="color: #5E5E5E;">/</span> <span class="dv" style="color: #AD0000;">2</span> <span class="op" style="color: #5E5E5E;">+</span> <span class="fl" style="color: #AD0000;">0.5</span>).clamp(<span class="dv" style="color: #AD0000;">0</span>, <span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb11-8">    image <span class="op" style="color: #5E5E5E;">=</span> image.detach().cpu().permute(<span class="dv" style="color: #AD0000;">0</span>, <span class="dv" style="color: #AD0000;">2</span>, <span class="dv" style="color: #AD0000;">3</span>, <span class="dv" style="color: #AD0000;">1</span>).numpy()</span>
<span id="cb11-9">    images <span class="op" style="color: #5E5E5E;">=</span> (image <span class="op" style="color: #5E5E5E;">*</span> <span class="dv" style="color: #AD0000;">255</span>).<span class="bu" style="color: null;">round</span>().astype(<span class="st" style="color: #20794D;">"uint8"</span>)</span>
<span id="cb11-10">    <span class="co" style="color: #5E5E5E;"># At this point, this is a single-item array of image data, so return only the item </span></span>
<span id="cb11-11">    <span class="co" style="color: #5E5E5E;"># to remove the extra diemension from the returned data</span></span>
<span id="cb11-12">    <span class="cf" style="color: #003B4F;">return</span> images[<span class="dv" style="color: #AD0000;">0</span>]</span></code></pre></div>
</div>
<div class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><span class="kw" style="color: #003B4F;">def</span> latents_to_pil(latents):</span>
<span id="cb12-2">    <span class="co" style="color: #5E5E5E;"># bath of latents -&gt; list of images</span></span>
<span id="cb12-3">    image <span class="op" style="color: #5E5E5E;">=</span> latents_to_array(latents)</span>
<span id="cb12-4">    pil_imagea <span class="op" style="color: #5E5E5E;">=</span> Image.fromarray(image)</span>
<span id="cb12-5">    <span class="cf" style="color: #003B4F;">return</span> pil_imagea</span></code></pre></div>
</div>
<div class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><span class="kw" style="color: #003B4F;">def</span> show_latents(latents):</span>
<span id="cb13-2">    fig, axs <span class="op" style="color: #5E5E5E;">=</span> plt.subplots(<span class="dv" style="color: #AD0000;">1</span>, <span class="dv" style="color: #AD0000;">4</span>, figsize<span class="op" style="color: #5E5E5E;">=</span>(<span class="dv" style="color: #AD0000;">16</span>, <span class="dv" style="color: #AD0000;">4</span>))</span>
<span id="cb13-3">    <span class="cf" style="color: #003B4F;">for</span> c <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="dv" style="color: #AD0000;">4</span>):</span>
<span id="cb13-4">        axs[c].imshow(latents[<span class="dv" style="color: #AD0000;">0</span>][c].cpu(), cmap<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'Greys'</span>)</span>
<span id="cb13-5">        axs[c].axis(<span class="st" style="color: #20794D;">'off'</span>)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><span class="kw" style="color: #003B4F;">def</span> load_image(path_to_image, size):</span>
<span id="cb14-2">    path_to_img <span class="op" style="color: #5E5E5E;">=</span> Path(path_to_image)</span>
<span id="cb14-3">    <span class="cf" style="color: #003B4F;">assert</span> path_to_img.is_file(), <span class="ss" style="color: #20794D;">f"No file found </span><span class="sc" style="color: #5E5E5E;">{</span>path_to_image<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">"</span></span>
<span id="cb14-4">    image <span class="op" style="color: #5E5E5E;">=</span> Image.<span class="bu" style="color: null;">open</span>(path_to_img).convert(<span class="st" style="color: #20794D;">'RGB'</span>)</span>
<span id="cb14-5">    <span class="cf" style="color: #003B4F;">return</span> image</span></code></pre></div>
</div>
<div class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><span class="kw" style="color: #003B4F;">def</span> denoising_loop(latents, text_emb, scheduler, g<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">7.5</span>, strength<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.5</span>, steps<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">50</span>, dim<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">512</span>, start_step<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">10</span>, torch_device<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"cuda"</span>):</span>
<span id="cb15-2">    <span class="cf" style="color: #003B4F;">with</span> autocast(torch_device):</span>
<span id="cb15-3">        noise_preds <span class="op" style="color: #5E5E5E;">=</span> torch.tensor([], device<span class="op" style="color: #5E5E5E;">=</span>torch_device)</span>
<span id="cb15-4">        <span class="cf" style="color: #003B4F;">for</span> i, t <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">enumerate</span>(scheduler.timesteps):</span>
<span id="cb15-5">            <span class="cf" style="color: #003B4F;">if</span> i <span class="op" style="color: #5E5E5E;">&gt;</span> start_step:</span>
<span id="cb15-6">                <span class="co" style="color: #5E5E5E;">#print(f"step: {i}")</span></span>
<span id="cb15-7">                latent_model_input <span class="op" style="color: #5E5E5E;">=</span> torch.cat([latents] <span class="op" style="color: #5E5E5E;">*</span> <span class="dv" style="color: #AD0000;">2</span>)</span>
<span id="cb15-8">                latent_model_input <span class="op" style="color: #5E5E5E;">=</span> scheduler.scale_model_input(latent_model_input, t)</span>
<span id="cb15-9">                <span class="cf" style="color: #003B4F;">with</span> torch.no_grad():</span>
<span id="cb15-10">                    noise_u,noise_t <span class="op" style="color: #5E5E5E;">=</span> unet(latent_model_input, t, encoder_hidden_states<span class="op" style="color: #5E5E5E;">=</span>text_emb).sample.chunk(<span class="dv" style="color: #AD0000;">2</span>)</span>
<span id="cb15-11">                noise_pred <span class="op" style="color: #5E5E5E;">=</span> noise_u <span class="op" style="color: #5E5E5E;">+</span> g<span class="op" style="color: #5E5E5E;">*</span>(noise_t <span class="op" style="color: #5E5E5E;">-</span> noise_u)</span>
<span id="cb15-12">                noise_preds <span class="op" style="color: #5E5E5E;">=</span> torch.concat([noise_preds, noise_pred])</span>
<span id="cb15-13">                latents <span class="op" style="color: #5E5E5E;">=</span> scheduler.step(noise_pred, t, latents).prev_sample</span>
<span id="cb15-14">        <span class="cf" style="color: #003B4F;">return</span> latents, noise_preds</span></code></pre></div>
</div>
<div class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><span class="kw" style="color: #003B4F;">def</span> show_image(image, seed<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">None</span>, scale_by<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.5</span>):</span>
<span id="cb16-2">    <span class="cf" style="color: #003B4F;">if</span> seed <span class="kw" style="color: #003B4F;">is</span> <span class="kw" style="color: #003B4F;">not</span> <span class="va" style="color: #111111;">None</span>:</span>
<span id="cb16-3">        <span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f'Seed: </span><span class="sc" style="color: #5E5E5E;">{</span>seed<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">'</span>)</span>
<span id="cb16-4">    <span class="cf" style="color: #003B4F;">return</span> image.resize(((<span class="bu" style="color: null;">int</span>)(image.width <span class="op" style="color: #5E5E5E;">*</span> scale_by), (<span class="bu" style="color: null;">int</span>)(image.height <span class="op" style="color: #5E5E5E;">*</span> scale_by)))</span></code></pre></div>
</div>
<div class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb17" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><span class="kw" style="color: #003B4F;">def</span> add_noise_to_image(latents, seed, scheduler, start_step):</span>
<span id="cb17-2">    torch.manual_seed(seed)</span>
<span id="cb17-3">    noise <span class="op" style="color: #5E5E5E;">=</span> torch.randn_like(latents)</span>
<span id="cb17-4">    noised_latents <span class="op" style="color: #5E5E5E;">=</span> scheduler.add_noise(</span>
<span id="cb17-5">        original_samples<span class="op" style="color: #5E5E5E;">=</span>latents, </span>
<span id="cb17-6">        noise<span class="op" style="color: #5E5E5E;">=</span>noise, </span>
<span id="cb17-7">        timesteps<span class="op" style="color: #5E5E5E;">=</span>torch.tensor([scheduler.timesteps[start_step]]))</span>
<span id="cb17-8">    <span class="cf" style="color: #003B4F;">return</span> noised_latents</span></code></pre></div>
</div>
<div class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb18" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><span class="kw" style="color: #003B4F;">def</span> show_images(nrows, ncols, images, titles<span class="op" style="color: #5E5E5E;">=</span>[], figsize<span class="op" style="color: #5E5E5E;">=</span>(<span class="dv" style="color: #AD0000;">16</span>, <span class="dv" style="color: #AD0000;">5</span>)):</span>
<span id="cb18-2">    num_axes <span class="op" style="color: #5E5E5E;">=</span> nrows<span class="op" style="color: #5E5E5E;">*</span>ncols</span>
<span id="cb18-3">    num_images <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">len</span>(images)</span>
<span id="cb18-4">    num_titles <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">len</span>(titles)</span>
<span id="cb18-5">    fig, axs <span class="op" style="color: #5E5E5E;">=</span> plt.subplots(nrows, ncols, figsize<span class="op" style="color: #5E5E5E;">=</span>figsize)</span>
<span id="cb18-6">    flt_ax <span class="op" style="color: #5E5E5E;">=</span> axs.flat</span>
<span id="cb18-7">    <span class="cf" style="color: #003B4F;">for</span> c <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(num_axes):</span>
<span id="cb18-8">        <span class="cf" style="color: #003B4F;">if</span> c <span class="op" style="color: #5E5E5E;">==</span> num_images: <span class="cf" style="color: #003B4F;">break</span></span>
<span id="cb18-9">        flt_ax[c].imshow(images[c])</span>
<span id="cb18-10">        flt_ax[c].axis(<span class="st" style="color: #20794D;">'off'</span>)</span>
<span id="cb18-11">        <span class="cf" style="color: #003B4F;">if</span> c <span class="op" style="color: #5E5E5E;">&lt;</span> num_titles:</span>
<span id="cb18-12">            flt_ax[c].set_title(titles[c])</span></code></pre></div>
</div>
</section>
<section id="define-parameters-for-analysis" class="level2">
<h2 class="anchored" data-anchor-id="define-parameters-for-analysis">Define parameters for analysis</h2>
<div class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb19" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1">resolution <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">512</span></span>
<span id="cb19-2">def_steps <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">70</span></span>
<span id="cb19-3">def_g <span class="op" style="color: #5E5E5E;">=</span> <span class="fl" style="color: #AD0000;">7.5</span></span>
<span id="cb19-4">def_strength <span class="op" style="color: #5E5E5E;">=</span> <span class="fl" style="color: #AD0000;">0.5</span></span>
<span id="cb19-5">def_sch <span class="op" style="color: #5E5E5E;">=</span> scheduler</span>
<span id="cb19-6">start_step <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">20</span></span></code></pre></div>
</div>
</section>
<section id="load-base-image-and-create-latents" class="level2">
<h2 class="anchored" data-anchor-id="load-base-image-and-create-latents">Load base image and create latents</h2>
<div class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb20" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1">path_to_image <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"/home/images/horse_1_512.jpg"</span></span>
<span id="cb20-2">image <span class="op" style="color: #5E5E5E;">=</span> load_image(path_to_image, resolution)</span>
<span id="cb20-3">latents <span class="op" style="color: #5E5E5E;">=</span> pil_to_latent(image)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb21" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><span class="co" style="color: #5E5E5E;"># show the base image</span></span>
<span id="cb21-2">show_image(image)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="21">
<p><img src="https://fromLittleAcorns.github.io/john.richmond.github.io/posts/2022-11-10-StableDiffusion/2022-11-11-diffedit_explore_files/figure-html/cell-22-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb22" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><span class="co" style="color: #5E5E5E;"># Plot the latents</span></span>
<span id="cb22-2">show_latents(latents)</span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="https://fromLittleAcorns.github.io/john.richmond.github.io/posts/2022-11-10-StableDiffusion/2022-11-11-diffedit_explore_files/figure-html/cell-23-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="define-prompts-and-create-embeddings" class="level2">
<h2 class="anchored" data-anchor-id="define-prompts-and-create-embeddings">Define Prompts and create embeddings</h2>
<div class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb23" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><span class="co" style="color: #5E5E5E;">#base_prompt = "A horse running on grass under a cloudy blue sky"</span></span>
<span id="cb23-2"><span class="co" style="color: #5E5E5E;">#target_prompt = "A zebra running on grass under a cloudy blue sky"</span></span>
<span id="cb23-3">base_prompt <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"A horse"</span></span>
<span id="cb23-4">target_prompt <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"Zebra"</span></span>
<span id="cb23-5">unguided_prompt <span class="op" style="color: #5E5E5E;">=</span> [<span class="st" style="color: #20794D;">""</span>]</span></code></pre></div>
</div>
<div class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb24" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1">base_prompt_emb <span class="op" style="color: #5E5E5E;">=</span> prompt_to_embedding(base_prompt, torch_device)</span>
<span id="cb24-2">target_prompt_emb <span class="op" style="color: #5E5E5E;">=</span> prompt_to_embedding(target_prompt, torch_device)</span>
<span id="cb24-3">unguided_prompt <span class="op" style="color: #5E5E5E;">=</span> prompt_to_embedding(unguided_prompt, torch_device)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb25" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1">base_emb_pair <span class="op" style="color: #5E5E5E;">=</span> torch.concat([unguided_prompt, base_prompt_emb])</span>
<span id="cb25-2">target_emb_pair <span class="op" style="color: #5E5E5E;">=</span> torch.concat([unguided_prompt, target_prompt_emb])</span></code></pre></div>
</div>
</section>
<section id="set-inference-timesteps" class="level2">
<h2 class="anchored" data-anchor-id="set-inference-timesteps">Set inference timesteps</h2>
<div class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb26" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><span class="kw" style="color: #003B4F;">def</span> prepare_scheduler_noise_image_and_denoise(latents, text_emb, steps, start_step, scheduler, seed, g, dim, device):</span>
<span id="cb26-2">    timesteps <span class="op" style="color: #5E5E5E;">=</span> scheduler.set_timesteps(steps)</span>
<span id="cb26-3">    strength<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.5</span></span>
<span id="cb26-4">    noised_latents <span class="op" style="color: #5E5E5E;">=</span> add_noise_to_image(latents, seed, scheduler, start_step)</span>
<span id="cb26-5">    dn_latents, noise <span class="op" style="color: #5E5E5E;">=</span> denoising_loop(noised_latents, text_emb<span class="op" style="color: #5E5E5E;">=</span>text_emb, scheduler<span class="op" style="color: #5E5E5E;">=</span>scheduler, g<span class="op" style="color: #5E5E5E;">=</span>g, strength<span class="op" style="color: #5E5E5E;">=</span>strength, steps<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">70</span>, </span>
<span id="cb26-6">                                           dim<span class="op" style="color: #5E5E5E;">=</span>dim, start_step<span class="op" style="color: #5E5E5E;">=</span>start_step, torch_device<span class="op" style="color: #5E5E5E;">=</span>device)</span>
<span id="cb26-7">    <span class="cf" style="color: #003B4F;">return</span> dn_latents, noise</span>
<span id="cb26-8">    </span></code></pre></div>
</div>
<div class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb27" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1">dn_base, noise_base <span class="op" style="color: #5E5E5E;">=</span> prepare_scheduler_noise_image_and_denoise(latents, </span>
<span id="cb27-2">                                                                text_emb<span class="op" style="color: #5E5E5E;">=</span>base_emb_pair, </span>
<span id="cb27-3">                                                                steps<span class="op" style="color: #5E5E5E;">=</span>def_steps, </span>
<span id="cb27-4">                                                                start_step<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">30</span>, </span>
<span id="cb27-5">                                                                scheduler<span class="op" style="color: #5E5E5E;">=</span>def_sch, </span>
<span id="cb27-6">                                                                seed<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">100</span>, </span>
<span id="cb27-7">                                                                g<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">7.5</span>,</span>
<span id="cb27-8">                                                                dim<span class="op" style="color: #5E5E5E;">=</span>resolution, </span>
<span id="cb27-9">                                                                device<span class="op" style="color: #5E5E5E;">=</span>torch_device)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/opt/conda/lib/python3.8/site-packages/diffusers/schedulers/scheduling_lms_discrete.py:155: IntegrationWarning: The maximum number of subdivisions (50) has been achieved.
  If increasing the limit yields no improvement it is advised to analyze 
  the integrand in order to determine the difficulties.  If the position of a 
  local difficulty can be determined (singularity, discontinuity) one will 
  probably gain from splitting up the interval and calling the integrator 
  on the subranges.  Perhaps a special-purpose integrator should be used.
  integrated_coeff = integrate.quad(lms_derivative, self.sigmas[t], self.sigmas[t + 1], epsrel=1e-4)[0]</code></pre>
</div>
</div>
<div class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb29" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1">latents_to_pil(dn_base)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="28">
<p><img src="https://fromLittleAcorns.github.io/john.richmond.github.io/posts/2022-11-10-StableDiffusion/2022-11-11-diffedit_explore_files/figure-html/cell-29-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb30" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1">dn_target, noise_target <span class="op" style="color: #5E5E5E;">=</span> prepare_scheduler_noise_image_and_denoise(latents, </span>
<span id="cb30-2">                                                                text_emb<span class="op" style="color: #5E5E5E;">=</span>target_emb_pair, </span>
<span id="cb30-3">                                                                steps<span class="op" style="color: #5E5E5E;">=</span>def_steps, </span>
<span id="cb30-4">                                                                start_step<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">30</span>, </span>
<span id="cb30-5">                                                                scheduler<span class="op" style="color: #5E5E5E;">=</span>def_sch, </span>
<span id="cb30-6">                                                                seed<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">100</span>, </span>
<span id="cb30-7">                                                                g<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">7.5</span>,</span>
<span id="cb30-8">                                                                dim<span class="op" style="color: #5E5E5E;">=</span>resolution, </span>
<span id="cb30-9">                                                                device<span class="op" style="color: #5E5E5E;">=</span>torch_device)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb31" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1">latents_to_pil(dn_target)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="30">
<p><img src="https://fromLittleAcorns.github.io/john.richmond.github.io/posts/2022-11-10-StableDiffusion/2022-11-11-diffedit_explore_files/figure-html/cell-31-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="31">
<div class="sourceCode cell-code" id="cb32" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1">show_images(<span class="dv" style="color: #AD0000;">1</span>, <span class="dv" style="color: #AD0000;">3</span> , [np.asarray(image), latents_to_array(dn_base), latents_to_array(dn_target)],</span>
<span id="cb32-2">           [<span class="st" style="color: #20794D;">"Original Image"</span>, <span class="st" style="color: #20794D;">"Denoised original"</span>, <span class="st" style="color: #20794D;">"Denoised Zebra"</span>, <span class="st" style="color: #20794D;">"Denoised Zebra with mask"</span>])</span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="https://fromLittleAcorns.github.io/john.richmond.github.io/posts/2022-11-10-StableDiffusion/2022-11-11-diffedit_explore_files/figure-html/cell-32-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="32">
<div class="sourceCode cell-code" id="cb33" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1">orig_img <span class="op" style="color: #5E5E5E;">=</span> np.asarray(image)</span>
<span id="cb33-2">orig_img.shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="32">
<pre><code>(512, 515, 3)</code></pre>
</div>
</div>
<div class="cell" data-execution_count="33">
<div class="sourceCode cell-code" id="cb35" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1">noise_base.shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="33">
<pre><code>torch.Size([39, 4, 64, 64])</code></pre>
</div>
</div>
<div class="cell" data-execution_count="34">
<div class="sourceCode cell-code" id="cb37" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1">diff_noises <span class="op" style="color: #5E5E5E;">=</span> (noise_base <span class="op" style="color: #5E5E5E;">-</span> noise_target).mean(<span class="dv" style="color: #AD0000;">0</span>, keepdim<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="35">
<div class="sourceCode cell-code" id="cb38" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1">diff_noises.shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="35">
<pre><code>torch.Size([1, 4, 64, 64])</code></pre>
</div>
</div>
<div class="cell" data-execution_count="36">
<div class="sourceCode cell-code" id="cb40" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1">diff_noise_normed <span class="op" style="color: #5E5E5E;">=</span> (diff_noises <span class="op" style="color: #5E5E5E;">-</span> diff_noises.<span class="bu" style="color: null;">min</span>())<span class="op" style="color: #5E5E5E;">/</span>(diff_noises <span class="op" style="color: #5E5E5E;">-</span> diff_noises.<span class="bu" style="color: null;">min</span>()).<span class="bu" style="color: null;">max</span>()</span></code></pre></div>
</div>
<div class="cell" data-execution_count="37">
<div class="sourceCode cell-code" id="cb41" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1">diff_noise_normed.shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="37">
<pre><code>torch.Size([1, 4, 64, 64])</code></pre>
</div>
</div>
<div class="cell" data-execution_count="38">
<div class="sourceCode cell-code" id="cb43" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1">show_latents(diff_noise_normed)</span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="https://fromLittleAcorns.github.io/john.richmond.github.io/posts/2022-11-10-StableDiffusion/2022-11-11-diffedit_explore_files/figure-html/cell-39-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="39">
<div class="sourceCode cell-code" id="cb44" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1">diff_noise_normed.<span class="bu" style="color: null;">min</span>(), diff_noise_normed.<span class="bu" style="color: null;">max</span>(), diff_noise_normed.std(), diff_noise_normed.mean()</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="39">
<pre><code>(tensor(0., device='cuda:0'),
 tensor(1., device='cuda:0'),
 tensor(0.0744, device='cuda:0'),
 tensor(0.5083, device='cuda:0'))</code></pre>
</div>
</div>
<div class="cell" data-execution_count="40">
<div class="sourceCode cell-code" id="cb46" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1">mask <span class="op" style="color: #5E5E5E;">=</span> ((diff_noise_normed<span class="op" style="color: #5E5E5E;">-</span><span class="fl" style="color: #AD0000;">0.5</span>).<span class="bu" style="color: null;">abs</span>()<span class="op" style="color: #5E5E5E;">+</span><span class="fl" style="color: #AD0000;">0.5</span>).mean(dim<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>).squeeze().cpu()</span></code></pre></div>
</div>
<div class="cell" data-execution_count="41">
<div class="sourceCode cell-code" id="cb47" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><span class="im" style="color: #00769E;">import</span> cv2</span></code></pre></div>
</div>
<div class="cell" data-execution_count="42">
<div class="sourceCode cell-code" id="cb48" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><span class="kw" style="color: #003B4F;">def</span> extract_channel_mask(img, do_inverse<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span>):</span>
<span id="cb48-2">    kernel <span class="op" style="color: #5E5E5E;">=</span> np.ones((<span class="dv" style="color: #AD0000;">3</span>,<span class="dv" style="color: #AD0000;">3</span>),np.uint8)</span>
<span id="cb48-3">    img <span class="op" style="color: #5E5E5E;">=</span> (img<span class="op" style="color: #5E5E5E;">*</span><span class="dv" style="color: #AD0000;">255</span>).squeeze().cpu().to(torch.uint8).numpy()</span>
<span id="cb48-4">    <span class="cf" style="color: #003B4F;">if</span> do_inverse:</span>
<span id="cb48-5">        ret2,img2 <span class="op" style="color: #5E5E5E;">=</span> cv2.threshold(img,<span class="dv" style="color: #AD0000;">0</span>,<span class="dv" style="color: #AD0000;">255</span>,cv2.THRESH_BINARY_INV<span class="op" style="color: #5E5E5E;">+</span>cv2.THRESH_OTSU)</span>
<span id="cb48-6">    <span class="cf" style="color: #003B4F;">else</span>:</span>
<span id="cb48-7">        ret2,img2 <span class="op" style="color: #5E5E5E;">=</span> cv2.threshold(img,<span class="dv" style="color: #AD0000;">0</span>,<span class="dv" style="color: #AD0000;">255</span>,cv2.THRESH_BINARY<span class="op" style="color: #5E5E5E;">+</span>cv2.THRESH_OTSU)</span>
<span id="cb48-8">    opening <span class="op" style="color: #5E5E5E;">=</span> cv2.dilate(img2, kernel)</span>
<span id="cb48-9">    <span class="cf" style="color: #003B4F;">return</span> opening</span></code></pre></div>
</div>
<div class="cell" data-execution_count="44">
<div class="sourceCode cell-code" id="cb49" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1">show_images(<span class="dv" style="color: #AD0000;">2</span>, <span class="dv" style="color: #AD0000;">1</span>, [mask, extract_channel_mask(mask, do_inverse<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span>)])</span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="https://fromLittleAcorns.github.io/john.richmond.github.io/posts/2022-11-10-StableDiffusion/2022-11-11-diffedit_explore_files/figure-html/cell-44-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="now-need-to-apply-the-mask-to-the-generated-zebra-image-and-then-run-the-decode-function" class="level2">
<h2 class="anchored" data-anchor-id="now-need-to-apply-the-mask-to-the-generated-zebra-image-and-then-run-the-decode-function">Now need to apply the mask to the generated zebra image and then run the decode function</h2>
<div class="cell" data-execution_count="45">
<div class="sourceCode cell-code" id="cb50" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1">binary_mask <span class="op" style="color: #5E5E5E;">=</span> torch.tensor(extract_channel_mask(mask, do_inverse<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span>)).<span class="bu" style="color: null;">bool</span>()</span></code></pre></div>
</div>
<div class="cell" data-execution_count="46">
<div class="sourceCode cell-code" id="cb51" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><span class="kw" style="color: #003B4F;">def</span> apply_mask_to_latents(original_latents, new_latents, mask):</span>
<span id="cb51-2">    comp_lat <span class="op" style="color: #5E5E5E;">=</span> torch.where(mask, new_latents.cpu(), original_latents.cpu())</span>
<span id="cb51-3">    <span class="cf" style="color: #003B4F;">return</span> comp_lat</span></code></pre></div>
</div>
<div class="cell" data-execution_count="47">
<div class="sourceCode cell-code" id="cb52" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1">final_latents <span class="op" style="color: #5E5E5E;">=</span> apply_mask_to_latents(dn_base, dn_target, binary_mask)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="48">
<div class="sourceCode cell-code" id="cb53" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1">latents_to_pil(final_latents.to(torch_device))</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="48">
<p><img src="https://fromLittleAcorns.github.io/john.richmond.github.io/posts/2022-11-10-StableDiffusion/2022-11-11-diffedit_explore_files/figure-html/cell-48-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="49">
<div class="sourceCode cell-code" id="cb54" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1">show_images(<span class="dv" style="color: #AD0000;">2</span>, <span class="dv" style="color: #AD0000;">2</span>, [np.asarray(image), latents_to_array(dn_base), latents_to_array(dn_target), latents_to_array(final_latents.to(torch_device))],</span>
<span id="cb54-2">           [<span class="st" style="color: #20794D;">"Original Image"</span>, <span class="st" style="color: #20794D;">"Denoised original"</span>, <span class="st" style="color: #20794D;">"Denoised Zebra"</span>, <span class="st" style="color: #20794D;">"Denoised Zebra with mask"</span>], figsize<span class="op" style="color: #5E5E5E;">=</span>(<span class="dv" style="color: #AD0000;">10</span>,<span class="dv" style="color: #AD0000;">10</span>))</span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="https://fromLittleAcorns.github.io/john.richmond.github.io/posts/2022-11-10-StableDiffusion/2022-11-11-diffedit_explore_files/figure-html/cell-49-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>In this case the final masked image is almost identical to that of the unmasked image since the background generated by the denoising process had almost no differene. In other cases this could clearly be more extreme. The issue of course would be that the mask would need to be carefully blended to facilitate a smooth merge.</p>
<p>Overall it seems to me that this is an approach that has very limited application an in many ways it is better to avoid using the mask</p>


</section>

 ]]></description>
  <guid>https://fromLittleAcorns.github.io/john.richmond.github.io/posts/2022-11-10-StableDiffusion/2022-11-11-diffedit_explore.html</guid>
  <pubDate>Sun, 27 Nov 2022 14:37:18 GMT</pubDate>
</item>
</channel>
</rss>
