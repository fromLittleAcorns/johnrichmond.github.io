[
  {
    "objectID": "posts/2022-11-30-fastai-course22p/04_dataloaders_optimisers_training.html",
    "href": "posts/2022-11-30-fastai-course22p/04_dataloaders_optimisers_training.html",
    "title": "Notebook to establish mini-batch training from first principles",
    "section": "",
    "text": "Notebook based upon the fastai course 22p, “Part 2 of Practical Deep Learning for Coders”. This notebook builds the capability for training a model in batches. As such it covers: * understanding managing and accessing model parameters * Cross entropy loss for classification * dataloaders, including samplers and multiprocessing * optimisers - implementation of SGD and how to implement using model parameters * setting up training loops with validation\n\nimport pickle,gzip,math,os,time,shutil,torch,matplotlib as mpl,numpy as np,matplotlib.pyplot as plt\nfrom pathlib import Path\nfrom torch import tensor,nn\nimport torch.nn.functional as F\nfrom fastcore.test import test_close\n\ntorch.set_printoptions(precision=2, linewidth=140, sci_mode=False)\ntorch.manual_seed(1)\nmpl.rcParams['image.cmap'] = 'gray'\n\npath_to_data = Path('/Users/johnrichmond/local_datasets') / Path('data')\npath_to_data.mkdir(exist_ok=True)\npath_gz = path_to_data / 'mnist.pkg.gz'\n\nwith gzip.open(path_gz, 'rb') as f: ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\nx_train, y_train, x_valid, y_valid = map(tensor, [x_train, y_train, x_valid, y_valid])"
  },
  {
    "objectID": "posts/2022-11-30-fastai-course22p/04_dataloaders_optimisers_training.html#data",
    "href": "posts/2022-11-30-fastai-course22p/04_dataloaders_optimisers_training.html#data",
    "title": "Notebook to establish mini-batch training from first principles",
    "section": "Data",
    "text": "Data\nAs before use Mnist data with a single hidden layer of 50 neurons\n\nn,m = x_train.shape\nc = y_train.max()+1\nnh = 50\nn, m, c\n\n(50000, 784, tensor(10))\n\n\nCreate a simple model that inherits from the pytorch nn.Module class\n\nclass Model(nn.Module):\n    def __init__(self, n_in, nh, n_out):\n        super().__init__()\n        self.layers = [nn.Linear(n_in,nh), nn.ReLU(), nn.Linear(nh,n_out)]\n        \n    def __call__(self, x):\n        for l in self.layers: x = l(x)\n        return x\n\nPass the training data though the model and store the predictions. Check the shape of the output and also what the range of the preds is\n\nmodel = Model(n_in=m, nh=nh, n_out=c)\npreds = model(x_train.clone())\npreds.shape, preds.min(), preds.max()\n\n(torch.Size([50000, 10]),\n tensor(-0.35, grad_fn=<MinBackward1>),\n tensor(0.42, grad_fn=<MaxBackward1>))"
  },
  {
    "objectID": "posts/2022-11-30-fastai-course22p/04_dataloaders_optimisers_training.html#a-simple-training-loop",
    "href": "posts/2022-11-30-fastai-course22p/04_dataloaders_optimisers_training.html#a-simple-training-loop",
    "title": "Notebook to establish mini-batch training from first principles",
    "section": "A simple training loop",
    "text": "A simple training loop\nBefore we train it is convenient to have a way to ascertain the accuracy of the model. To develop this a single batch of data will be processed\n\nbs = 64\n# load a minibatch\nxb = x_train[0:bs]\nyb = y_train[0:bs]\npreds = model(xb)\npreds.shape, preds[0]\n\n(torch.Size([64, 10]),\n tensor([-0.09, -0.21, -0.08,  0.10, -0.04,  0.08, -0.04, -0.03,  0.01,  0.06], grad_fn=<SelectBackward0>))\n\n\n\n# Define a loss function and calculate the loss for the batch\nloss_func = F.cross_entropy\nloss = loss_func(preds, yb)\nloss\n\ntensor(2.30, grad_fn=<NllLossBackward0>)\n\n\n\nCalculate Accuracy\nThe predicted class can be determined from the index of the class having the highest predicted value\n\npred_class = torch.argmax(preds, axis=1)\npred_class\n\ntensor([3, 9, 3, 8, 5, 9, 3, 9, 3, 9, 5, 3, 9, 9, 3, 9, 9, 5, 8, 7, 9, 5, 3, 8, 9, 5, 9, 5, 5, 9, 3, 5, 9, 7, 5, 7, 9, 9, 3, 9, 3, 5, 3, 8,\n        3, 5, 9, 5, 9, 5, 3, 9, 3, 8, 9, 5, 9, 5, 9, 5, 8, 8, 9, 8])\n\n\nFrom this the accuracy can be calculated\n\ndef accuracy(preds, targets):\n    return (torch.argmax(preds, axis=1) == targets).float().mean()\n\n\naccuracy(preds, yb)\n\ntensor(0.09)\n\n\nNot suprisingly, at this point the accuracy is what would be expected for a random prediction.\nTo improve this we need to train the model. To do that it is necessary to say what learning rate we would like and how many epochs to train for\n\nlr = 0.5\nepochs = 3\n\nNow create a simple training loop following the above steps. Note that the loss, backward and parameter update is happening for every batch\n\nfor epoch in range(epochs):\n    # iterate through batches\n    for batch in range(0, n, bs):\n        # get data\n        s = slice(batch, min(n, batch+bs))\n        xs = x_train[s]\n        ys = y_train[s]\n        # Pass data through the model\n        preds = model(xs)\n        # Calculate loss\n        loss = loss_func(preds, ys)\n        # Calculate the gradients\n        loss.backward()\n        # Print the loss periodically\n        if batch // bs // 10 == 0: print(f\"Loss: {loss.item()}, accuracy = {accuracy(preds, ys).item()}\")\n        with torch.no_grad():\n            for l in model.layers:\n                if hasattr(l, \"weight\"):\n                    l.weight -= l.weight.grad * lr\n                    l.bias -= l.bias.grad * lr\n                    l.weight.grad.zero_()\n                    l.bias.grad.zero_()\n        \n\nLoss: 2.3036487102508545, accuracy = 0.09375\nLoss: 2.2052316665649414, accuracy = 0.296875\nLoss: 2.1905035972595215, accuracy = 0.21875\nLoss: 2.010064125061035, accuracy = 0.53125\nLoss: 1.9125093221664429, accuracy = 0.5\nLoss: 1.7081104516983032, accuracy = 0.703125\nLoss: 1.6426200866699219, accuracy = 0.5\nLoss: 1.6326062679290771, accuracy = 0.625\nLoss: 1.5491729974746704, accuracy = 0.4375\nLoss: 1.5245637893676758, accuracy = 0.53125\nLoss: 0.12374816089868546, accuracy = 0.96875\nLoss: 0.14763614535331726, accuracy = 0.984375\nLoss: 0.29614493250846863, accuracy = 0.890625\nLoss: 0.16717804968357086, accuracy = 0.9375\nLoss: 0.1868995726108551, accuracy = 0.90625\nLoss: 0.05588085949420929, accuracy = 0.984375\nLoss: 0.10521863400936127, accuracy = 0.96875\nLoss: 0.2990257740020752, accuracy = 0.9375\nLoss: 0.04911200702190399, accuracy = 1.0\nLoss: 0.25950485467910767, accuracy = 0.9375\nLoss: 0.09232573211193085, accuracy = 0.96875\nLoss: 0.10159078985452652, accuracy = 0.984375\nLoss: 0.28459155559539795, accuracy = 0.9375\nLoss: 0.05941237136721611, accuracy = 0.984375\nLoss: 0.11075811833143234, accuracy = 0.9375\nLoss: 0.031431593000888824, accuracy = 0.984375\nLoss: 0.07170089334249496, accuracy = 0.984375\nLoss: 0.24644441902637482, accuracy = 0.953125\nLoss: 0.05402196943759918, accuracy = 0.984375\nLoss: 0.18466435372829437, accuracy = 0.984375"
  },
  {
    "objectID": "posts/2022-11-30-fastai-course22p/04_dataloaders_optimisers_training.html#adding-parameters-and-optim",
    "href": "posts/2022-11-30-fastai-course22p/04_dataloaders_optimisers_training.html#adding-parameters-and-optim",
    "title": "Notebook to establish mini-batch training from first principles",
    "section": "Adding parameters and optim",
    "text": "Adding parameters and optim\nThe above training loop works but accessing the model parameters us cumbersome since it requires advanced knowledge of the layers. Pytorch has some methods to allow accessing and modifying layer information in a more straightforward manner\n\nParameters\nRecreate the model with individual layers\n\nclass Model(nn.Module):\n    def __init__(self, n_in, nh, n_out):\n        super().__init__()\n        self.l1 = nn.Linear(n_in,nh)\n        self.relu = nn.ReLU()\n        self.l2 = nn.Linear(nh,n_out)\n        \n    def __call__(self, x):\n        x = self.l2(self.relu(self.l1(x)))\n        return x\n\n\nmodel = Model(m, nh, c)\n\nExamine the model parameters\n\nfor name, layer in model.named_children():\n    print(f\"Layer: {name}, Parameters: {layer}\")\n\nLayer: l1, Parameters: Linear(in_features=784, out_features=50, bias=True)\nLayer: relu, Parameters: ReLU()\nLayer: l2, Parameters: Linear(in_features=50, out_features=10, bias=True)\n\n\nThe layers can be accessed using the name alone, for example\n\nmodel.l1\n\nLinear(in_features=784, out_features=50, bias=True)\n\n\nThe layer parameters can also be accessed by the model.parameters property, however, this is an iterator and hence needs to be listed through a loop or creation of a list…\n\nfor param in model.parameters():\n    print(param.shape)\n\ntorch.Size([50, 784])\ntorch.Size([50])\ntorch.Size([10, 50])\ntorch.Size([10])\n\n\n\nfor param in model.parameters():\n    print(param[0:2])\n\ntensor([[ 0.04,  0.03, -0.02,  ...,  0.03,  0.03,  0.00],\n        [ 0.04,  0.01,  0.03,  ..., -0.01,  0.02,  0.03]], grad_fn=<SliceBackward0>)\ntensor([-0.01, -0.01], grad_fn=<SliceBackward0>)\ntensor([[-0.05,  0.02,  0.07,  0.09,  0.10,  0.12, -0.13,  0.10, -0.09, -0.08, -0.12, -0.01,  0.07, -0.00,  0.12, -0.03, -0.07, -0.14,\n          0.10,  0.13, -0.12,  0.14,  0.12,  0.08, -0.11,  0.03, -0.09,  0.12,  0.01, -0.03,  0.06, -0.00,  0.01, -0.05,  0.11,  0.14,\n          0.07,  0.05, -0.09, -0.03, -0.01, -0.01,  0.08,  0.02, -0.09, -0.05,  0.03,  0.13, -0.08,  0.13],\n        [ 0.09, -0.04,  0.00,  0.14, -0.13, -0.06,  0.03, -0.09, -0.11,  0.05,  0.04, -0.02, -0.04, -0.03,  0.01, -0.10, -0.03, -0.02,\n          0.00,  0.07,  0.10, -0.08, -0.14, -0.02, -0.13, -0.08,  0.07,  0.04, -0.13, -0.13, -0.05,  0.12, -0.08,  0.13,  0.13, -0.10,\n          0.06,  0.08, -0.13, -0.08, -0.06, -0.11,  0.07,  0.06,  0.09,  0.04, -0.13,  0.04, -0.10, -0.01]], grad_fn=<SliceBackward0>)\ntensor([-0.01, -0.06], grad_fn=<SliceBackward0>)\n\n\nUsing parameters it is possible to simplify the code to optimise the weights and biases by looping through teh model parameters. It is also possible to zero all of the model’s gradients with a since call:\n\ndef fit():\n    for epoch in range(epochs):\n        # iterate through batches\n        for batch in range(0, n, bs):\n            # get data\n            s = slice(batch, min(n, batch+bs))\n            xs = x_train[s]\n            ys = y_train[s]\n            # Pass data through the model\n            preds = model(xs)\n            # Calculate loss\n            loss = loss_func(preds, ys)\n            # Calculate the gradients\n            loss.backward()\n            # Print the loss periodically\n            if batch // bs // 10 == 0: print(f\"Loss: {loss.item()}, accuracy = {accuracy(preds, ys).item()}\")\n            with torch.no_grad():\n                for params in model.parameters():\n                    params -= params.grad * lr\n                model.zero_grad()                      \n\n\nfit()\n\nLoss: 2.309434175491333, accuracy = 0.0625\nLoss: 2.177271842956543, accuracy = 0.234375\nLoss: 2.259394645690918, accuracy = 0.078125\nLoss: 2.0509822368621826, accuracy = 0.578125\nLoss: 1.9572300910949707, accuracy = 0.34375\nLoss: 1.7929021120071411, accuracy = 0.71875\nLoss: 1.6334275007247925, accuracy = 0.65625\nLoss: 1.5452338457107544, accuracy = 0.625\nLoss: 1.507757306098938, accuracy = 0.4375\nLoss: 1.6010795831680298, accuracy = 0.484375\nLoss: 0.2006874829530716, accuracy = 0.953125\nLoss: 0.11892185360193253, accuracy = 0.96875\nLoss: 0.2844753861427307, accuracy = 0.90625\nLoss: 0.17238563299179077, accuracy = 0.96875\nLoss: 0.15464989840984344, accuracy = 0.90625\nLoss: 0.04285932332277298, accuracy = 1.0\nLoss: 0.12839168310165405, accuracy = 0.96875\nLoss: 0.3218374252319336, accuracy = 0.953125\nLoss: 0.09430787712335587, accuracy = 0.96875\nLoss: 0.3029904365539551, accuracy = 0.953125\nLoss: 0.18196934461593628, accuracy = 0.9375\nLoss: 0.0731976106762886, accuracy = 0.984375\nLoss: 0.29567304253578186, accuracy = 0.9375\nLoss: 0.0792182981967926, accuracy = 0.984375\nLoss: 0.09979861974716187, accuracy = 0.953125\nLoss: 0.028896179050207138, accuracy = 1.0\nLoss: 0.10094335675239563, accuracy = 0.96875\nLoss: 0.28324049711227417, accuracy = 0.9375\nLoss: 0.055503442883491516, accuracy = 0.984375\nLoss: 0.2787169814109802, accuracy = 0.96875\n\n\nThe way that the model parameters are setup is by Pytorch overriding the setattrib method in nn.Module. The way that this works is as below\n\nclass DummyModule():\n    def __init__(self, n_in, nh, n_out):\n        self._modules = {}\n        self.l1 = nn.Linear(n_in,nh)\n        self.relu = nn.ReLU()\n        self.l2 = nn.Linear(nh,n_out)\n        \n    def __setattr__(self, k,v):\n        # Assign the layers to the modules dict before calling the set attribute class.  The parameters are \n        # simply name (k), value (v)\n        if not k.startswith(\"_\"):\n            self._modules[k] = v\n        super().__setattr__(k,v)\n            \n    def __repr__(self): \n        # Setup so that the official string representation of the class instance is a listing of the module\n        return f\"{self._modules}\"\n    \n    def parameters(self):\n        # Create an iterator to yield all of the model parameters\n        for layer in self._modules.values():\n            for param in layer.parameters(): yield(param)\n            \n\n\ndm = DummyModule(m, nh, c)\ndm\n\n{'l1': Linear(in_features=784, out_features=50, bias=True), 'relu': ReLU(), 'l2': Linear(in_features=50, out_features=10, bias=True)}\n\n\n\nfor param in dm.parameters():\n    print(param.shape)\n\ntorch.Size([50, 784])\ntorch.Size([50])\ntorch.Size([10, 50])\ntorch.Size([10])\n\n\nThis approach won’t work as it with lists of layers as things were originally setup. To allow this. to work the layers have to be individually registered\n\nlayers = [nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10)]\n\n\nclass Model(nn.Module):\n    def __init__(self, layers):\n        super().__init__()\n        self.layers = layers\n        for i, layer in enumerate(self.layers):\n            self.add_module(f\"layer_{i}\", layer)\n    \n    def __call__(self, x):\n        for layer in self.layers:\n            x = layer(x)\n        return x\n    \n\n\nmodel = Model(layers)\n\n\nmodel\n\nModel(\n  (layer_0): Linear(in_features=784, out_features=50, bias=True)\n  (layer_1): ReLU()\n  (layer_2): Linear(in_features=50, out_features=10, bias=True)\n)\n\n\n\nfit()\n\nLoss: 2.3222596645355225, accuracy = 0.03125\nLoss: 2.2330336570739746, accuracy = 0.21875\nLoss: 2.222136974334717, accuracy = 0.140625\nLoss: 2.079106330871582, accuracy = 0.421875\nLoss: 1.958361029624939, accuracy = 0.5625\nLoss: 1.7925820350646973, accuracy = 0.625\nLoss: 1.675372838973999, accuracy = 0.515625\nLoss: 1.6172857284545898, accuracy = 0.609375\nLoss: 1.4986895322799683, accuracy = 0.546875\nLoss: 1.5764800310134888, accuracy = 0.515625\nLoss: 0.16430063545703888, accuracy = 0.953125\nLoss: 0.12203794717788696, accuracy = 0.984375\nLoss: 0.3307473063468933, accuracy = 0.890625\nLoss: 0.21634256839752197, accuracy = 0.921875\nLoss: 0.17808055877685547, accuracy = 0.921875\nLoss: 0.04548545181751251, accuracy = 0.984375\nLoss: 0.13622808456420898, accuracy = 0.96875\nLoss: 0.32709184288978577, accuracy = 0.9375\nLoss: 0.09109017997980118, accuracy = 0.984375\nLoss: 0.27433082461357117, accuracy = 0.953125\nLoss: 0.09979915618896484, accuracy = 0.96875\nLoss: 0.08315068483352661, accuracy = 0.984375\nLoss: 0.27988025546073914, accuracy = 0.90625\nLoss: 0.1201467365026474, accuracy = 0.953125\nLoss: 0.14109008014202118, accuracy = 0.9375\nLoss: 0.01877472549676895, accuracy = 1.0\nLoss: 0.10770482569932938, accuracy = 0.96875\nLoss: 0.3286954462528229, accuracy = 0.953125\nLoss: 0.06740975379943848, accuracy = 0.984375\nLoss: 0.18299852311611176, accuracy = 0.984375\n\n\nRegistering the layers can be done using the nn.ModuleList class: Link to nn.ModuleList docs\nThus this allows further simplification\n\nclass SequentialModel(nn.Module):\n    def __init__(self, layers):\n        super().__init__()\n        self.layers = nn.ModuleList(layers)\n        \n    def __call__(self, x):\n        for l in self.layers: x = l(x)\n        return x\n\n\nmodel = SequentialModel(layers)\nmodel\n\nSequentialModel(\n  (layers): ModuleList(\n    (0): Linear(in_features=784, out_features=50, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=50, out_features=10, bias=True)\n  )\n)\n\n\n\nfit()\n\nLoss: 0.06515892595052719, accuracy = 0.96875\nLoss: 0.06317746639251709, accuracy = 0.984375\nLoss: 0.21157535910606384, accuracy = 0.921875\nLoss: 0.10860879719257355, accuracy = 0.96875\nLoss: 0.08964528143405914, accuracy = 0.9375\nLoss: 0.011501152999699116, accuracy = 1.0\nLoss: 0.08443643152713776, accuracy = 0.96875\nLoss: 0.3277687132358551, accuracy = 0.9375\nLoss: 0.06517940014600754, accuracy = 0.984375\nLoss: 0.17099246382713318, accuracy = 0.96875\nLoss: 0.04173600673675537, accuracy = 0.984375\nLoss: 0.04945621266961098, accuracy = 0.984375\nLoss: 0.1965624988079071, accuracy = 0.90625\nLoss: 0.0814109668135643, accuracy = 0.96875\nLoss: 0.06253797560930252, accuracy = 0.96875\nLoss: 0.009009594097733498, accuracy = 1.0\nLoss: 0.08011716604232788, accuracy = 0.96875\nLoss: 0.32447099685668945, accuracy = 0.9375\nLoss: 0.05112294852733612, accuracy = 0.984375\nLoss: 0.15682904422283173, accuracy = 0.984375\nLoss: 0.02698095701634884, accuracy = 1.0\nLoss: 0.035535961389541626, accuracy = 0.984375\nLoss: 0.15110039710998535, accuracy = 0.921875\nLoss: 0.0608980655670166, accuracy = 0.96875\nLoss: 0.03945880010724068, accuracy = 1.0\nLoss: 0.005177198443561792, accuracy = 1.0\nLoss: 0.07748386263847351, accuracy = 0.96875\nLoss: 0.3055875599384308, accuracy = 0.9375\nLoss: 0.04133985936641693, accuracy = 0.984375\nLoss: 0.16388559341430664, accuracy = 0.984375\n\n\n\nloss_func(model(xb), yb), accuracy(model(xb), yb)\n\n(tensor(0.02, grad_fn=<NllLossBackward0>), tensor(1.))\n\n\n\n\nThe nn.Sequential Class\nThis class does what is done above, in other words it takes a list of layers and created a model by registering and then saving the layers in sequence.\nThe sequential class will not accept a list as an input and so we have to pass in the individual layers\n\nmodel = nn.Sequential(nn.Linear(m, nh), nn.ReLU(), nn.Linear(nh, c))\nmodel\n\nSequential(\n  (0): Linear(in_features=784, out_features=50, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=50, out_features=10, bias=True)\n)\n\n\n\nfit()\n\nLoss: 2.3087124824523926, accuracy = 0.09375\nLoss: 2.220724582672119, accuracy = 0.4375\nLoss: 2.2094054222106934, accuracy = 0.15625\nLoss: 2.0799782276153564, accuracy = 0.484375\nLoss: 1.9859000444412231, accuracy = 0.421875\nLoss: 1.8239319324493408, accuracy = 0.5625\nLoss: 1.692657232284546, accuracy = 0.625\nLoss: 1.5962902307510376, accuracy = 0.671875\nLoss: 1.4929006099700928, accuracy = 0.46875\nLoss: 1.5572547912597656, accuracy = 0.53125\nLoss: 0.2031664103269577, accuracy = 0.90625\nLoss: 0.12023147940635681, accuracy = 0.984375\nLoss: 0.36551305651664734, accuracy = 0.890625\nLoss: 0.1421089768409729, accuracy = 0.96875\nLoss: 0.13040338456630707, accuracy = 0.921875\nLoss: 0.03445771709084511, accuracy = 0.984375\nLoss: 0.14158782362937927, accuracy = 0.96875\nLoss: 0.29142311215400696, accuracy = 0.953125\nLoss: 0.1052810475230217, accuracy = 0.953125\nLoss: 0.22577130794525146, accuracy = 0.984375\nLoss: 0.20330604910850525, accuracy = 0.921875\nLoss: 0.1524616777896881, accuracy = 0.96875\nLoss: 0.3139619827270508, accuracy = 0.921875\nLoss: 0.10483880341053009, accuracy = 0.953125\nLoss: 0.09913913160562515, accuracy = 0.9375\nLoss: 0.02614644728600979, accuracy = 1.0\nLoss: 0.1354278326034546, accuracy = 0.96875\nLoss: 0.2590445876121521, accuracy = 0.953125\nLoss: 0.06993013620376587, accuracy = 0.96875\nLoss: 0.1877637505531311, accuracy = 0.984375\n\n\n\n\nIntroducing the optimiser class Optim\nSo far we have developed the loss function and the model but the appliction of the weights has been done using a relatively simple implmentation of SGD. The process of optimising the model parameters can be built into a class. In Pytorch this is the Optim class. This will now be developed.\nNote that in the class below there are two things to ensure: 1. The params are converted into a list in the init function (to facilitate iteration) 2. when zeroing the gradients it is important to use the in place version of the function\n\nclass Optimizer():\n    def __init__(self, params, lr=0.5):\n        self.params = list(params); self.lr = lr\n        \n    def step(self):\n        with torch.no_grad():\n            for param in self.params: param -= param.grad * self.lr\n    \n    def zero_grad(self):\n        for param in self.params: param.grad.data.zero_() \n\n\nmodel = nn.Sequential(nn.Linear(m, nh), nn.ReLU(), nn.Linear(nh, c))\n\n\nopt = Optimizer(model.parameters())\n\nUpdate the training loop to work with the optimizer\n\nfor epoch in range(epochs):\n    for batch in range(0, n, bs):\n        # get data\n        s = slice(batch, min(n, batch+bs))\n        xs = x_train[s]\n        ys = y_train[s]\n        # Pass data through the model\n        preds = model(xs)\n        loss = loss_func(preds, ys)\n        # Print results\n        if batch // bs // 10 == 0: print(f\"Loss: {loss.item()}, accuracy = {accuracy(preds, ys).item()}\")\n        # Calculate gradients\n        loss.backward()\n        # Apply the optimiser\n        opt.step()\n        opt.zero_grad()\n\nLoss: 2.30017352104187, accuracy = 0.0625\nLoss: 2.1758975982666016, accuracy = 0.234375\nLoss: 2.199198007583618, accuracy = 0.1875\nLoss: 2.005563497543335, accuracy = 0.453125\nLoss: 1.9248197078704834, accuracy = 0.359375\nLoss: 1.772184133529663, accuracy = 0.578125\nLoss: 1.62507963180542, accuracy = 0.546875\nLoss: 1.5354200601577759, accuracy = 0.640625\nLoss: 1.5023629665374756, accuracy = 0.46875\nLoss: 1.6080732345581055, accuracy = 0.421875\nLoss: 0.13067957758903503, accuracy = 0.96875\nLoss: 0.1198587417602539, accuracy = 0.984375\nLoss: 0.2864210307598114, accuracy = 0.921875\nLoss: 0.21201932430267334, accuracy = 0.921875\nLoss: 0.1917288601398468, accuracy = 0.921875\nLoss: 0.04466291889548302, accuracy = 0.96875\nLoss: 0.132561594247818, accuracy = 0.96875\nLoss: 0.3979755938053131, accuracy = 0.9375\nLoss: 0.10060809552669525, accuracy = 0.953125\nLoss: 0.3057703375816345, accuracy = 0.9375\nLoss: 0.12934750318527222, accuracy = 0.96875\nLoss: 0.08657151460647583, accuracy = 0.96875\nLoss: 0.21188294887542725, accuracy = 0.90625\nLoss: 0.14039435982704163, accuracy = 0.96875\nLoss: 0.09377827495336533, accuracy = 0.984375\nLoss: 0.0185707900673151, accuracy = 1.0\nLoss: 0.081519216299057, accuracy = 0.96875\nLoss: 0.32961714267730713, accuracy = 0.953125\nLoss: 0.051018428057432175, accuracy = 1.0\nLoss: 0.21368607878684998, accuracy = 0.984375\n\n\nThe same can be achieved by the use of the pytorch optim library, specifically in this case optim.SGD\n\nfrom torch import optim\n\n\ndef get_model_and_optimizer():\n    model = nn.Sequential(nn.Linear(m, nh), nn.ReLU(), nn.Linear(nh, c))\n    opt = optim.SGD(model.parameters(), lr=0.5)\n    return model, opt\n\n\nmodel, opt = get_model_and_optimizer()\nloss_func(model(xb), yb)\n\ntensor(2.31, grad_fn=<NllLossBackward0>)\n\n\n\nfor epoch in range(epochs):\n    for batch in range(0, n, bs):\n        # get data\n        s = slice(batch, min(n, batch+bs))\n        xs = x_train[s]\n        ys = y_train[s]\n        # Pass data through the model\n        preds = model(xs)\n        loss = loss_func(preds, ys)\n        # Print results\n        if batch // bs // 10 == 0: print(f\"Loss: {loss.item()}, accuracy = {accuracy(preds, ys).item()}\")\n        # Calculate gradients\n        loss.backward()\n        # Apply the optimiser\n        opt.step()\n        opt.zero_grad()\n\nLoss: 2.312685012817383, accuracy = 0.078125\nLoss: 2.1906702518463135, accuracy = 0.265625\nLoss: 2.1999802589416504, accuracy = 0.1875\nLoss: 2.0161678791046143, accuracy = 0.546875\nLoss: 1.8945502042770386, accuracy = 0.421875\nLoss: 1.7321070432662964, accuracy = 0.640625\nLoss: 1.5854045152664185, accuracy = 0.640625\nLoss: 1.4941891431808472, accuracy = 0.671875\nLoss: 1.4101829528808594, accuracy = 0.546875\nLoss: 1.4672781229019165, accuracy = 0.5\nLoss: 0.2142210453748703, accuracy = 0.90625\nLoss: 0.14814303815364838, accuracy = 0.96875\nLoss: 0.32211652398109436, accuracy = 0.90625\nLoss: 0.1859450340270996, accuracy = 0.953125\nLoss: 0.12843002378940582, accuracy = 0.9375\nLoss: 0.05411830171942711, accuracy = 0.984375\nLoss: 0.13670694828033447, accuracy = 0.96875\nLoss: 0.3170986473560333, accuracy = 0.953125\nLoss: 0.10294653475284576, accuracy = 0.953125\nLoss: 0.1652507185935974, accuracy = 0.953125\nLoss: 0.178289994597435, accuracy = 0.921875\nLoss: 0.10243190824985504, accuracy = 0.96875\nLoss: 0.2527705132961273, accuracy = 0.921875\nLoss: 0.13069084286689758, accuracy = 0.953125\nLoss: 0.09603864699602127, accuracy = 0.953125\nLoss: 0.02239409275352955, accuracy = 1.0\nLoss: 0.1111244335770607, accuracy = 0.96875\nLoss: 0.2913336157798767, accuracy = 0.953125\nLoss: 0.04398681968450546, accuracy = 1.0\nLoss: 0.1203981563448906, accuracy = 0.96875\n\n\n\n\nDataset and DataLoader\nThe next part of the development is to make the dataloading more generic, faster and more robust. This is done through the creation of datasets and dataloaders\n\nDataset\nIt’s clunky to iterate through minibatches of x and y values separately:\n    xb = x_train[s]\n    yb = y_train[s]\nInstead, let’s do these two steps together, by introducing a Dataset class:\n    xb,yb = train_ds[s]\nIn essence the dataset class is a way to robustly link inputs and targets through index values\n\nclass Dataset():\n    def __init__(self, x, y): self.x = x; self.y = y\n    def __len__(self): return len(self.x)\n    def __getitem__(self, index): return self.x[index], self.y[index]\n\n\ntrain_ds, valid_ds = Dataset(x_train, y_train), Dataset(x_valid, y_valid)\nassert len(x_train) == len(train_ds)\nassert len(x_valid) == len(valid_ds)\n\n\ndatum_xb, datum_yb = x_train[0:bs], y_train[0:bs]\nxb, yb = train_ds[0:bs]\nassert datum_xb.shape == xb.shape\nassert datum_yb.shape == yb.shape\n\n\nxb[0:3], yb[0:3]\n\n(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.]]),\n tensor([5, 0, 4]))\n\n\nNow adapt the training loop to use the dataset\n\nmodel, opt = get_model_and_optimizer()\n\n\nfor epoch in range(epochs):\n    for batch in range(0, n, bs):\n        # get data\n        xs, ys = train_ds[batch: min(n, batch+bs)]\n        # Pass data through the model\n        preds = model(xs)\n        loss = loss_func(preds, ys)\n        # Print results\n        if batch // bs // 10 == 0: print(f\"Loss: {loss.item()}, accuracy = {accuracy(preds, ys).item()}\")\n        # Calculate gradients\n        loss.backward()\n        # Apply the optimiser\n        opt.step()\n        opt.zero_grad()\n\nLoss: 2.2979648113250732, accuracy = 0.09375\nLoss: 2.1677284240722656, accuracy = 0.40625\nLoss: 2.192817211151123, accuracy = 0.171875\nLoss: 1.992324709892273, accuracy = 0.5\nLoss: 1.8881453275680542, accuracy = 0.5\nLoss: 1.6967313289642334, accuracy = 0.703125\nLoss: 1.6032072305679321, accuracy = 0.578125\nLoss: 1.5213433504104614, accuracy = 0.65625\nLoss: 1.4437696933746338, accuracy = 0.46875\nLoss: 1.5202784538269043, accuracy = 0.53125\nLoss: 0.2117222249507904, accuracy = 0.953125\nLoss: 0.15176746249198914, accuracy = 0.96875\nLoss: 0.2823413908481598, accuracy = 0.921875\nLoss: 0.1383940577507019, accuracy = 0.953125\nLoss: 0.13499413430690765, accuracy = 0.921875\nLoss: 0.04650742560625076, accuracy = 0.984375\nLoss: 0.12225606292486191, accuracy = 0.953125\nLoss: 0.31971240043640137, accuracy = 0.953125\nLoss: 0.07508628815412521, accuracy = 0.96875\nLoss: 0.22737926244735718, accuracy = 0.96875\nLoss: 0.1587153971195221, accuracy = 0.921875\nLoss: 0.12036815285682678, accuracy = 0.984375\nLoss: 0.27983012795448303, accuracy = 0.90625\nLoss: 0.07115809619426727, accuracy = 0.984375\nLoss: 0.10483404994010925, accuracy = 0.9375\nLoss: 0.025242304429411888, accuracy = 1.0\nLoss: 0.09612352401018143, accuracy = 0.953125\nLoss: 0.2911374866962433, accuracy = 0.953125\nLoss: 0.05527857318520546, accuracy = 0.984375\nLoss: 0.16760416328907013, accuracy = 0.984375\n\n\n\n\nDataLoader\nThe dataloader takes responsibility for which data to load, which allows choices to me made such as whether to sample randomly or sequentially etc. Effectively the dataloader is an iterator that will feed data one batch at a time until the dataset is exhausted.\nIn the example below a simple sequential sampler it implemented, others will be added later\n\nclass DataLoader():\n    def __init__(self, dataset, bs):\n        self.dataset, self.bs = dataset, bs\n    \n    def __iter__(self):\n        for i in range(0, len(self.dataset), self.bs):\n            yield self.dataset[i:min(i+self.bs, len(self.dataset))]\n    \n\n\nlen(train_ds)\n\n50000\n\n\n\ntrain_dl = DataLoader(train_ds, bs)\nvalid_dl = DataLoader(valid_ds, bs)\n\n\nxb, yb = next(iter(train_dl))\n\n\nxb.shape, yb.shape\n\n(torch.Size([64, 784]), torch.Size([64]))\n\n\n\nplt.imshow(xb[0].view(28,28))\nyb[0]\n\ntensor(5)\n\n\n\n\n\nNow implement a training loop with the dataloader\n\nmodel, opt = get_model_and_optimizer()\n\nNow create a function for the fit process since this will be used several times\n\ndef fit():\n    for epoch in range(epochs):\n        for xb, yb in train_dl:\n            # Pass data through the model\n            preds = model(xb)\n            loss = loss_func(preds, yb)\n            # Calculate gradients\n            loss.backward()\n            # Apply the optimiser\n            opt.step()\n            opt.zero_grad()\n        print(f'After epoch {epoch}, batch loss is: {loss.item()}')\n\n\nfit()\n\nAfter epoch 0, batch loss is: 0.3089807331562042\nAfter epoch 1, batch loss is: 0.19600419700145721\nAfter epoch 2, batch loss is: 0.10103154182434082\n\n\n\nloss_func(model(xb), yb), accuracy(model(xb), yb)\n\n(tensor(0.17, grad_fn=<NllLossBackward0>), tensor(0.94))\n\n\n\nxb.shape\n\ntorch.Size([64, 784])"
  },
  {
    "objectID": "posts/2022-11-30-fastai-course22p/04_dataloaders_optimisers_training.html#adding-in-different-sampling-methods",
    "href": "posts/2022-11-30-fastai-course22p/04_dataloaders_optimisers_training.html#adding-in-different-sampling-methods",
    "title": "Notebook to establish mini-batch training from first principles",
    "section": "Adding in different sampling methods",
    "text": "Adding in different sampling methods\nTo enable random or linear sampling a class is added to manage the sampling. This will now be developed. The inputs to the sampler need to be the dataset and a flag to indicate the type of sampling. The return should be the same dataset either shuffled or processed as necessary. This could be done upon just the item index values or the whole dataset could be processed and returned. In this case the item indexies are returned as a list as requested by the iter call\nI’m not sure why yield is not used here - something to check up on\n\nimport random\n\n\nclass Sampler():\n    def __init__(self, ds, shuffle=False):\n        self.n = len(ds)\n        self.ds = ds\n        self.shuffle = shuffle\n    \n    def __iter__(self):\n        indecies = list(range(self.n))\n        if self.shuffle:\n            random.shuffle(indecies)\n        return iter(indecies)\n        \n\n\nfrom itertools import islice\n\n\nsampler = Sampler(x_train, shuffle=False)\niterator = iter(sampler)\nids = []\nfor i in range(5): ids.append(next(iterator))\nids\n\n[0, 1, 2, 3, 4]\n\n\nThis is returning things one item at a time from the iterator. It is possible to use islice to generate a range of items. Note that since this iterator has already returned five entries it will supply the next 5, it doesn’t start from to front of the list unless a new iterator is created\n\nids = list(islice(iterator, 0, 5))\nids\n\n[5, 6, 7, 8, 9]\n\n\nWhen the random flag is set to true then\n\nsampler = Sampler(x_train, shuffle=True)\nlist(islice(sampler, 0, 10))\n\n[19149, 3623, 33271, 45722, 34626, 44572, 33273, 31591, 1328, 44705]\n\n\n\nsampler_v = Sampler(x_valid, shuffle=False)\nlist(islice(sampler_v, 0, 10))\n\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n\nHaving proven the basics of the sampler the fastcore library will be used to create a batch sampler. Two fastcore methods are used: 1. store_attr. This simply saves any parameters supplied as class properties with the same name as the supplied parameter 2. chunked. This returns batches of indecies from an iterator of user defined size with the option to specity whether to drop the last chunk if not the batch size, and also to provide only a defined number of chunks (useful to use a part of a dataset when get things working)\n\nimport fastcore.all as fc\n\nThe class BatchSampler is require to take as input the dataset, batch size, the sampler, and whether to drop the last batch if not the correct size. In this case the dataset is already embedded into the sampler and hence does not need to be added separately. The output should be an iterator returning batches of indecies\n\nclass BatchSampler:\n    def __init__(self, sampler, bs=16, drop_last=False):\n        fc.store_attr()\n    \n    def __iter__(self, ):\n        yield from fc.chunked(iter(self.sampler), chunk_sz=self.bs, drop_last=self.drop_last)\n\n\nbatches = BatchSampler(sampler, bs=4, drop_last=False)\n\n\nnext(iter(batches))\n\n[25976, 3805, 2452, 7221]\n\n\n\nlist(islice(batches, 5))\n\n[[43889, 38760, 1660, 43812],\n [19474, 27881, 1648, 5994],\n [43292, 20760, 47927, 46678],\n [16207, 19001, 31035, 2216],\n [21505, 234, 47366, 36409]]\n\n\nBatchSampler will provide a list of ids with which to make up the batch. The Sampler will then return the individual input data and targets for each id. The then have to collate the inputs and targets into torch arrays for processing.\nThis collation and stacking of data is done by a collate function\nWe now need a collate method to take the samples and convert them into a stacked tensor or inputs and outputs\nCollator inputs: list of tuples of input value pairs outputs: tuple of input and target values as torch tensors\n\ndef collate(b):\n    # the * means that multiple items will be received, which effectively means the list is taken an item\n    # at a time\n    xb, yb = zip(*b)\n    return (torch.stack(xb), torch.stack(yb))\n    \n\n\nbi = [train_ds[0], train_ds[1], train_ds[2]]\ncollate(bi)\n\n(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.]]),\n tensor([5, 0, 4]))\n\n\n\nFinally create a full dataloader using the above components\ninputs:dataset, batch size, sampling method to use, whether to use last batch\noutputs: tuple of stacked array of inputs and targets\nmethods: initiation - setup the dataset, sampling method, batch size, options iter - return a batch of data as a tuple\n\nclass DataLoader():\n    def __init__(self, dataset, collate_fn=collate, bs=64, shuffle=False, drop_last=False):\n        fc.store_attr()\n        sampler = Sampler(ds=dataset, shuffle=shuffle)\n        self.batch_sampler = BatchSampler(sampler, bs, drop_last)\n        \n    def __iter__(self):\n        yield from (self.collate_fn(self.dataset[i] for i in b) for b in self.batch_sampler)\n        \n\n\ntrain_dl = DataLoader(train_ds, collate, bs=64, shuffle=True)\nvalid_dl = DataLoader(valid_ds, collate, bs=64, shuffle=False)\n\n\ntrain_batch = next(iter(train_dl))\ntrain_batch[0].shape, train_batch[1].shape\n\n(torch.Size([64, 784]), torch.Size([64]))\n\n\n\nxb,yb = next(iter(valid_dl))\nxb.shape\n\ntorch.Size([64, 784])\n\n\n\nxb,yb = next(iter(valid_dl))\nplt.imshow(xb[0].view(28,28))\nyb[0]\n\ntensor(3)\n\n\n\n\n\n\n# Check the accuracy of the model when trained with the Dataloader\nmodel,opt = get_model_and_optimizer()\nfit()\n\nloss_func(model(xb), yb), accuracy(model(xb), yb)\n\nAfter epoch 0, batch loss is: 0.05507710948586464\nAfter epoch 1, batch loss is: 0.055910542607307434\nAfter epoch 2, batch loss is: 0.07595346868038177\n\n\n(tensor(0.10, grad_fn=<NllLossBackward0>), tensor(0.97))"
  },
  {
    "objectID": "posts/2022-11-30-fastai-course22p/04_dataloaders_optimisers_training.html#multiprocesssing-dataloader",
    "href": "posts/2022-11-30-fastai-course22p/04_dataloaders_optimisers_training.html#multiprocesssing-dataloader",
    "title": "Notebook to establish mini-batch training from first principles",
    "section": "Multiprocesssing DataLoader",
    "text": "Multiprocesssing DataLoader\nData loading is often a constraint in terms or time to process a job. Fortunately this is a task that lends itself to multi-processing and hence it makes sense to apply this.\nPytorch provide a multi-processing library and that is what will be used here.\nThe way it will be done is that the above library will provide a pool or workers, the number of which can be defined. Each worker can then be asked to load a batch of data. The batches returned will be returned as requested\n\nimport torch.multiprocessing as mp\nfrom fastcore.basics import store_attr\n\nThe way a dataset returns values based upon the index uses the getitem dunder as can be seen here\n\ntrain_ds.__getitem__([[3,5,8,10]])\n\n(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.]]),\n tensor([1, 2, 1, 3]))\n\n\n\ntrain_ds.__getitem__([3,5,8,10])\n\n(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.]]),\n tensor([1, 2, 1, 3]))\n\n\nIt is possible to use the map function to split individual groups of items as below\n\nfor o in map(train_ds.__getitem__, [[3,5], [8,10]]):\n    print(o)\n\n(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([1, 2]))\n(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([1, 3]))\n\n\nThe dataloader can then be modified so that each worker in a pool loads a batch and return it when requested. Note that it appears that the collate function is not needed here since the way in which the batch sampler passes a list of indecies results in an array of values being returned.\n\nclass DataLoader():\n    def __init__(self, dataset, bs=64, shuffle=False, drop_last=False, num_workers=1):\n        fc.store_attr()\n        sampler = Sampler(ds=dataset, shuffle=shuffle)\n        self.batch_sampler = BatchSampler(sampler, bs, drop_last)\n        \n    def __iter__(self):\n        with mp.Pool(self.num_workers) as ex:\n            yield from ex.map(self.dataset.__getitem__, iter(self.batch_sampler))\n\n        \n\n\ntrain_dl = DataLoader(train_ds, bs=64, shuffle=True, num_workers=2)\n\n\nit = iter(train_dl)\nres = next(it)\n\n\nxb, yb = next(it)\nxb.shape, yb.shape\n\n(torch.Size([64, 784]), torch.Size([64]))\n\n\n\nlen(res),res[0].shape\n\n(2, torch.Size([64, 784]))"
  },
  {
    "objectID": "posts/2022-11-30-fastai-course22p/04_dataloaders_optimisers_training.html#pytorch-dataloader",
    "href": "posts/2022-11-30-fastai-course22p/04_dataloaders_optimisers_training.html#pytorch-dataloader",
    "title": "Notebook to establish mini-batch training from first principles",
    "section": "Pytorch DataLoader",
    "text": "Pytorch DataLoader\n\nfrom torch.utils.data import DataLoader, SequentialSampler, RandomSampler, BatchSampler\n\nSteps to follow: 1. Create batch sampler 2. create dataloader (with multi worker) 3. Train model entirely using PyTorch 4.\n\ntrain_bs = BatchSampler(RandomSampler(train_ds), batch_size=bs, drop_last=False)\nvalid_bs = BatchSampler(SequentialSampler(valid_ds), batch_size=bs, drop_last=False)\n\n\n# In this case the collate function is not required\ntrain_dl = DataLoader(train_ds, batch_sampler=train_bs, num_workers=4, collate_fn=collate)\nvalid_dl = DataLoader(valid_ds, batch_sampler=valid_bs, num_workers=4)\n\nIn this case the collate function is not necessary as the dataset will already do this, as was shown above by the way a list of items returns stacked arrays. Allowing Pytorch to autogenerate teh samplers as well then this can all be simplified to\n\ntrain_dl = DataLoader(train_ds, batch_size=bs, shuffle=True, drop_last=True, num_workers=4)\nvalid_dl = DataLoader(valid_ds, batch_size=bs, shuffle=False, drop_last=False, num_workers=4)\n\nCheck accuracy as usual\n\nmodel,opt = get_model_and_optimizer()\nfit()\n\nloss_func(model(xb), yb), accuracy(model(xb), yb)\n\nAfter epoch 0, batch loss is: 0.3000752925872803\nAfter epoch 1, batch loss is: 0.034155331552028656\nAfter epoch 2, batch loss is: 0.07146771252155304\n\n\n(tensor(0.12, grad_fn=<NllLossBackward0>), tensor(0.97))"
  },
  {
    "objectID": "posts/2022-11-30-fastai-course22p/04_dataloaders_optimisers_training.html#validation",
    "href": "posts/2022-11-30-fastai-course22p/04_dataloaders_optimisers_training.html#validation",
    "title": "Notebook to establish mini-batch training from first principles",
    "section": "Validation",
    "text": "Validation\nIt is good (essential) practice to have a validation set and to check the accuracu of the model periodically, such as at the end of each epoch.\nBefore calling the validation dataset it is necessary to put the model into eval mode, which avoids issues with batchnorm and dropout layers, where different setting are used for training\nCreate an update fit routine which calculates and prints out loss and accuracy at the end of each epoch. As input define the number of epochs, the model to use, the loss function, the optimiser and the train and test dataloasers\n\ndef fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n    # Loop through the epochs\n    for epoch in range(epochs):\n        # Set model into training mode\n        model.train()\n        #iterate through each batch\n        for xb, yb in train_dl:\n            preds = model(xb)\n            loss = loss_func(preds, yb)\n            loss.backward()\n            opt.step()\n            opt.zero_grad()\n        \n        # Set model to eval mode for validation\n        model.eval()\n        # Run without grad calculations for validation (speed up model and use less memory)\n        with torch.no_grad():\n            # Reset the loss, accuracy and input count totals so that they can be used to sum values over the whole ds\n            total_loss = 0.; total_acc=0.; total_count=0\n            for xb, yb in valid_dl:\n                n_items = len(yb)\n                total_count += n_items\n                preds = model(xb)\n                total_loss += loss_func(preds, yb)*n_items\n                total_acc += (torch.topk(preds, 1)[1][:,0]==yb).sum()\n            print(f\"Epoch: {epoch}, loss: {total_loss/total_count}, acc: {total_acc/total_count}\")\n    \n\n\nmodel,opt = get_model_and_optimizer()\n\n\nfit(epochs=3, model=model, loss_func=loss_func, opt=opt, train_dl=train_dl, valid_dl=valid_dl)\n\nEpoch: 0, loss: 0.16384194791316986, acc: 0.9517999887466431\nEpoch: 1, loss: 0.11577688157558441, acc: 0.963699996471405\nEpoch: 2, loss: 0.11353033781051636, acc: 0.9666000008583069\n\n\nFinally create the dataloader using a function and then simplify the whole training process to three lines\n\ndef get_data_loaderers(train_ds, valid_ds, bs):\n    train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True)\n    valid_ds = DataLoader(valid_ds, batch_size=bs, shuffle=False)\n    return train_dl, valid_dl\n\n\nmodel, opt = get_model_and_optimizer()\ntrain_dl, valid_dl = get_data_loaderers(train_ds, valid_ds, bs=bs)\nfit(epochs=3, model=model, loss_func=loss_func, opt=opt, train_dl=train_dl, valid_dl=valid_dl)\n\nEpoch: 0, loss: 0.20603491365909576, acc: 0.9343000054359436\nEpoch: 1, loss: 0.20518435537815094, acc: 0.9363999962806702\nEpoch: 2, loss: 0.1369418054819107, acc: 0.9598000049591064"
  },
  {
    "objectID": "posts/2022-11-10-StableDiffusion/2022-11-11-diffedit_explore.html",
    "href": "posts/2022-11-10-StableDiffusion/2022-11-11-diffedit_explore.html",
    "title": "Notebook to explore DiffEdit",
    "section": "",
    "text": "This follows the paper: http://arxiv.org/abs/2210.11427\nThe first part of the notebook is setup to generate image masks based upon the differences in images generated by starting with the same noised image and denoising it with two different prompts, the first one the prompt that goes with the image, the second a prompt related to a “target” image, which is what it is desired to change some aspect of the image into.\nThe methodology in the paper is not very well described and so a few alternative approaches are considered\n\nfrom pathlib import Path\nfrom PIL import Image\nfrom tqdm.auto import tqdm\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport torch\nfrom torchvision import transforms as tfms\nfrom torch import autocast\n\nfrom diffusers import DDIMScheduler, LMSDiscreteScheduler\nfrom transformers import CLIPTextModel, CLIPTokenizer\nfrom diffusers import AutoencoderKL, UNet2DConditionModel\nfrom transformers import logging\n\n\n#!pip install accelerate\n\n\n# Note that this step is helpful to avoid verbose warnings when loading the text encoder\nlogging.set_verbosity_error()\n\n\n# Set device\ntorch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n\n# Load the tokenizer and text encoder\ntokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=torch.float16)\ntext_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=torch.float16).to(torch_device)\n\n\n# Load the VAE and Unet\nvae = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-ema\", torch_dtype=torch.float16).to(torch_device)\nunet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\", torch_dtype=torch.float16).to(torch_device)\n\n\n# Create a DDIM scheduler\nddim_sched = DDIMScheduler(beta_start=0.00085, \n                                    beta_end=0.012, \n                                    beta_schedule='scaled_linear', \n                                    clip_sample=False, \n                                    set_alpha_to_one=False)\n\n\n# Create a LMS scheduler\nscheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)"
  },
  {
    "objectID": "posts/2022-11-10-StableDiffusion/2022-11-11-diffedit_explore.html#add-functions",
    "href": "posts/2022-11-10-StableDiffusion/2022-11-11-diffedit_explore.html#add-functions",
    "title": "Notebook to explore DiffEdit",
    "section": "Add Functions",
    "text": "Add Functions\n\ndef prompt_to_embedding(prompt: str, torch_device):\n    text_input = tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n    with torch.no_grad():\n        embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]\n    return embeddings\n\n\ndef pil_to_latent(input_im):\n    # Single image -> single latent in a batch (so size 1, 4, 64, 64)\n    with torch.no_grad():\n        latent = vae.encode(tfms.ToTensor()(input_im).unsqueeze(0).half().to(torch_device)*2-1) # Note scaling\n    return 0.18215 * latent.latent_dist.sample()\n\n\ndef latents_to_array(latents):\n    latents = 1 / 0.18215 * latents\n    with torch.no_grad():\n        image = vae.decode(latents).sample\n\n    # Create image array\n    image = (image / 2 + 0.5).clamp(0, 1)\n    image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n    images = (image * 255).round().astype(\"uint8\")\n    # At this point, this is a single-item array of image data, so return only the item \n    # to remove the extra diemension from the returned data\n    return images[0]\n\n\ndef latents_to_pil(latents):\n    # bath of latents -> list of images\n    image = latents_to_array(latents)\n    pil_imagea = Image.fromarray(image)\n    return pil_imagea\n\n\ndef show_latents(latents):\n    fig, axs = plt.subplots(1, 4, figsize=(16, 4))\n    for c in range(4):\n        axs[c].imshow(latents[0][c].cpu(), cmap='Greys')\n        axs[c].axis('off')\n\n\ndef load_image(path_to_image, size):\n    path_to_img = Path(path_to_image)\n    assert path_to_img.is_file(), f\"No file found {path_to_image}\"\n    image = Image.open(path_to_img).convert('RGB')\n    return image\n\n\ndef denoising_loop(latents, text_emb, scheduler, g=7.5, strength=0.5, steps=50, dim=512, start_step=10, torch_device=\"cuda\"):\n    with autocast(torch_device):\n        noise_preds = torch.tensor([], device=torch_device)\n        for i, t in enumerate(scheduler.timesteps):\n            if i > start_step:\n                #print(f\"step: {i}\")\n                latent_model_input = torch.cat([latents] * 2)\n                latent_model_input = scheduler.scale_model_input(latent_model_input, t)\n                with torch.no_grad():\n                    noise_u,noise_t = unet(latent_model_input, t, encoder_hidden_states=text_emb).sample.chunk(2)\n                noise_pred = noise_u + g*(noise_t - noise_u)\n                noise_preds = torch.concat([noise_preds, noise_pred])\n                latents = scheduler.step(noise_pred, t, latents).prev_sample\n        return latents, noise_preds\n\n\ndef show_image(image, seed=None, scale_by=0.5):\n    if seed is not None:\n        print(f'Seed: {seed}')\n    return image.resize(((int)(image.width * scale_by), (int)(image.height * scale_by)))\n\n\ndef add_noise_to_image(latents, seed, scheduler, start_step):\n    torch.manual_seed(seed)\n    noise = torch.randn_like(latents)\n    noised_latents = scheduler.add_noise(\n        original_samples=latents, \n        noise=noise, \n        timesteps=torch.tensor([scheduler.timesteps[start_step]]))\n    return noised_latents\n\n\ndef show_images(nrows, ncols, images, titles=[], figsize=(16, 5)):\n    num_axes = nrows*ncols\n    num_images = len(images)\n    num_titles = len(titles)\n    fig, axs = plt.subplots(nrows, ncols, figsize=figsize)\n    flt_ax = axs.flat\n    for c in range(num_axes):\n        if c == num_images: break\n        flt_ax[c].imshow(images[c])\n        flt_ax[c].axis('off')\n        if c < num_titles:\n            flt_ax[c].set_title(titles[c])"
  },
  {
    "objectID": "posts/2022-11-10-StableDiffusion/2022-11-11-diffedit_explore.html#define-parameters-for-analysis",
    "href": "posts/2022-11-10-StableDiffusion/2022-11-11-diffedit_explore.html#define-parameters-for-analysis",
    "title": "Notebook to explore DiffEdit",
    "section": "Define parameters for analysis",
    "text": "Define parameters for analysis\n\nresolution = 512\ndef_steps = 70\ndef_g = 7.5\ndef_strength = 0.5\ndef_sch = scheduler\nstart_step = 20"
  },
  {
    "objectID": "posts/2022-11-10-StableDiffusion/2022-11-11-diffedit_explore.html#load-base-image-and-create-latents",
    "href": "posts/2022-11-10-StableDiffusion/2022-11-11-diffedit_explore.html#load-base-image-and-create-latents",
    "title": "Notebook to explore DiffEdit",
    "section": "Load base image and create latents",
    "text": "Load base image and create latents\n\npath_to_image = \"/home/images/horse_1_512.jpg\"\nimage = load_image(path_to_image, resolution)\nlatents = pil_to_latent(image)\n\n\n# show the base image\nshow_image(image)\n\n\n\n\n\n# Plot the latents\nshow_latents(latents)"
  },
  {
    "objectID": "posts/2022-11-10-StableDiffusion/2022-11-11-diffedit_explore.html#define-prompts-and-create-embeddings",
    "href": "posts/2022-11-10-StableDiffusion/2022-11-11-diffedit_explore.html#define-prompts-and-create-embeddings",
    "title": "Notebook to explore DiffEdit",
    "section": "Define Prompts and create embeddings",
    "text": "Define Prompts and create embeddings\n\n#base_prompt = \"A horse running on grass under a cloudy blue sky\"\n#target_prompt = \"A zebra running on grass under a cloudy blue sky\"\nbase_prompt = \"A horse\"\ntarget_prompt = \"Zebra\"\nunguided_prompt = [\"\"]\n\n\nbase_prompt_emb = prompt_to_embedding(base_prompt, torch_device)\ntarget_prompt_emb = prompt_to_embedding(target_prompt, torch_device)\nunguided_prompt = prompt_to_embedding(unguided_prompt, torch_device)\n\n\nbase_emb_pair = torch.concat([unguided_prompt, base_prompt_emb])\ntarget_emb_pair = torch.concat([unguided_prompt, target_prompt_emb])"
  },
  {
    "objectID": "posts/2022-11-10-StableDiffusion/2022-11-11-diffedit_explore.html#set-inference-timesteps",
    "href": "posts/2022-11-10-StableDiffusion/2022-11-11-diffedit_explore.html#set-inference-timesteps",
    "title": "Notebook to explore DiffEdit",
    "section": "Set inference timesteps",
    "text": "Set inference timesteps\n\ndef prepare_scheduler_noise_image_and_denoise(latents, text_emb, steps, start_step, scheduler, seed, g, dim, device):\n    timesteps = scheduler.set_timesteps(steps)\n    strength=0.5\n    noised_latents = add_noise_to_image(latents, seed, scheduler, start_step)\n    dn_latents, noise = denoising_loop(noised_latents, text_emb=text_emb, scheduler=scheduler, g=g, strength=strength, steps=70, \n                                           dim=dim, start_step=start_step, torch_device=device)\n    return dn_latents, noise\n    \n\n\ndn_base, noise_base = prepare_scheduler_noise_image_and_denoise(latents, \n                                                                text_emb=base_emb_pair, \n                                                                steps=def_steps, \n                                                                start_step=30, \n                                                                scheduler=def_sch, \n                                                                seed=100, \n                                                                g=7.5,\n                                                                dim=resolution, \n                                                                device=torch_device)\n\n/opt/conda/lib/python3.8/site-packages/diffusers/schedulers/scheduling_lms_discrete.py:155: IntegrationWarning: The maximum number of subdivisions (50) has been achieved.\n  If increasing the limit yields no improvement it is advised to analyze \n  the integrand in order to determine the difficulties.  If the position of a \n  local difficulty can be determined (singularity, discontinuity) one will \n  probably gain from splitting up the interval and calling the integrator \n  on the subranges.  Perhaps a special-purpose integrator should be used.\n  integrated_coeff = integrate.quad(lms_derivative, self.sigmas[t], self.sigmas[t + 1], epsrel=1e-4)[0]\n\n\n\nlatents_to_pil(dn_base)\n\n\n\n\n\ndn_target, noise_target = prepare_scheduler_noise_image_and_denoise(latents, \n                                                                text_emb=target_emb_pair, \n                                                                steps=def_steps, \n                                                                start_step=30, \n                                                                scheduler=def_sch, \n                                                                seed=100, \n                                                                g=7.5,\n                                                                dim=resolution, \n                                                                device=torch_device)\n\n\nlatents_to_pil(dn_target)\n\n\n\n\n\nshow_images(1, 3 , [np.asarray(image), latents_to_array(dn_base), latents_to_array(dn_target)],\n           [\"Original Image\", \"Denoised original\", \"Denoised Zebra\", \"Denoised Zebra with mask\"])\n\n\n\n\n\norig_img = np.asarray(image)\norig_img.shape\n\n(512, 515, 3)\n\n\n\nnoise_base.shape\n\ntorch.Size([39, 4, 64, 64])\n\n\n\ndiff_noises = (noise_base - noise_target).mean(0, keepdim=True)\n\n\ndiff_noises.shape\n\ntorch.Size([1, 4, 64, 64])\n\n\n\ndiff_noise_normed = (diff_noises - diff_noises.min())/(diff_noises - diff_noises.min()).max()\n\n\ndiff_noise_normed.shape\n\ntorch.Size([1, 4, 64, 64])\n\n\n\nshow_latents(diff_noise_normed)\n\n\n\n\n\ndiff_noise_normed.min(), diff_noise_normed.max(), diff_noise_normed.std(), diff_noise_normed.mean()\n\n(tensor(0., device='cuda:0'),\n tensor(1., device='cuda:0'),\n tensor(0.0744, device='cuda:0'),\n tensor(0.5083, device='cuda:0'))\n\n\n\nmask = ((diff_noise_normed-0.5).abs()+0.5).mean(dim=1).squeeze().cpu()\n\n\nimport cv2\n\n\ndef extract_channel_mask(img, do_inverse=False):\n    kernel = np.ones((3,3),np.uint8)\n    img = (img*255).squeeze().cpu().to(torch.uint8).numpy()\n    if do_inverse:\n        ret2,img2 = cv2.threshold(img,0,255,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)\n    else:\n        ret2,img2 = cv2.threshold(img,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n    opening = cv2.dilate(img2, kernel)\n    return opening\n\n\nshow_images(2, 1, [mask, extract_channel_mask(mask, do_inverse=False)])"
  },
  {
    "objectID": "posts/2022-11-10-StableDiffusion/2022-11-11-diffedit_explore.html#now-need-to-apply-the-mask-to-the-generated-zebra-image-and-then-run-the-decode-function",
    "href": "posts/2022-11-10-StableDiffusion/2022-11-11-diffedit_explore.html#now-need-to-apply-the-mask-to-the-generated-zebra-image-and-then-run-the-decode-function",
    "title": "Notebook to explore DiffEdit",
    "section": "Now need to apply the mask to the generated zebra image and then run the decode function",
    "text": "Now need to apply the mask to the generated zebra image and then run the decode function\n\nbinary_mask = torch.tensor(extract_channel_mask(mask, do_inverse=False)).bool()\n\n\ndef apply_mask_to_latents(original_latents, new_latents, mask):\n    comp_lat = torch.where(mask, new_latents.cpu(), original_latents.cpu())\n    return comp_lat\n\n\nfinal_latents = apply_mask_to_latents(dn_base, dn_target, binary_mask)\n\n\nlatents_to_pil(final_latents.to(torch_device))\n\n\n\n\n\nshow_images(2, 2, [np.asarray(image), latents_to_array(dn_base), latents_to_array(dn_target), latents_to_array(final_latents.to(torch_device))],\n           [\"Original Image\", \"Denoised original\", \"Denoised Zebra\", \"Denoised Zebra with mask\"], figsize=(10,10))\n\n\n\n\nIn this case the final masked image is almost identical to that of the unmasked image since the background generated by the denoising process had almost no differene. In other cases this could clearly be more extreme. The issue of course would be that the mask would need to be carefully blended to facilitate a smooth merge.\nOverall it seems to me that this is an approach that has very limited application an in many ways it is better to avoid using the mask"
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "Talks",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nML in the wild\n\n\n0 min\n\n\n\nDeep Learning\n\n\nAudio\n\n\nVision\n\n\nFastAI\n\n\n\n\n\n\n\nDec 15, 2020\n\n\n\n\n\n\nLocation\n\n\nUS\n\n\n\n\nVenue\n\n\nMeridian Winter Webinar Series\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "talks/2020-12-15-OrcaHello/index.html",
    "href": "talks/2020-12-15-OrcaHello/index.html",
    "title": "ML in the wild",
    "section": "",
    "text": "Sharing our insights from deploying OrcaHello - an open source, AI-assisted 24x7 hydrophone monitoring & Southern Resident Killer Whale alert system in Puget Sound.\n\nYoutube Link - (1:10:00)\nAI For Orcas Webiste"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "John’s Blog",
    "section": "",
    "text": "Notebook to establish mini-batch training from first principles\n\n\n\n\n\n\n\n\n\n\n\n\nJohn Richmond\n\n\n\n\n\n\n\n\nNotebook to explore DiffEdit\n\n\n\n\n\n\n\n\n\n\n\n\n\nJohn Richmond\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "I’m an experienced Machine Learning / AI specialist currently working in the Applied AI team at GSK. By background I am a Chartered Engineer and prior to joining GSK worked in the Tata Group as Chief Engineer in the Group Technology Office. I also spent many years as Head Of Advanced Engineering at Tata Motors, during which time I was responsible for the development of Eletric and Hybrid vehicles.\nMy interests include deep learning technologies, computer science and sport. I enjoy cycling and skiing (looking forward to the next opportunity to get to the slopes following the last few years)\nI am married with two amazing children and three grandchildren."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n \n\n\n\nNotebook to establish mini-batch training from first principles\n\n\n10 min\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\nNotebook to explore DiffEdit\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  }
]