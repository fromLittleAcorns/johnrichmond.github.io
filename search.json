[
  {
    "objectID": "posts/2022-11-10-StableDiffusion/2022-11-11-diffedit_explore.html",
    "href": "posts/2022-11-10-StableDiffusion/2022-11-11-diffedit_explore.html",
    "title": "Notebook to explore DiffEdit",
    "section": "",
    "text": "This follows the paper: http://arxiv.org/abs/2210.11427\nThe first part of the notebook is setup to generate image masks based upon the differences in images generated by starting with the same noised image and denoising it with two different prompts, the first one the prompt that goes with the image, the second a prompt related to a “target” image, which is what it is desired to change some aspect of the image into.\nThe methodology in the paper is not very well described and so a few alternative approaches are considered\n\nfrom pathlib import Path\nfrom PIL import Image\nfrom tqdm.auto import tqdm\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport torch\nfrom torchvision import transforms as tfms\nfrom torch import autocast\n\nfrom diffusers import DDIMScheduler, LMSDiscreteScheduler\nfrom transformers import CLIPTextModel, CLIPTokenizer\nfrom diffusers import AutoencoderKL, UNet2DConditionModel\nfrom transformers import logging\n\n\n#!pip install accelerate\n\n\n# Note that this step is helpful to avoid verbose warnings when loading the text encoder\nlogging.set_verbosity_error()\n\n\n# Set device\ntorch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n\n# Load the tokenizer and text encoder\ntokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=torch.float16)\ntext_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=torch.float16).to(torch_device)\n\n\n# Load the VAE and Unet\nvae = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-ema\", torch_dtype=torch.float16).to(torch_device)\nunet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\", torch_dtype=torch.float16).to(torch_device)\n\n\n# Create a DDIM scheduler\nddim_sched = DDIMScheduler(beta_start=0.00085, \n                                    beta_end=0.012, \n                                    beta_schedule='scaled_linear', \n                                    clip_sample=False, \n                                    set_alpha_to_one=False)\n\n\n# Create a LMS scheduler\nscheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)"
  },
  {
    "objectID": "posts/2022-11-10-StableDiffusion/2022-11-11-diffedit_explore.html#add-functions",
    "href": "posts/2022-11-10-StableDiffusion/2022-11-11-diffedit_explore.html#add-functions",
    "title": "Notebook to explore DiffEdit",
    "section": "Add Functions",
    "text": "Add Functions\n\ndef prompt_to_embedding(prompt: str, torch_device):\n    text_input = tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n    with torch.no_grad():\n        embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]\n    return embeddings\n\n\ndef pil_to_latent(input_im):\n    # Single image -> single latent in a batch (so size 1, 4, 64, 64)\n    with torch.no_grad():\n        latent = vae.encode(tfms.ToTensor()(input_im).unsqueeze(0).half().to(torch_device)*2-1) # Note scaling\n    return 0.18215 * latent.latent_dist.sample()\n\n\ndef latents_to_array(latents):\n    latents = 1 / 0.18215 * latents\n    with torch.no_grad():\n        image = vae.decode(latents).sample\n\n    # Create image array\n    image = (image / 2 + 0.5).clamp(0, 1)\n    image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n    images = (image * 255).round().astype(\"uint8\")\n    # At this point, this is a single-item array of image data, so return only the item \n    # to remove the extra diemension from the returned data\n    return images[0]\n\n\ndef latents_to_pil(latents):\n    # bath of latents -> list of images\n    image = latents_to_array(latents)\n    pil_imagea = Image.fromarray(image)\n    return pil_imagea\n\n\ndef show_latents(latents):\n    fig, axs = plt.subplots(1, 4, figsize=(16, 4))\n    for c in range(4):\n        axs[c].imshow(latents[0][c].cpu(), cmap='Greys')\n        axs[c].axis('off')\n\n\ndef load_image(path_to_image, size):\n    path_to_img = Path(path_to_image)\n    assert path_to_img.is_file(), f\"No file found {path_to_image}\"\n    image = Image.open(path_to_img).convert('RGB')\n    return image\n\n\ndef denoising_loop(latents, text_emb, scheduler, g=7.5, strength=0.5, steps=50, dim=512, start_step=10, torch_device=\"cuda\"):\n    with autocast(torch_device):\n        noise_preds = torch.tensor([], device=torch_device)\n        for i, t in enumerate(scheduler.timesteps):\n            if i > start_step:\n                #print(f\"step: {i}\")\n                latent_model_input = torch.cat([latents] * 2)\n                latent_model_input = scheduler.scale_model_input(latent_model_input, t)\n                with torch.no_grad():\n                    noise_u,noise_t = unet(latent_model_input, t, encoder_hidden_states=text_emb).sample.chunk(2)\n                noise_pred = noise_u + g*(noise_t - noise_u)\n                noise_preds = torch.concat([noise_preds, noise_pred])\n                latents = scheduler.step(noise_pred, t, latents).prev_sample\n        return latents, noise_preds\n\n\ndef show_image(image, seed=None, scale_by=0.5):\n    if seed is not None:\n        print(f'Seed: {seed}')\n    return image.resize(((int)(image.width * scale_by), (int)(image.height * scale_by)))\n\n\ndef add_noise_to_image(latents, seed, scheduler, start_step):\n    torch.manual_seed(seed)\n    noise = torch.randn_like(latents)\n    noised_latents = scheduler.add_noise(\n        original_samples=latents, \n        noise=noise, \n        timesteps=torch.tensor([scheduler.timesteps[start_step]]))\n    return noised_latents\n\n\ndef show_images(nrows, ncols, images, titles=[], figsize=(16, 5)):\n    num_axes = nrows*ncols\n    num_images = len(images)\n    num_titles = len(titles)\n    fig, axs = plt.subplots(nrows, ncols, figsize=figsize)\n    flt_ax = axs.flat\n    for c in range(num_axes):\n        if c == num_images: break\n        flt_ax[c].imshow(images[c])\n        flt_ax[c].axis('off')\n        if c < num_titles:\n            flt_ax[c].set_title(titles[c])"
  },
  {
    "objectID": "posts/2022-11-10-StableDiffusion/2022-11-11-diffedit_explore.html#define-parameters-for-analysis",
    "href": "posts/2022-11-10-StableDiffusion/2022-11-11-diffedit_explore.html#define-parameters-for-analysis",
    "title": "Notebook to explore DiffEdit",
    "section": "Define parameters for analysis",
    "text": "Define parameters for analysis\n\nresolution = 512\ndef_steps = 70\ndef_g = 7.5\ndef_strength = 0.5\ndef_sch = scheduler\nstart_step = 20"
  },
  {
    "objectID": "posts/2022-11-10-StableDiffusion/2022-11-11-diffedit_explore.html#load-base-image-and-create-latents",
    "href": "posts/2022-11-10-StableDiffusion/2022-11-11-diffedit_explore.html#load-base-image-and-create-latents",
    "title": "Notebook to explore DiffEdit",
    "section": "Load base image and create latents",
    "text": "Load base image and create latents\n\npath_to_image = \"/home/images/horse_1_512.jpg\"\nimage = load_image(path_to_image, resolution)\nlatents = pil_to_latent(image)\n\n\n# show the base image\nshow_image(image)\n\n\n\n\n\n# Plot the latents\nshow_latents(latents)"
  },
  {
    "objectID": "posts/2022-11-10-StableDiffusion/2022-11-11-diffedit_explore.html#define-prompts-and-create-embeddings",
    "href": "posts/2022-11-10-StableDiffusion/2022-11-11-diffedit_explore.html#define-prompts-and-create-embeddings",
    "title": "Notebook to explore DiffEdit",
    "section": "Define Prompts and create embeddings",
    "text": "Define Prompts and create embeddings\n\n#base_prompt = \"A horse running on grass under a cloudy blue sky\"\n#target_prompt = \"A zebra running on grass under a cloudy blue sky\"\nbase_prompt = \"A horse\"\ntarget_prompt = \"Zebra\"\nunguided_prompt = [\"\"]\n\n\nbase_prompt_emb = prompt_to_embedding(base_prompt, torch_device)\ntarget_prompt_emb = prompt_to_embedding(target_prompt, torch_device)\nunguided_prompt = prompt_to_embedding(unguided_prompt, torch_device)\n\n\nbase_emb_pair = torch.concat([unguided_prompt, base_prompt_emb])\ntarget_emb_pair = torch.concat([unguided_prompt, target_prompt_emb])"
  },
  {
    "objectID": "posts/2022-11-10-StableDiffusion/2022-11-11-diffedit_explore.html#set-inference-timesteps",
    "href": "posts/2022-11-10-StableDiffusion/2022-11-11-diffedit_explore.html#set-inference-timesteps",
    "title": "Notebook to explore DiffEdit",
    "section": "Set inference timesteps",
    "text": "Set inference timesteps\n\ndef prepare_scheduler_noise_image_and_denoise(latents, text_emb, steps, start_step, scheduler, seed, g, dim, device):\n    timesteps = scheduler.set_timesteps(steps)\n    strength=0.5\n    noised_latents = add_noise_to_image(latents, seed, scheduler, start_step)\n    dn_latents, noise = denoising_loop(noised_latents, text_emb=text_emb, scheduler=scheduler, g=g, strength=strength, steps=70, \n                                           dim=dim, start_step=start_step, torch_device=device)\n    return dn_latents, noise\n    \n\n\ndn_base, noise_base = prepare_scheduler_noise_image_and_denoise(latents, \n                                                                text_emb=base_emb_pair, \n                                                                steps=def_steps, \n                                                                start_step=30, \n                                                                scheduler=def_sch, \n                                                                seed=100, \n                                                                g=7.5,\n                                                                dim=resolution, \n                                                                device=torch_device)\n\n/opt/conda/lib/python3.8/site-packages/diffusers/schedulers/scheduling_lms_discrete.py:155: IntegrationWarning: The maximum number of subdivisions (50) has been achieved.\n  If increasing the limit yields no improvement it is advised to analyze \n  the integrand in order to determine the difficulties.  If the position of a \n  local difficulty can be determined (singularity, discontinuity) one will \n  probably gain from splitting up the interval and calling the integrator \n  on the subranges.  Perhaps a special-purpose integrator should be used.\n  integrated_coeff = integrate.quad(lms_derivative, self.sigmas[t], self.sigmas[t + 1], epsrel=1e-4)[0]\n\n\n\nlatents_to_pil(dn_base)\n\n\n\n\n\ndn_target, noise_target = prepare_scheduler_noise_image_and_denoise(latents, \n                                                                text_emb=target_emb_pair, \n                                                                steps=def_steps, \n                                                                start_step=30, \n                                                                scheduler=def_sch, \n                                                                seed=100, \n                                                                g=7.5,\n                                                                dim=resolution, \n                                                                device=torch_device)\n\n\nlatents_to_pil(dn_target)\n\n\n\n\n\nshow_images(1, 3 , [np.asarray(image), latents_to_array(dn_base), latents_to_array(dn_target)],\n           [\"Original Image\", \"Denoised original\", \"Denoised Zebra\", \"Denoised Zebra with mask\"])\n\n\n\n\n\norig_img = np.asarray(image)\norig_img.shape\n\n(512, 515, 3)\n\n\n\nnoise_base.shape\n\ntorch.Size([39, 4, 64, 64])\n\n\n\ndiff_noises = (noise_base - noise_target).mean(0, keepdim=True)\n\n\ndiff_noises.shape\n\ntorch.Size([1, 4, 64, 64])\n\n\n\ndiff_noise_normed = (diff_noises - diff_noises.min())/(diff_noises - diff_noises.min()).max()\n\n\ndiff_noise_normed.shape\n\ntorch.Size([1, 4, 64, 64])\n\n\n\nshow_latents(diff_noise_normed)\n\n\n\n\n\ndiff_noise_normed.min(), diff_noise_normed.max(), diff_noise_normed.std(), diff_noise_normed.mean()\n\n(tensor(0., device='cuda:0'),\n tensor(1., device='cuda:0'),\n tensor(0.0744, device='cuda:0'),\n tensor(0.5083, device='cuda:0'))\n\n\n\nmask = ((diff_noise_normed-0.5).abs()+0.5).mean(dim=1).squeeze().cpu()\n\n\nimport cv2\n\n\ndef extract_channel_mask(img, do_inverse=False):\n    kernel = np.ones((3,3),np.uint8)\n    img = (img*255).squeeze().cpu().to(torch.uint8).numpy()\n    if do_inverse:\n        ret2,img2 = cv2.threshold(img,0,255,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)\n    else:\n        ret2,img2 = cv2.threshold(img,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n    opening = cv2.dilate(img2, kernel)\n    return opening\n\n\nshow_images(2, 1, [mask, extract_channel_mask(mask, do_inverse=False)])"
  },
  {
    "objectID": "posts/2022-11-10-StableDiffusion/2022-11-11-diffedit_explore.html#now-need-to-apply-the-mask-to-the-generated-zebra-image-and-then-run-the-decode-function",
    "href": "posts/2022-11-10-StableDiffusion/2022-11-11-diffedit_explore.html#now-need-to-apply-the-mask-to-the-generated-zebra-image-and-then-run-the-decode-function",
    "title": "Notebook to explore DiffEdit",
    "section": "Now need to apply the mask to the generated zebra image and then run the decode function",
    "text": "Now need to apply the mask to the generated zebra image and then run the decode function\n\nbinary_mask = torch.tensor(extract_channel_mask(mask, do_inverse=False)).bool()\n\n\ndef apply_mask_to_latents(original_latents, new_latents, mask):\n    comp_lat = torch.where(mask, new_latents.cpu(), original_latents.cpu())\n    return comp_lat\n\n\nfinal_latents = apply_mask_to_latents(dn_base, dn_target, binary_mask)\n\n\nlatents_to_pil(final_latents.to(torch_device))\n\n\n\n\n\nshow_images(2, 2, [np.asarray(image), latents_to_array(dn_base), latents_to_array(dn_target), latents_to_array(final_latents.to(torch_device))],\n           [\"Original Image\", \"Denoised original\", \"Denoised Zebra\", \"Denoised Zebra with mask\"], figsize=(10,10))\n\n\n\n\nIn this case the final masked image is almost identical to that of the unmasked image since the background generated by the denoising process had almost no differene. In other cases this could clearly be more extreme. The issue of course would be that the mask would need to be carefully blended to facilitate a smooth merge.\nOverall it seems to me that this is an approach that has very limited application an in many ways it is better to avoid using the mask"
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "Talks",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nML in the wild\n\n\n0 min\n\n\n\nDeep Learning\n\n\nAudio\n\n\nVision\n\n\nFastAI\n\n\n\n\n\n\n\nDec 15, 2020\n\n\n\n\n\n\nLocation\n\n\nUS\n\n\n\n\nVenue\n\n\nMeridian Winter Webinar Series\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "talks/2020-12-15-OrcaHello/index.html",
    "href": "talks/2020-12-15-OrcaHello/index.html",
    "title": "ML in the wild",
    "section": "",
    "text": "Sharing our insights from deploying OrcaHello - an open source, AI-assisted 24x7 hydrophone monitoring & Southern Resident Killer Whale alert system in Puget Sound.\n\nYoutube Link - (1:10:00)\nAI For Orcas Webiste"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "John’s Blog",
    "section": "",
    "text": "Notebook to explore DiffEdit\n\n\n\n\n\n\n\n\n\n\n\n\n\nJohn Richmond\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "I’m an experienced Data Scientist with specialized skills in machine learning-based solutions. I enjoy staying on top of cutting-edge data technologies, including big data platforms, deep learning, optimization methods, and business analytics. My current work involves building data-driven products to enable smarter recommendations for Microsoft Partners, M365 service administrators and end-users to ensure the best usage of M365 services. Before that, I have experience working in various verticals like agricultural technology, pharmaceuticals, retail, e-commerce, and ride-sharing business model."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n \n\n\n\nNotebook to explore DiffEdit\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  }
]